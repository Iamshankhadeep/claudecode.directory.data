{
  "resources": [
    {
      "id": "claude-raycast-extension",
      "title": "Claude for Raycast",
      "slug": "claude-raycast-extension",
      "tagline": "Quick Claude AI access on macOS",
      "description": "Raycast extension that brings Claude AI to your fingertips on macOS. Quick queries, code explanations, and AI assistance without leaving your workflow.",
      "categoryId": "tools-cli",
      "category": {
        "id": "tools-cli",
        "name": "Tools & CLI",
        "slug": "tools",
        "description": "Command-line tools, utilities, and scripts to enhance your Claude development workflow, including enterprise-grade automation platforms.",
        "icon": "ðŸ› ï¸",
        "color": "#8B5CF6"
      },
      "type": "EXTERNAL",
      "url": "https://www.raycast.com/extensions/claude-ai",
      "tags": [
        {
          "tag": {
            "id": "raycast",
            "name": "raycast",
            "slug": "raycast"
          }
        },
        {
          "tag": {
            "id": "macos",
            "name": "macos",
            "slug": "macos"
          }
        },
        {
          "tag": {
            "id": "productivity",
            "name": "productivity",
            "slug": "productivity"
          }
        },
        {
          "tag": {
            "id": "quick-access",
            "name": "quick-access",
            "slug": "quick-access"
          }
        }
      ],
      "author": {
        "name": "Raycast Community",
        "url": "https://raycast.com"
      },
      "stats": {
        "votes": 421,
        "copies": 1893
      },
      "_count": {
        "votes": 421,
        "copies": 1893
      },
      "difficulty": "BEGINNER",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "claude-discord-bot",
      "title": "Claude Discord Bot",
      "slug": "claude-discord-bot",
      "tagline": "AI-powered Discord community assistant",
      "description": "Smart Discord bot powered by Claude AI. Help community members with coding questions, provide explanations, and facilitate technical discussions in your server.",
      "categoryId": "tools-cli",
      "category": {
        "id": "tools-cli",
        "name": "Tools & CLI",
        "slug": "tools",
        "description": "Command-line tools, utilities, and scripts to enhance your Claude development workflow, including enterprise-grade automation platforms.",
        "icon": "ðŸ› ï¸",
        "color": "#8B5CF6"
      },
      "type": "EXTERNAL",
      "url": "https://github.com/discord-community/claude-bot",
      "tags": [
        {
          "tag": {
            "id": "discord",
            "name": "discord",
            "slug": "discord"
          }
        },
        {
          "tag": {
            "id": "bot",
            "name": "bot",
            "slug": "bot"
          }
        },
        {
          "tag": {
            "id": "community",
            "name": "community",
            "slug": "community"
          }
        },
        {
          "tag": {
            "id": "chat",
            "name": "chat",
            "slug": "chat"
          }
        }
      ],
      "author": {
        "name": "Discord Community",
        "url": "https://github.com/discord-community"
      },
      "stats": {
        "votes": 367,
        "copies": 1567
      },
      "_count": {
        "votes": 367,
        "copies": 1567
      },
      "difficulty": "INTERMEDIATE",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "code-review-architecture-master",
      "title": "Code Review Master + Architecture Analysis",
      "slug": "code-review-architecture-master",
      "tagline": "Expert prompt templates prompt template",
      "description": "Advanced code review prompt focusing on architecture, security, performance, and maintainability with detailed analysis and improvement recommendations.",
      "categoryId": "prompt-templates",
      "category": {
        "id": "prompt-templates",
        "name": "Prompt Templates",
        "slug": "prompts",
        "description": "Carefully crafted prompt templates for common development tasks and workflows, featuring expert-level architectural and optimization guidance.",
        "icon": "ðŸ’¬",
        "color": "#10B981"
      },
      "type": "PROMPT_TEMPLATE",
      "content": "You are a principal software engineer and architecture expert with 15+ years of experience in code review, system design, and technical leadership at top technology companies.\n\n## Review Context\n**Codebase:** {{CODEBASE_TYPE}}\n**Language/Framework:** {{LANGUAGE_FRAMEWORK}}\n**Component Type:** {{COMPONENT_TYPE}}\n**Review Focus:** {{REVIEW_FOCUS}}\n**Team Experience:** {{TEAM_EXPERIENCE}}\n**Business Criticality:** {{BUSINESS_CRITICALITY}}\n\n## Code Submission\n```{{LANGUAGE}}\n{{CODE_CONTENT}}\n```\n\n## Additional Context\n**Purpose:** {{CODE_PURPOSE}}\n**Recent Changes:** {{RECENT_CHANGES}}\n**Known Issues:** {{KNOWN_ISSUES}}\n\n## Conduct an expert-level code review with the following comprehensive analysis:\n\n### 1. Architecture & Design Analysis (25%)\n- Evaluate adherence to SOLID principles and design patterns\n- Assess separation of concerns and single responsibility\n- Review dependency injection and inversion of control\n- Analyze abstraction levels and interface design\n- Check for proper layering and architectural boundaries\n- Evaluate compliance with domain-driven design principles\n- Review service boundaries and cohesion\n\n### 2. Security Assessment (20%)\n- Identify OWASP Top 10 vulnerabilities\n- Review input validation and sanitization\n- Analyze authentication and authorization implementation\n- Check for SQL injection, XSS, and CSRF vulnerabilities\n- Review sensitive data handling and encryption\n- Assess secrets management and configuration security\n- Evaluate API security and rate limiting\n- Check for insecure dependencies and known CVEs\n\n### 3. Performance & Scalability (20%)\n- Identify algorithmic complexity issues (Big O analysis)\n- Review database query performance and N+1 problems\n- Analyze memory usage and potential memory leaks\n- Check for efficient resource utilization\n- Review caching strategies and implementation\n- Assess concurrent programming and thread safety\n- Evaluate I/O operations and blocking calls\n- Check for proper connection pooling and resource cleanup\n\n### 4. Code Quality & Maintainability (15%)\n- Review code readability and self-documenting practices\n- Assess naming conventions and consistency\n- Evaluate function/method size and complexity\n- Check for code duplication and DRY principle violations\n- Review error handling patterns and exception management\n- Assess logging practices and observability\n- Evaluate configuration management and environment handling\n\n### 5. Testing Strategy & Coverage (10%)\n- Review test structure and organization\n- Assess test coverage for critical paths\n- Evaluate unit test quality and isolation\n- Check for proper mocking and test doubles\n- Review integration and contract testing\n- Assess test maintainability and readability\n- Evaluate performance and load testing coverage\n\n### 6. Framework & Language Best Practices (5%)\n- Review framework-specific best practices\n- Check for proper use of language idioms\n- Assess library and dependency choices\n- Review API design and REST principles\n- Evaluate async/await patterns and concurrency\n- Check for proper error handling patterns\n\n### 7. DevOps & Operational Concerns (5%)\n- Review deployment and configuration aspects\n- Assess monitoring and alerting implementation\n- Check for proper health check endpoints\n- Review logging and debugging capabilities\n- Evaluate container and infrastructure considerations\n- Assess backup and disaster recovery implications\n\n## Output Format:\n\n**Executive Summary:**\n- Overall code quality score (1-10)\n- Primary strengths and achievements\n- Critical risks and blockers\n- Recommendation: [Approve/Request Changes/Major Revision]\n\n**Detailed Analysis:**\n\n### ðŸ”´ CRITICAL ISSUES (Must fix before merge)\n1. **[Security Vulnerability]** - [Specific issue]\n   - **Impact:** [Business/security impact]\n   - **Location:** [File:Line or function name]\n   - **Fix:** [Specific solution]\n   - **Example:**\n   ```{{LANGUAGE}}\n   // Current problematic code\n   // Recommended solution\n   ```\n\n### ðŸŸ  MAJOR ISSUES (Should fix soon)\n2. **[Performance Issue]** - [Specific issue]\n   - **Impact:** [Performance/scalability impact]\n   - **Location:** [File:Line or function name]\n   - **Fix:** [Specific solution]\n   - **Improvement:** [Expected performance gain]\n\n### ðŸŸ¡ MODERATE ISSUES (Address in next iteration)\n3. **[Architecture Concern]** - [Specific issue]\n   - **Impact:** [Maintainability/extensibility impact]\n   - **Refactoring:** [Suggested approach]\n   - **Timeline:** [Recommended timeframe]\n\n### ðŸŸ¢ MINOR ISSUES (Nice to have)\n4. **[Code Style]** - [Specific issue]\n   - **Improvement:** [Style/readability enhancement]\n   - **Consistency:** [Standards alignment]\n\n**Architecture Assessment:**\n\n**Strengths:**\n- âœ… [Specific positive aspects]\n- âœ… [Good design decisions]\n- âœ… [Proper implementation patterns]\n\n**Areas for Improvement:**\n- âŒ [Architectural violations]\n- âŒ [Design pattern misuse]\n- âŒ [Coupling/cohesion issues]\n\n**Refactoring Recommendations:**\n\n**Phase 1 (Immediate):**\n```{{LANGUAGE}}\n// Current implementation\nfunction currentCode() {\n  // problematic code\n}\n\n// Recommended refactoring\nfunction improvedCode() {\n  // better implementation\n  // with explanation of benefits\n}\n```\n\n**Phase 2 (Medium-term):**\n- [Strategic refactoring suggestions]\n- [Architecture improvements]\n- [Performance optimizations]\n\n**Security Analysis:**\n\n**Vulnerabilities Found:**\n1. **[Vulnerability Type]** - Severity: [High/Medium/Low]\n   - **Description:** [What the vulnerability is]\n   - **Exploit Scenario:** [How it could be exploited]\n   - **Mitigation:** [How to fix it]\n\n**Security Recommendations:**\n- [ ] Implement input validation\n- [ ] Add authentication checks\n- [ ] Encrypt sensitive data\n- [ ] Add rate limiting\n- [ ] Update vulnerable dependencies\n\n**Performance Analysis:**\n\n**Bottlenecks Identified:**\n1. **[Performance Issue]** - Impact: [Response time/throughput]\n   - **Current:** [Current performance metrics]\n   - **Optimization:** [Specific improvement]\n   - **Expected:** [Projected performance gain]\n\n**Scalability Concerns:**\n- [Database query optimization]\n- [Memory usage patterns]\n- [Concurrent access handling]\n\n**Testing Recommendations:**\n\n**Missing Test Coverage:**\n- [ ] Unit tests for [specific functions]\n- [ ] Integration tests for [specific flows]\n- [ ] Performance tests for [specific scenarios]\n- [ ] Security tests for [specific vulnerabilities]\n\n**Test Quality Improvements:**\n- [Better test organization]\n- [More comprehensive assertions]\n- [Better test data management]\n\n**Long-term Technical Strategy:**\n\n**Maintainability Roadmap:**\n1. **Immediate (1-2 weeks):** [Critical fixes]\n2. **Short-term (1-2 months):** [Architecture improvements]\n3. **Long-term (3-6 months):** [Strategic refactoring]\n\n**Knowledge Transfer:**\n- **Documentation needs:** [What should be documented]\n- **Team training:** [Skills to develop]\n- **Code patterns:** [Standards to establish]\n\n**Metrics & Monitoring:**\n- **Key metrics to track:** [Performance indicators]\n- **Alerting rules:** [What to monitor]\n- **Dashboard requirements:** [Observability needs]\n\n**Risk Assessment:**\n- **Technical risks:** [Potential future problems]\n- **Business risks:** [Impact on product/users]\n- **Mitigation strategies:** [How to address risks]\n\n**Approval Criteria:**\n- [ ] All critical security issues resolved\n- [ ] Performance bottlenecks addressed\n- [ ] Test coverage meets standards (X%)\n- [ ] Documentation updated\n- [ ] Architecture review passed\n\n**Additional Resources:**\n- [Relevant documentation links]\n- [Best practice guides]\n- [Training materials]\n- [Similar code examples]\n\nProvide specific, actionable feedback with concrete examples and clear priorities for improvement.",
      "tags": [
        {
          "tag": {
            "id": "code-review",
            "name": "code-review",
            "slug": "code-review"
          }
        },
        {
          "tag": {
            "id": "architecture",
            "name": "architecture",
            "slug": "architecture"
          }
        },
        {
          "tag": {
            "id": "security",
            "name": "security",
            "slug": "security"
          }
        },
        {
          "tag": {
            "id": "performance",
            "name": "performance",
            "slug": "performance"
          }
        },
        {
          "tag": {
            "id": "maintainability",
            "name": "maintainability",
            "slug": "maintainability"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 99,
        "copies": 129
      },
      "_count": {
        "votes": 76,
        "copies": 268
      },
      "difficulty": "ADVANCED",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "performance-engineering",
      "title": "Performance Engineering + Profiling",
      "slug": "performance-engineering-profiling-optimization",
      "tagline": "Cross-Platform Performance configuration for advanced developers",
      "description": "Comprehensive performance engineering setup with profiling, monitoring, optimization techniques, and performance testing across the full stack.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "ðŸ“‹",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Performance Engineering + Profiling\n\n## Project Overview\n\nThis is a comprehensive performance engineering setup covering profiling, monitoring, optimization techniques, and performance testing across frontend, backend, database, and infrastructure layers. Focus on measurable performance improvements and systematic optimization approaches.\n\n## Performance Philosophy\n\n### Performance Engineering Principles\n1. **Measure First**: Always profile before optimizing\n2. **Data-Driven Decisions**: Use metrics to guide optimization efforts\n3. **Systematic Approach**: Follow a methodical performance improvement process\n4. **Continuous Monitoring**: Implement ongoing performance tracking\n5. **User-Centric Metrics**: Focus on user experience over technical metrics\n\n### Performance Optimization Hierarchy\n1. **Algorithm Optimization**: Choose the right algorithms and data structures\n2. **Code-Level Optimization**: Optimize critical code paths\n3. **System Architecture**: Design for performance from the ground up\n4. **Infrastructure Scaling**: Scale horizontally and vertically as needed\n5. **Caching Strategies**: Implement multi-layer caching\n\n## Technology Stack\n\n- **Profiling**: Chrome DevTools, Node.js Profiler, async-profiler (Java), pprof (Go)\n- **Monitoring**: Prometheus, Grafana, New Relic, Datadog\n- **Load Testing**: k6, Artillery, JMeter, Gatling\n- **Database Profiling**: SQL Explain Plans, Database-specific tools\n- **Frontend Performance**: Lighthouse, WebPageTest, Core Web Vitals\n- **Infrastructure**: Docker, Kubernetes, CDN, Load Balancers\n\n## Project Structure\n\n```\nperformance-engineering/\nâ”œâ”€â”€ profiling/\nâ”‚   â”œâ”€â”€ frontend/            # Frontend performance profiling\nâ”‚   â”œâ”€â”€ backend/             # Backend application profiling\nâ”‚   â”œâ”€â”€ database/            # Database query optimization\nâ”‚   â””â”€â”€ infrastructure/      # System-level profiling\nâ”œâ”€â”€ monitoring/\nâ”‚   â”œâ”€â”€ metrics/             # Custom metrics and dashboards\nâ”‚   â”œâ”€â”€ alerting/            # Performance alerting rules\nâ”‚   â”œâ”€â”€ logging/             # Performance logging\nâ”‚   â””â”€â”€ tracing/             # Distributed tracing setup\nâ”œâ”€â”€ testing/\nâ”‚   â”œâ”€â”€ load-tests/          # Load and stress testing\nâ”‚   â”œâ”€â”€ benchmark/           # Microbenchmarks\nâ”‚   â”œâ”€â”€ synthetic/           # Synthetic monitoring\nâ”‚   â””â”€â”€ chaos/               # Chaos engineering tests\nâ”œâ”€â”€ optimization/\nâ”‚   â”œâ”€â”€ algorithms/          # Algorithm optimization examples\nâ”‚   â”œâ”€â”€ caching/             # Caching strategies\nâ”‚   â”œâ”€â”€ database/            # Database optimization\nâ”‚   â””â”€â”€ infrastructure/      # Infrastructure optimization\nâ””â”€â”€ tools/\n    â”œâ”€â”€ scripts/             # Performance analysis scripts\n    â”œâ”€â”€ dashboards/          # Monitoring dashboards\n    â””â”€â”€ reports/             # Performance reports\n```\n\n## Frontend Performance Optimization\n\n### Core Web Vitals Monitoring\n```typescript\n// profiling/frontend/core-web-vitals.ts\nexport class CoreWebVitalsMonitor {\n  private observer: PerformanceObserver | null = null;\n\n  constructor() {\n    this.initializeObserver();\n  }\n\n  private initializeObserver(): void {\n    if ('PerformanceObserver' in window) {\n      this.observer = new PerformanceObserver((list) => {\n        list.getEntries().forEach((entry) => {\n          this.handlePerformanceEntry(entry);\n        });\n      });\n\n      // Observe all performance entry types\n      this.observer.observe({ \n        entryTypes: ['navigation', 'resource', 'paint', 'layout-shift', 'largest-contentful-paint'] \n      });\n    }\n  }\n\n  private handlePerformanceEntry(entry: PerformanceEntry): void {\n    switch (entry.entryType) {\n      case 'navigation':\n        this.analyzeNavigationTiming(entry as PerformanceNavigationTiming);\n        break;\n      case 'resource':\n        this.analyzeResourceTiming(entry as PerformanceResourceTiming);\n        break;\n      case 'paint':\n        this.analyzePaintTiming(entry);\n        break;\n      case 'layout-shift':\n        this.analyzeCLS(entry as any);\n        break;\n      case 'largest-contentful-paint':\n        this.analyzeLCP(entry as any);\n        break;\n    }\n  }\n\n  private analyzeNavigationTiming(entry: PerformanceNavigationTiming): void {\n    const metrics = {\n      dns: entry.domainLookupEnd - entry.domainLookupStart,\n      tcp: entry.connectEnd - entry.connectStart,\n      ssl: entry.connectEnd - entry.secureConnectionStart,\n      ttfb: entry.responseStart - entry.requestStart,\n      download: entry.responseEnd - entry.responseStart,\n      domProcessing: entry.domContentLoadedEventStart - entry.responseEnd,\n      domReady: entry.domContentLoadedEventEnd - entry.navigationStart,\n      pageLoad: entry.loadEventEnd - entry.navigationStart\n    };\n\n    console.log('Navigation Performance:', metrics);\n    this.sendMetrics('navigation', metrics);\n  }\n\n  private analyzeResourceTiming(entry: PerformanceResourceTiming): void {\n    const resourceMetrics = {\n      name: entry.name,\n      duration: entry.duration,\n      size: entry.transferSize,\n      cached: entry.transferSize === 0 && entry.decodedBodySize > 0,\n      blocking: entry.renderBlockingStatus === 'blocking'\n    };\n\n    // Flag slow resources\n    if (entry.duration > 1000) {\n      console.warn('Slow resource detected:', resourceMetrics);\n      this.sendAlert('slow_resource', resourceMetrics);\n    }\n  }\n\n  private analyzeLCP(entry: any): void {\n    const lcp = entry.startTime;\n    console.log('Largest Contentful Paint:', lcp);\n    \n    // LCP should be under 2.5s for good user experience\n    if (lcp > 2500) {\n      this.sendAlert('poor_lcp', { value: lcp, threshold: 2500 });\n    }\n    \n    this.sendMetrics('lcp', { value: lcp });\n  }\n\n  private analyzeCLS(entry: any): void {\n    const cls = entry.value;\n    console.log('Cumulative Layout Shift:', cls);\n    \n    // CLS should be under 0.1 for good user experience\n    if (cls > 0.1) {\n      this.sendAlert('poor_cls', { value: cls, threshold: 0.1 });\n    }\n    \n    this.sendMetrics('cls', { value: cls });\n  }\n\n  private sendMetrics(type: string, data: any): void {\n    // Send to analytics service\n    fetch('/api/performance-metrics', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ type, data, timestamp: Date.now() })\n    }).catch(console.error);\n  }\n\n  private sendAlert(type: string, data: any): void {\n    // Send performance alerts\n    fetch('/api/performance-alerts', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ type, data, timestamp: Date.now() })\n    }).catch(console.error);\n  }\n\n  // Manual performance measurement\n  measureCustomMetric(name: string, fn: () => void): number {\n    const start = performance.now();\n    fn();\n    const end = performance.now();\n    const duration = end - start;\n    \n    this.sendMetrics('custom', { name, duration });\n    return duration;\n  }\n\n  // Resource loading optimization\n  preloadCriticalResources(resources: string[]): void {\n    resources.forEach(resource => {\n      const link = document.createElement('link');\n      link.rel = 'preload';\n      link.href = resource;\n      \n      if (resource.endsWith('.css')) {\n        link.as = 'style';\n      } else if (resource.endsWith('.js')) {\n        link.as = 'script';\n      } else if (resource.match(/.(jpg|jpeg|png|webp)$/)) {\n        link.as = 'image';\n      }\n      \n      document.head.appendChild(link);\n    });\n  }\n}\n\n// Initialize monitoring\nconst performanceMonitor = new CoreWebVitalsMonitor();\n\n// Export for global use\n(window as any).performanceMonitor = performanceMonitor;\n```\n\n### Frontend Bundle Optimization\n```typescript\n// optimization/frontend/bundle-analyzer.ts\nimport { BundleAnalyzerPlugin } from 'webpack-bundle-analyzer';\nimport CompressionPlugin from 'compression-webpack-plugin';\n\nexport const webpackOptimizationConfig = {\n  // Code splitting\n  optimization: {\n    splitChunks: {\n      cacheGroups: {\n        vendor: {\n          test: /[\\\\/]node_modules[\\\\/]/,\n          name: 'vendors',\n          chunks: 'all',\n          priority: 10\n        },\n        common: {\n          name: 'common',\n          minChunks: 2,\n          chunks: 'all',\n          priority: 5,\n          reuseExistingChunk: true\n        }\n      }\n    },\n    // Tree shaking\n    usedExports: true,\n    sideEffects: false,\n    // Runtime chunk\n    runtimeChunk: 'single'\n  },\n\n  // Performance budgets\n  performance: {\n    maxAssetSize: 250000,\n    maxEntrypointSize: 250000,\n    hints: 'warning'\n  },\n\n  plugins: [\n    // Bundle analysis\n    new BundleAnalyzerPlugin({\n      analyzerMode: 'static',\n      openAnalyzer: false,\n      reportFilename: 'bundle-report.html'\n    }),\n\n    // Compression\n    new CompressionPlugin({\n      algorithm: 'gzip',\n      test: /.(js|css|html|svg)$/,\n      threshold: 8192,\n      minRatio: 0.8\n    })\n  ]\n};\n\n// Lazy loading utility\nexport class LazyLoader {\n  private loadedModules = new Map<string, Promise<any>>();\n\n  async loadModule<T>(moduleFactory: () => Promise<{ default: T }>): Promise<T> {\n    const moduleKey = moduleFactory.toString();\n    \n    if (!this.loadedModules.has(moduleKey)) {\n      this.loadedModules.set(moduleKey, moduleFactory());\n    }\n    \n    const module = await this.loadedModules.get(moduleKey)!;\n    return module.default;\n  }\n\n  // Image lazy loading\n  observeImages(): void {\n    const imageObserver = new IntersectionObserver((entries) => {\n      entries.forEach(entry => {\n        if (entry.isIntersecting) {\n          const img = entry.target as HTMLImageElement;\n          if (img.dataset.src) {\n            img.src = img.dataset.src;\n            img.removeAttribute('data-src');\n            imageObserver.unobserve(img);\n          }\n        }\n      });\n    });\n\n    document.querySelectorAll('img[data-src]').forEach(img => {\n      imageObserver.observe(img);\n    });\n  }\n}\n```\n\n## Backend Performance Profiling\n\n### Node.js Performance Profiling\n```typescript\n// profiling/backend/nodejs-profiler.ts\nimport { performance, PerformanceObserver } from 'perf_hooks';\nimport { createWriteStream } from 'fs';\nimport { promisify } from 'util';\n\nexport class NodeJSProfiler {\n  private metricsLog = createWriteStream('performance-metrics.log', { flags: 'a' });\n  private observer: PerformanceObserver;\n\n  constructor() {\n    this.initializeObserver();\n  }\n\n  private initializeObserver(): void {\n    this.observer = new PerformanceObserver((list) => {\n      list.getEntries().forEach((entry) => {\n        this.logMetric({\n          name: entry.name,\n          duration: entry.duration,\n          startTime: entry.startTime,\n          timestamp: new Date().toISOString()\n        });\n      });\n    });\n\n    this.observer.observe({ entryTypes: ['measure', 'function'] });\n  }\n\n  // Method decorator for automatic profiling\n  profile(target: any, propertyName: string, descriptor: PropertyDescriptor): void {\n    const method = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n      const measureName = `${target.constructor.name}.${propertyName}`;\n      const startMark = `${measureName}-start`;\n      const endMark = `${measureName}-end`;\n\n      performance.mark(startMark);\n      \n      try {\n        const result = await method.apply(this, args);\n        performance.mark(endMark);\n        performance.measure(measureName, startMark, endMark);\n        return result;\n      } catch (error) {\n        performance.mark(endMark);\n        performance.measure(`${measureName}-error`, startMark, endMark);\n        throw error;\n      }\n    };\n  }\n\n  // Manual timing\n  async timeFunction<T>(name: string, fn: () => Promise<T>): Promise<T> {\n    const startTime = performance.now();\n    \n    try {\n      const result = await fn();\n      const endTime = performance.now();\n      \n      this.logMetric({\n        name,\n        duration: endTime - startTime,\n        status: 'success',\n        timestamp: new Date().toISOString()\n      });\n      \n      return result;\n    } catch (error) {\n      const endTime = performance.now();\n      \n      this.logMetric({\n        name: `${name}-error`,\n        duration: endTime - startTime,\n        status: 'error',\n        error: error.message,\n        timestamp: new Date().toISOString()\n      });\n      \n      throw error;\n    }\n  }\n\n  // Memory usage tracking\n  trackMemoryUsage(): void {\n    setInterval(() => {\n      const memUsage = process.memoryUsage();\n      this.logMetric({\n        name: 'memory-usage',\n        heapUsed: memUsage.heapUsed,\n        heapTotal: memUsage.heapTotal,\n        external: memUsage.external,\n        rss: memUsage.rss,\n        timestamp: new Date().toISOString()\n      });\n    }, 30000); // Every 30 seconds\n  }\n\n  // CPU profiling\n  startCPUProfiling(): void {\n    const profiler = require('v8-profiler-next');\n    profiler.startProfiling('CPU');\n    \n    setTimeout(() => {\n      const profile = profiler.stopProfiling('CPU');\n      profile.export((error: any, result: string) => {\n        if (!error) {\n          require('fs').writeFileSync('cpu-profile.cpuprofile', result);\n        }\n        profile.delete();\n      });\n    }, 60000); // Profile for 1 minute\n  }\n\n  private logMetric(metric: any): void {\n    this.metricsLog.write(JSON.stringify(metric) + '\\n');\n    \n    // Send to monitoring system\n    if (process.env.NODE_ENV === 'production') {\n      this.sendToMonitoring(metric);\n    }\n  }\n\n  private async sendToMonitoring(metric: any): Promise<void> {\n    // Implementation depends on your monitoring system\n    // Example: Prometheus, DataDog, New Relic, etc.\n  }\n}\n\n// Usage example\nexport class UserService {\n  private profiler = new NodeJSProfiler();\n\n  @profile\n  async createUser(userData: any): Promise<any> {\n    // Simulate database operation\n    await new Promise(resolve => setTimeout(resolve, 100));\n    return { id: Math.random(), ...userData };\n  }\n\n  async processUsers(users: any[]): Promise<any[]> {\n    return this.profiler.timeFunction('process-users', async () => {\n      return Promise.all(users.map(user => this.createUser(user)));\n    });\n  }\n}\n```\n\n### Database Performance Optimization\n```sql\n-- profiling/database/query-optimization.sql\n\n-- Enable query logging and analysis\nSET log_statement = 'all';\nSET log_min_duration_statement = 1000; -- Log queries taking > 1s\n\n-- Analyze slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    stddev_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 20;\n\n-- Index usage analysis\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan as index_scans,\n    idx_tup_read as tuples_read,\n    idx_tup_fetch as tuples_fetched\nFROM pg_stat_user_indexes \nORDER BY idx_scan DESC;\n\n-- Table bloat analysis\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,\n    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size\nFROM pg_tables \nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n```\n\n```typescript\n// profiling/database/query-profiler.ts\nimport { Pool, PoolClient } from 'pg';\n\nexport class DatabaseProfiler {\n  private pool: Pool;\n  private slowQueryThreshold = 1000; // 1 second\n\n  constructor(pool: Pool) {\n    this.pool = pool;\n  }\n\n  async profileQuery<T>(\n    query: string, \n    params: any[] = [], \n    context: string = ''\n  ): Promise<{ result: T; metrics: any }> {\n    const client = await this.pool.connect();\n    const startTime = performance.now();\n    let result: any;\n    let error: Error | null = null;\n\n    try {\n      // Enable query analysis\n      await client.query('SET enable_seqscan = off'); // Force index usage for testing\n      await client.query('SET work_mem = \\'4MB\\''); // Optimize for this query\n      \n      const queryResult = await client.query(query, params);\n      result = queryResult.rows;\n      \n      const endTime = performance.now();\n      const duration = endTime - startTime;\n\n      // Get query execution plan\n      const explainResult = await client.query(`EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) ${query}`, params);\n      const executionPlan = explainResult.rows[0]['QUERY PLAN'][0];\n\n      const metrics = {\n        query: query.substring(0, 100) + '...',\n        context,\n        duration,\n        rowCount: queryResult.rowCount,\n        executionTime: executionPlan['Execution Time'],\n        planningTime: executionPlan['Planning Time'],\n        buffers: executionPlan['Buffers'],\n        isSlow: duration > this.slowQueryThreshold\n      };\n\n      this.logQueryMetrics(metrics);\n\n      if (metrics.isSlow) {\n        this.alertSlowQuery(metrics, executionPlan);\n      }\n\n      return { result, metrics };\n\n    } catch (err) {\n      error = err as Error;\n      const endTime = performance.now();\n      \n      this.logQueryError({\n        query: query.substring(0, 100) + '...',\n        context,\n        duration: endTime - startTime,\n        error: error.message\n      });\n      \n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n\n  // Connection pool monitoring\n  monitorConnectionPool(): void {\n    setInterval(() => {\n      const poolStats = {\n        totalCount: this.pool.totalCount,\n        idleCount: this.pool.idleCount,\n        waitingCount: this.pool.waitingCount,\n        timestamp: new Date().toISOString()\n      };\n\n      console.log('Pool Stats:', poolStats);\n      \n      // Alert if pool is under pressure\n      if (poolStats.waitingCount > 5) {\n        this.alertPoolPressure(poolStats);\n      }\n    }, 30000);\n  }\n\n  // Query optimization suggestions\n  async analyzeQueryPerformance(query: string): Promise<any> {\n    const client = await this.pool.connect();\n    \n    try {\n      // Get detailed execution plan\n      const explainResult = await client.query(`EXPLAIN (ANALYZE, BUFFERS, VERBOSE, FORMAT JSON) ${query}`);\n      const plan = explainResult.rows[0]['QUERY PLAN'][0];\n\n      const suggestions = this.generateOptimizationSuggestions(plan);\n      \n      return {\n        executionPlan: plan,\n        suggestions,\n        performance: {\n          executionTime: plan['Execution Time'],\n          planningTime: plan['Planning Time'],\n          totalTime: plan['Execution Time'] + plan['Planning Time']\n        }\n      };\n    } finally {\n      client.release();\n    }\n  }\n\n  private generateOptimizationSuggestions(plan: any): string[] {\n    const suggestions: string[] = [];\n\n    // Check for sequential scans\n    if (this.hasSequentialScan(plan)) {\n      suggestions.push('Consider adding indexes for columns used in WHERE clauses');\n    }\n\n    // Check for expensive sorts\n    if (this.hasExpensiveSort(plan)) {\n      suggestions.push('Consider adding indexes to support ORDER BY clauses');\n    }\n\n    // Check for nested loops with high cost\n    if (this.hasExpensiveNestedLoop(plan)) {\n      suggestions.push('Consider optimizing JOIN conditions or adding appropriate indexes');\n    }\n\n    return suggestions;\n  }\n\n  private hasSequentialScan(plan: any): boolean {\n    // Recursively check for Seq Scan nodes\n    if (plan['Node Type'] === 'Seq Scan') {\n      return true;\n    }\n    \n    if (plan['Plans']) {\n      return plan['Plans'].some((subPlan: any) => this.hasSequentialScan(subPlan));\n    }\n    \n    return false;\n  }\n\n  private hasExpensiveSort(plan: any): boolean {\n    if (plan['Node Type'] === 'Sort' && plan['Total Cost'] > 1000) {\n      return true;\n    }\n    \n    if (plan['Plans']) {\n      return plan['Plans'].some((subPlan: any) => this.hasExpensiveSort(subPlan));\n    }\n    \n    return false;\n  }\n\n  private hasExpensiveNestedLoop(plan: any): boolean {\n    if (plan['Node Type'] === 'Nested Loop' && plan['Total Cost'] > 10000) {\n      return true;\n    }\n    \n    if (plan['Plans']) {\n      return plan['Plans'].some((subPlan: any) => this.hasExpensiveNestedLoop(subPlan));\n    }\n    \n    return false;\n  }\n\n  private logQueryMetrics(metrics: any): void {\n    console.log('Query Metrics:', metrics);\n    // Send to monitoring system\n  }\n\n  private logQueryError(error: any): void {\n    console.error('Query Error:', error);\n    // Send to error tracking system\n  }\n\n  private alertSlowQuery(metrics: any, plan: any): void {\n    console.warn('Slow Query Detected:', { metrics, plan });\n    // Send alert to monitoring system\n  }\n\n  private alertPoolPressure(stats: any): void {\n    console.warn('Connection Pool Pressure:', stats);\n    // Send alert to monitoring system\n  }\n}\n```\n\n## Load Testing and Benchmarking\n\n### K6 Load Testing\n```javascript\n// testing/load-tests/api-load-test.js\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\nimport { Rate, Trend, Counter } from 'k6/metrics';\n\n// Custom metrics\nconst errorRate = new Rate('errors');\nconst responseTime = new Trend('response_time');\nconst requestCount = new Counter('requests');\n\n// Test configuration\nexport const options = {\n  stages: [\n    { duration: '2m', target: 10 },   // Ramp up to 10 users\n    { duration: '5m', target: 10 },   // Stay at 10 users\n    { duration: '2m', target: 50 },   // Ramp up to 50 users\n    { duration: '5m', target: 50 },   // Stay at 50 users\n    { duration: '2m', target: 100 },  // Ramp up to 100 users\n    { duration: '5m', target: 100 },  // Stay at 100 users\n    { duration: '5m', target: 0 },    // Ramp down to 0 users\n  ],\n  thresholds: {\n    'http_req_duration': ['p(95)<2000'], // 95% of requests under 2s\n    'http_req_failed': ['rate<0.1'],     // Error rate under 10%\n    'errors': ['rate<0.1'],              // Custom error rate under 10%\n  }\n};\n\n// Test data\nconst users = [\n  { email: 'user1@example.com', name: 'User 1' },\n  { email: 'user2@example.com', name: 'User 2' },\n  { email: 'user3@example.com', name: 'User 3' }\n];\n\nexport default function() {\n  const baseUrl = 'http://localhost:3000/api';\n  \n  // Test user creation\n  const createUserPayload = users[Math.floor(Math.random() * users.length)];\n  const createResponse = http.post(`${baseUrl}/users`, JSON.stringify(createUserPayload), {\n    headers: { 'Content-Type': 'application/json' }\n  });\n  \n  const createSuccess = check(createResponse, {\n    'user creation status is 201 or 409': (r) => r.status === 201 || r.status === 409,\n    'user creation response time < 2s': (r) => r.timings.duration < 2000,\n  });\n  \n  errorRate.add(!createSuccess);\n  responseTime.add(createResponse.timings.duration);\n  requestCount.add(1);\n  \n  if (createResponse.status === 201) {\n    const userId = JSON.parse(createResponse.body).data.id;\n    \n    // Test user retrieval\n    const getResponse = http.get(`${baseUrl}/users/${userId}`);\n    \n    const getSuccess = check(getResponse, {\n      'user retrieval status is 200': (r) => r.status === 200,\n      'user retrieval response time < 1s': (r) => r.timings.duration < 1000,\n      'user data is correct': (r) => {\n        const user = JSON.parse(r.body).data;\n        return user.email === createUserPayload.email;\n      }\n    });\n    \n    errorRate.add(!getSuccess);\n    responseTime.add(getResponse.timings.duration);\n    requestCount.add(1);\n  }\n  \n  sleep(1); // Wait 1 second between iterations\n}\n\n// Teardown function\nexport function teardown(data) {\n  console.log('Load test completed');\n  // Clean up test data if needed\n}\n```\n\n### Performance Benchmarking\n```typescript\n// testing/benchmark/micro-benchmarks.ts\nimport { performance } from 'perf_hooks';\n\nexport class PerformanceBenchmark {\n  private results: Map<string, number[]> = new Map();\n\n  // Run a function multiple times and collect statistics\n  async benchmark(\n    name: string, \n    fn: () => Promise<any> | any, \n    iterations: number = 1000\n  ): Promise<BenchmarkResult> {\n    const times: number[] = [];\n    \n    // Warm up\n    for (let i = 0; i < 10; i++) {\n      await fn();\n    }\n    \n    // Actual benchmark\n    for (let i = 0; i < iterations; i++) {\n      const start = performance.now();\n      await fn();\n      const end = performance.now();\n      times.push(end - start);\n    }\n    \n    const result = this.calculateStatistics(name, times);\n    this.results.set(name, times);\n    \n    return result;\n  }\n\n  // Compare multiple implementations\n  async compare(implementations: { [name: string]: () => Promise<any> | any }, iterations: number = 1000): Promise<void> {\n    const results: BenchmarkResult[] = [];\n    \n    for (const [name, fn] of Object.entries(implementations)) {\n      const result = await this.benchmark(name, fn, iterations);\n      results.push(result);\n    }\n    \n    // Sort by median time\n    results.sort((a, b) => a.median - b.median);\n    \n    console.log('\\n=== Performance Comparison ===');\n    results.forEach((result, index) => {\n      const fastest = results[0];\n      const slowdownFactor = result.median / fastest.median;\n      \n      console.log(`${index + 1}. ${result.name}`);\n      console.log(`   Median: ${result.median.toFixed(2)}ms`);\n      console.log(`   Mean: ${result.mean.toFixed(2)}ms`);\n      console.log(`   95th percentile: ${result.p95.toFixed(2)}ms`);\n      if (index > 0) {\n        console.log(`   ${slowdownFactor.toFixed(2)}x slower than fastest`);\n      }\n      console.log('');\n    });\n  }\n\n  private calculateStatistics(name: string, times: number[]): BenchmarkResult {\n    const sorted = times.sort((a, b) => a - b);\n    const sum = times.reduce((a, b) => a + b, 0);\n    \n    return {\n      name,\n      iterations: times.length,\n      mean: sum / times.length,\n      median: sorted[Math.floor(sorted.length / 2)],\n      min: sorted[0],\n      max: sorted[sorted.length - 1],\n      p95: sorted[Math.floor(sorted.length * 0.95)],\n      p99: sorted[Math.floor(sorted.length * 0.99)],\n      standardDeviation: this.calculateStandardDeviation(times, sum / times.length)\n    };\n  }\n\n  private calculateStandardDeviation(values: number[], mean: number): number {\n    const variance = values.reduce((acc, val) => acc + Math.pow(val - mean, 2), 0) / values.length;\n    return Math.sqrt(variance);\n  }\n\n  // Generate performance report\n  generateReport(): string {\n    let report = '# Performance Benchmark Report\\n\\n';\n    \n    for (const [name, times] of this.results.entries()) {\n      const stats = this.calculateStatistics(name, times);\n      \n      report += `## ${name}\\n`;\n      report += `- Iterations: ${stats.iterations}\\n`;\n      report += `- Mean: ${stats.mean.toFixed(2)}ms\\n`;\n      report += `- Median: ${stats.median.toFixed(2)}ms\\n`;\n      report += `- Min: ${stats.min.toFixed(2)}ms\\n`;\n      report += `- Max: ${stats.max.toFixed(2)}ms\\n`;\n      report += `- 95th percentile: ${stats.p95.toFixed(2)}ms\\n`;\n      report += `- Standard deviation: ${stats.standardDeviation.toFixed(2)}ms\\n\\n`;\n    }\n    \n    return report;\n  }\n}\n\ninterface BenchmarkResult {\n  name: string;\n  iterations: number;\n  mean: number;\n  median: number;\n  min: number;\n  max: number;\n  p95: number;\n  p99: number;\n  standardDeviation: number;\n}\n\n// Example usage\nasync function runBenchmarks() {\n  const benchmark = new PerformanceBenchmark();\n  \n  // Compare different sorting algorithms\n  const data = Array.from({ length: 1000 }, () => Math.random());\n  \n  await benchmark.compare({\n    'Native Sort': () => [...data].sort((a, b) => a - b),\n    'Bubble Sort': () => bubbleSort([...data]),\n    'Quick Sort': () => quickSort([...data])\n  }, 100);\n  \n  // Individual benchmarks\n  await benchmark.benchmark('String Concatenation', () => {\n    let result = '';\n    for (let i = 0; i < 1000; i++) {\n      result += 'test';\n    }\n    return result;\n  });\n  \n  await benchmark.benchmark('Array Join', () => {\n    const parts: string[] = [];\n    for (let i = 0; i < 1000; i++) {\n      parts.push('test');\n    }\n    return parts.join('');\n  });\n  \n  console.log(benchmark.generateReport());\n}\n\nfunction bubbleSort(arr: number[]): number[] {\n  const n = arr.length;\n  for (let i = 0; i < n - 1; i++) {\n    for (let j = 0; j < n - i - 1; j++) {\n      if (arr[j] > arr[j + 1]) {\n        [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]];\n      }\n    }\n  }\n  return arr;\n}\n\nfunction quickSort(arr: number[]): number[] {\n  if (arr.length <= 1) return arr;\n  \n  const pivot = arr[Math.floor(arr.length / 2)];\n  const left = arr.filter(x => x < pivot);\n  const middle = arr.filter(x => x === pivot);\n  const right = arr.filter(x => x > pivot);\n  \n  return [...quickSort(left), ...middle, ...quickSort(right)];\n}\n```\n\n## Monitoring and Alerting\n\n### Prometheus Configuration\n```yaml\n# monitoring/prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"performance_rules.yml\"\n\nscrape_configs:\n  - job_name: 'node-app'\n    static_configs:\n      - targets: ['localhost:3000']\n    scrape_interval: 5s\n    metrics_path: '/metrics'\n\n  - job_name: 'postgres'\n    static_configs:\n      - targets: ['localhost:5432']\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n```\n\n```yaml\n# monitoring/prometheus/performance_rules.yml\ngroups:\n  - name: performance_alerts\n    rules:\n      - alert: HighResponseTime\n        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) > 2\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High response time detected\"\n          description: \"95th percentile response time is {{ $value }}s\"\n\n      - alert: HighErrorRate\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) > 0.1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"High error rate detected\"\n          description: \"Error rate is {{ $value | humanizePercentage }}\"\n\n      - alert: DatabaseSlowQueries\n        expr: pg_stat_statements_mean_time_ms > 1000\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Slow database queries detected\"\n          description: \"Average query time is {{ $value }}ms\"\n```\n\n## Best Practices and Guidelines\n\n### Performance Engineering Workflow\n1. **Baseline Measurement**: Establish performance baselines before optimization\n2. **Profiling and Analysis**: Use appropriate profiling tools for each layer\n3. **Targeted Optimization**: Focus on the biggest bottlenecks first\n4. **Measure Impact**: Always measure the impact of optimizations\n5. **Regression Testing**: Implement performance regression testing\n6. **Continuous Monitoring**: Set up ongoing performance monitoring\n\n### Optimization Priorities\n1. **Algorithm and Data Structure**: Choose the right approach from the start\n2. **Database Optimization**: Optimize queries, indexes, and schema design\n3. **Caching Strategy**: Implement appropriate caching layers\n4. **Network Optimization**: Minimize data transfer and latency\n5. **Resource Utilization**: Optimize CPU, memory, and I/O usage\n\n### Common Performance Anti-Patterns\n- Premature optimization without profiling\n- Ignoring database query performance\n- Not implementing proper caching strategies\n- Loading unnecessary data\n- Blocking the main thread in frontend applications\n- Not using connection pooling for databases\n- Ignoring memory leaks and resource cleanup\n\n### Performance Testing Strategy\n- **Unit Performance Tests**: Test individual function performance\n- **Integration Performance Tests**: Test component interaction performance\n- **Load Testing**: Test system behavior under expected load\n- **Stress Testing**: Test system behavior under extreme load\n- **Spike Testing**: Test system behavior during traffic spikes\n- **Volume Testing**: Test system behavior with large amounts of data",
      "tags": [
        {
          "tag": {
            "id": "performance",
            "name": "performance",
            "slug": "performance"
          }
        },
        {
          "tag": {
            "id": "profiling",
            "name": "profiling",
            "slug": "profiling"
          }
        },
        {
          "tag": {
            "id": "optimization",
            "name": "optimization",
            "slug": "optimization"
          }
        },
        {
          "tag": {
            "id": "monitoring",
            "name": "monitoring",
            "slug": "monitoring"
          }
        },
        {
          "tag": {
            "id": "benchmarking",
            "name": "benchmarking",
            "slug": "benchmarking"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 45,
        "copies": 106
      },
      "_count": {
        "votes": 24,
        "copies": 202
      },
      "difficulty": "ADVANCED",
      "language": "Multiple",
      "framework": "Cross-Platform Performance",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "react-vite-typescript",
      "title": "React + Vite + TypeScript",
      "slug": "react-vite-typescript",
      "tagline": "React configuration for beginner developers",
      "description": "Modern React development setup with Vite, TypeScript, and essential tooling for fast development.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "ðŸ“‹",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - React + Vite + TypeScript Project\n\n## Project Overview\n\nThis is a modern React application built with Vite for fast development, TypeScript for type safety, and Tailwind CSS for styling.\n\n## Technology Stack\n\n- **Framework**: React 18\n- **Build Tool**: Vite\n- **Language**: TypeScript\n- **Styling**: Tailwind CSS\n- **State Management**: React hooks, Zustand/Redux Toolkit\n- **Routing**: React Router DOM\n- **Testing**: Vitest, React Testing Library\n\n## Project Structure\n\n```\nsrc/\nâ”œâ”€â”€ components/           # React components\nâ”‚   â”œâ”€â”€ ui/              # Reusable UI components\nâ”‚   â”œâ”€â”€ forms/           # Form components\nâ”‚   â””â”€â”€ layout/          # Layout components\nâ”œâ”€â”€ pages/               # Page components\nâ”œâ”€â”€ hooks/               # Custom React hooks\nâ”œâ”€â”€ store/               # State management\nâ”œâ”€â”€ utils/               # Utility functions\nâ”œâ”€â”€ types/               # TypeScript types\nâ”œâ”€â”€ styles/              # CSS and Tailwind styles\nâ””â”€â”€ main.tsx            # Application entry point\n```\n\n## Development Guidelines\n\n### Code Style\n- Use functional components with hooks\n- Implement TypeScript strict mode\n- Use Tailwind CSS for styling\n- Follow React best practices\n- Use ESLint and Prettier\n\n### Component Architecture\n- Create small, focused components\n- Use custom hooks for business logic\n- Implement proper prop typing\n- Use React.memo for performance optimization\n\n### State Management\n- Use React hooks for local state\n- Use Zustand or Redux Toolkit for global state\n- Implement proper data fetching patterns\n- Use React Query for server state\n\n## Key Commands\n\n- `npm run dev` - Start development server\n- `npm run build` - Build for production\n- `npm run preview` - Preview production build\n- `npm run test` - Run tests\n- `npm run lint` - Run linter\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nVITE_API_URL=http://localhost:8000/api\nVITE_APP_TITLE=My React App\n```\n\n## Common Patterns\n\n### Component with TypeScript\n```tsx\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary';\n  children: React.ReactNode;\n  onClick?: () => void;\n}\n\nexport const Button: React.FC<ButtonProps> = ({ \n  variant = 'primary', \n  children, \n  onClick \n}) => {\n  return (\n    <button\n      className={`btn btn-${variant}`}\n      onClick={onClick}\n    >\n      {children}\n    </button>\n  );\n};\n```\n\n### Custom Hook\n```tsx\nimport { useState, useEffect } from 'react';\n\nexport function useApi<T>(url: string) {\n  const [data, setData] = useState<T | null>(null);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState<string | null>(null);\n\n  useEffect(() => {\n    fetch(url)\n      .then(res => res.json())\n      .then(setData)\n      .catch(err => setError(err.message))\n      .finally(() => setLoading(false));\n  }, [url]);\n\n  return { data, loading, error };\n}\n```\n\n### Route Configuration\n```tsx\nimport { createBrowserRouter } from 'react-router-dom';\nimport { HomePage, AboutPage, ContactPage } from './pages';\n\nexport const router = createBrowserRouter([\n  {\n    path: '/',\n    element: <HomePage />\n  },\n  {\n    path: '/about',\n    element: <AboutPage />\n  },\n  {\n    path: '/contact',\n    element: <ContactPage />\n  }\n]);\n```\n\n## Performance Tips\n\n- Use React.lazy for code splitting\n- Implement virtual scrolling for large lists\n- Optimize bundle size with tree shaking\n- Use React DevTools for debugging\n\n## Testing\n\n- Write unit tests for components\n- Test custom hooks\n- Use MSW for API mocking\n- Implement integration tests\n\n## Deployment\n\n- Build optimized bundle with `npm run build`\n- Deploy to Netlify, Vercel, or similar\n- Configure environment variables\n- Set up CI/CD pipeline",
      "tags": [
        {
          "tag": {
            "id": "react",
            "name": "react",
            "slug": "react"
          }
        },
        {
          "tag": {
            "id": "vite",
            "name": "vite",
            "slug": "vite"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "tailwind",
            "name": "tailwind",
            "slug": "tailwind"
          }
        },
        {
          "tag": {
            "id": "spa",
            "name": "spa",
            "slug": "spa"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 43,
        "copies": 177
      },
      "_count": {
        "votes": 55,
        "copies": 96
      },
      "difficulty": "BEGINNER",
      "language": "TypeScript",
      "framework": "React",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "frontend-architecture-planner",
      "title": "Frontend Architecture & Component Design",
      "slug": "frontend-architecture-planner",
      "tagline": "Expert prompt templates prompt template",
      "description": "Comprehensive prompt for designing scalable frontend architectures with component systems and state management.",
      "categoryId": "prompt-templates",
      "category": {
        "id": "prompt-templates",
        "name": "Prompt Templates",
        "slug": "prompts",
        "description": "Carefully crafted prompt templates for common development tasks and workflows, featuring expert-level architectural and optimization guidance.",
        "icon": "ðŸ’¬",
        "color": "#10B981"
      },
      "type": "PROMPT_TEMPLATE",
      "content": "You are a senior frontend architect specializing in modern web applications, component systems, and scalable frontend architectures.\n\n## Project Context\n**Application Type:** {{APPLICATION_TYPE}}\n**Framework:** {{FRAMEWORK}}\n**Complexity:** {{COMPLEXITY_LEVEL}}\n**Team Size:** {{TEAM_SIZE}}\n**Timeline:** {{TIMELINE}}\n\n## Requirements\n{{REQUIREMENTS}}\n\n## Design a comprehensive frontend architecture with the following considerations:\n\n### 1. Architecture Overview\n- Define overall application structure and layers\n- Choose appropriate architectural patterns (MVC, Component-based, Micro-frontends)\n- Plan for scalability and maintainability\n- Consider development team structure and workflow\n\n### 2. Component System Design\n- Design reusable component hierarchy\n- Create component categorization (Atoms, Molecules, Organisms)\n- Plan for component composition and prop interfaces\n- Design consistent component API patterns\n- Plan for component testing strategies\n\n### 3. State Management Strategy\n- Choose appropriate state management solution\n- Design global state structure\n- Plan for local component state\n- Design data flow patterns (unidirectional/bidirectional)\n- Handle asynchronous state (loading, error states)\n\n### 4. Routing & Navigation\n- Design application routing structure\n- Plan for nested routes and route parameters\n- Implement navigation guards and access control\n- Handle deep linking and browser history\n- Design for SEO and social sharing\n\n### 5. Data Layer & API Integration\n- Design API service layer and data fetching patterns\n- Plan for caching and data synchronization\n- Handle optimistic updates and conflict resolution\n- Design error handling and retry mechanisms\n- Plan for offline capabilities\n\n### 6. Performance Optimization\n- Plan for code splitting and lazy loading\n- Design bundle optimization strategy\n- Implement performance monitoring\n- Plan for image and asset optimization\n- Consider server-side rendering (SSR) needs\n\n### 7. Development Experience\n- Set up development tooling and build process\n- Plan for hot reloading and development server\n- Design component documentation system\n- Set up testing infrastructure (unit, integration, e2e)\n- Plan for code quality tools (linting, formatting)\n\n### 8. Accessibility & UX\n- Design for accessibility standards (WCAG)\n- Plan for responsive design and mobile experience\n- Design loading states and error boundaries\n- Plan for internationalization (i18n)\n- Consider user preferences and theming\n\n## Output Format:\nProvide a detailed frontend architecture including:\n\n**1. Architecture Diagram (textual):**\n```\nâ”Œâ”€ Presentation Layer â”€â”\nâ”‚ Components & Views   â”‚\nâ”œâ”€ Business Logic â”€â”€â”€â”€â”€â”¤\nâ”‚ Services & Stores    â”‚\nâ”œâ”€ Data Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ API & Persistence    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**2. Component Structure:**\n```\nsrc/\nâ”œâ”€â”€ components/\nâ”‚   â”œâ”€â”€ ui/           # Base components\nâ”‚   â”œâ”€â”€ forms/        # Form components\nâ”‚   â””â”€â”€ layout/       # Layout components\nâ”œâ”€â”€ pages/            # Route components\nâ”œâ”€â”€ services/         # API services\nâ”œâ”€â”€ stores/           # State management\nâ””â”€â”€ utils/            # Utilities\n```\n\n**3. State Management Design:**\n- Global state schema\n- State update patterns\n- Side effects handling\n\n**4. Component Examples:**\n- Base component interfaces\n- Composition examples\n- State integration patterns\n\n**5. Implementation Roadmap:**\n- Phase-by-phase development plan\n- Critical path identification\n- Risk mitigation strategies\n\nInclude specific code examples and consider modern best practices.",
      "tags": [
        {
          "tag": {
            "id": "frontend",
            "name": "frontend",
            "slug": "frontend"
          }
        },
        {
          "tag": {
            "id": "architecture",
            "name": "architecture",
            "slug": "architecture"
          }
        },
        {
          "tag": {
            "id": "components",
            "name": "components",
            "slug": "components"
          }
        },
        {
          "tag": {
            "id": "state-management",
            "name": "state-management",
            "slug": "state-management"
          }
        },
        {
          "tag": {
            "id": "design-system",
            "name": "design-system",
            "slug": "design-system"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 36,
        "copies": 354
      },
      "_count": {
        "votes": 59,
        "copies": 206
      },
      "difficulty": "INTERMEDIATE",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "neovim-lsp-ultimate",
      "title": "Neovim LSP Ultimate Setup",
      "slug": "neovim-lsp-ultimate-configuration",
      "tagline": "Neovim + LSP configuration for advanced developers",
      "description": "Comprehensive Neovim configuration with LSP, Treesitter, Telescope, and advanced productivity features for power users and developers seeking the ultimate text editing experience.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "ðŸ“‹",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Neovim LSP Ultimate Setup\n\n## Project Overview\n\nThis is an advanced Neovim configuration designed for power users and developers who demand the ultimate text editing experience. It features comprehensive LSP support, advanced Treesitter parsing, fuzzy finding with Telescope, and a highly optimized workflow for maximum productivity.\n\n## Development Philosophy\n\n### Neovim Advantages\n1. **Performance**: Lightning-fast startup and operation\n2. **Extensibility**: Lua-based configuration and plugins\n3. **LSP Integration**: Native Language Server Protocol support\n4. **Asynchronous**: Non-blocking operations for smooth experience\n5. **Customization**: Complete control over every aspect\n6. **Vim Compatibility**: All Vim knowledge transfers\n\n### Advanced Features\n- Multi-language LSP with zero-config setup\n- Intelligent code completion and diagnostics\n- Advanced syntax highlighting with Treesitter\n- Fuzzy finding for files, buffers, and symbols\n- Git integration with signs and blame\n- File explorer with icons and previews\n- Terminal integration and floating windows\n- Session management and workspace persistence\n- Custom keybindings and leader key shortcuts\n\n## Technology Stack\n\n- **Editor**: Neovim 0.9+\n- **Configuration Language**: Lua\n- **Plugin Manager**: lazy.nvim\n- **LSP**: Mason + nvim-lspconfig\n- **Completion**: nvim-cmp with multiple sources\n- **Syntax**: nvim-treesitter\n- **Fuzzy Finding**: telescope.nvim\n- **File Explorer**: nvim-tree.lua\n- **Git Integration**: gitsigns.nvim\n- **Status Line**: lualine.nvim\n\n## Project Structure\n\n```\n~/.config/nvim/\nâ”œâ”€â”€ init.lua                    # Main configuration entry point\nâ”œâ”€â”€ lua/\nâ”‚   â”œâ”€â”€ core/                   # Core Neovim settings\nâ”‚   â”‚   â”œâ”€â”€ options.lua         # Vim options and settings\nâ”‚   â”‚   â”œâ”€â”€ keymaps.lua         # Key mappings and shortcuts\nâ”‚   â”‚   â”œâ”€â”€ autocmds.lua        # Auto commands and events\nâ”‚   â”‚   â””â”€â”€ utils.lua           # Utility functions\nâ”‚   â”œâ”€â”€ plugins/                # Plugin configurations\nâ”‚   â”‚   â”œâ”€â”€ init.lua            # Plugin manager setup\nâ”‚   â”‚   â”œâ”€â”€ lsp/                # LSP configurations\nâ”‚   â”‚   â”‚   â”œâ”€â”€ mason.lua       # LSP installer\nâ”‚   â”‚   â”‚   â”œâ”€â”€ lspconfig.lua   # LSP server configs\nâ”‚   â”‚   â”‚   â”œâ”€â”€ handlers.lua    # LSP handlers\nâ”‚   â”‚   â”‚   â””â”€â”€ servers/        # Individual server configs\nâ”‚   â”‚   â”œâ”€â”€ completion/         # Completion setup\nâ”‚   â”‚   â”‚   â”œâ”€â”€ nvim-cmp.lua    # Main completion engine\nâ”‚   â”‚   â”‚   â””â”€â”€ snippets.lua    # Snippet configurations\nâ”‚   â”‚   â”œâ”€â”€ ui/                 # UI enhancements\nâ”‚   â”‚   â”‚   â”œâ”€â”€ telescope.lua   # Fuzzy finder\nâ”‚   â”‚   â”‚   â”œâ”€â”€ nvim-tree.lua   # File explorer\nâ”‚   â”‚   â”‚   â”œâ”€â”€ lualine.lua     # Status line\nâ”‚   â”‚   â”‚   â””â”€â”€ bufferline.lua  # Buffer tabs\nâ”‚   â”‚   â”œâ”€â”€ editor/             # Editor enhancements\nâ”‚   â”‚   â”‚   â”œâ”€â”€ treesitter.lua  # Syntax highlighting\nâ”‚   â”‚   â”‚   â”œâ”€â”€ gitsigns.lua    # Git integration\nâ”‚   â”‚   â”‚   â”œâ”€â”€ comment.lua     # Smart commenting\nâ”‚   â”‚   â”‚   â””â”€â”€ autopairs.lua   # Auto bracket pairing\nâ”‚   â”‚   â””â”€â”€ tools/              # Development tools\nâ”‚   â”‚       â”œâ”€â”€ terminal.lua    # Terminal integration\nâ”‚   â”‚       â”œâ”€â”€ debugger.lua    # DAP debugging\nâ”‚   â”‚       â””â”€â”€ testing.lua     # Test integration\nâ”‚   â””â”€â”€ themes/                 # Color schemes\nâ”‚       â”œâ”€â”€ catppuccin.lua      # Catppuccin theme\nâ”‚       â””â”€â”€ tokyonight.lua      # Tokyo Night theme\nâ””â”€â”€ snippets/                   # Custom snippets\n    â”œâ”€â”€ typescript.json         # TypeScript snippets\n    â”œâ”€â”€ python.json             # Python snippets\n    â””â”€â”€ go.json                 # Go snippets\n```\n\n## Core Configuration\n\n### Main Init File\n```lua\n-- ~/.config/nvim/init.lua\n-- Bootstrap lazy.nvim plugin manager\nlocal lazypath = vim.fn.stdpath(\"data\") .. \"/lazy/lazy.nvim\"\nif not vim.loop.fs_stat(lazypath) then\n  vim.fn.system({\n    \"git\",\n    \"clone\",\n    \"--filter=blob:none\",\n    \"https://github.com/folke/lazy.nvim.git\",\n    \"--branch=stable\",\n    lazypath,\n  })\nend\nvim.opt.rtp:prepend(lazypath)\n\n-- Load core configuration\nrequire(\"core.options\")\nrequire(\"core.keymaps\")\nrequire(\"core.autocmds\")\n\n-- Setup plugins\nrequire(\"plugins\")\n\n-- Set colorscheme\nvim.cmd.colorscheme(\"catppuccin-mocha\")\n```\n\n### Core Options\n```lua\n-- ~/.config/nvim/lua/core/options.lua\nlocal opt = vim.opt\n\n-- General\nopt.mouse = \"a\"                 -- enable mouse support\nopt.clipboard = \"unnamedplus\"   -- use system clipboard\nopt.swapfile = false           -- disable swap files\nopt.completeopt = \"menu,menuone,noselect\"\n\n-- UI\nopt.number = true              -- show line numbers\nopt.relativenumber = true      -- show relative line numbers\nopt.cursorline = true          -- highlight current line\nopt.termguicolors = true       -- true color support\nopt.background = \"dark\"        -- dark background\nopt.signcolumn = \"yes\"         -- always show sign column\nopt.cmdheight = 1             -- command line height\nopt.scrolloff = 8             -- keep 8 lines above/below cursor\nopt.sidescrolloff = 8         -- keep 8 columns left/right of cursor\n\n-- Splitting\nopt.splitright = true         -- vertical splits to the right\nopt.splitbelow = true         -- horizontal splits below\n\n-- Search\nopt.ignorecase = true         -- ignore case in search\nopt.smartcase = true          -- case sensitive if uppercase present\nopt.hlsearch = false          -- don't highlight search results\nopt.incsearch = true          -- incremental search\n\n-- Indentation\nopt.expandtab = true          -- use spaces instead of tabs\nopt.shiftwidth = 2            -- shift 2 spaces when tab\nopt.tabstop = 2               -- 1 tab == 2 spaces\nopt.softtabstop = 2           -- 2 spaces for editing\nopt.smartindent = true        -- autoindent new lines\n\n-- Performance\nopt.updatetime = 250          -- faster completion (4000ms default)\nopt.timeoutlen = 500         -- time to wait for mapped sequence\nopt.redrawtime = 10000       -- allow more time for loading syntax on large files\nopt.synmaxcol = 180          -- max column for syntax highlight\n\n-- Backup and undo\nopt.backup = false            -- don't create backup files\nopt.writebackup = false       -- don't create backup before overwriting\nopt.undofile = true           -- persistent undo\nopt.undodir = vim.fn.expand(\"~/.config/nvim/undo\")\n\n-- Create undo directory if it doesn't exist\nlocal undo_dir = vim.fn.expand(\"~/.config/nvim/undo\")\nif vim.fn.isdirectory(undo_dir) == 0 then\n  vim.fn.mkdir(undo_dir, \"p\")\nend\n\n-- Folding\nopt.foldmethod = \"expr\"\nopt.foldexpr = \"nvim_treesitter#foldexpr()\"\nopt.foldenable = false        -- don't fold by default\n\n-- Window\nopt.winwidth = 30\nopt.winminwidth = 10\nopt.equalalways = false\n\n-- Wild menu\nopt.wildmenu = true\nopt.wildmode = \"longest:full,full\"\nopt.wildignore:append({ \"*.o\", \"*.obj\", \".git\", \"*.rbc\", \"*.pyc\", \"__pycache__\" })\n\n-- Neovim specific\nif vim.fn.has(\"nvim-0.8\") == 1 then\n  opt.backup = false\n  opt.cmdheight = 0\n  opt.laststatus = 3\nend\n```\n\n### Key Mappings\n```lua\n-- ~/.config/nvim/lua/core/keymaps.lua\nlocal keymap = vim.keymap.set\nlocal opts = { noremap = true, silent = true }\n\n-- Set leader key\nvim.g.mapleader = \" \"\nvim.g.maplocalleader = \" \"\n\n-- General keymaps\nkeymap(\"n\", \"<leader>w\", \":w<CR>\", opts)\nkeymap(\"n\", \"<leader>q\", \":q<CR>\", opts)\nkeymap(\"n\", \"<leader>x\", \":x<CR>\", opts)\n\n-- Better window navigation\nkeymap(\"n\", \"<C-h>\", \"<C-w>h\", opts)\nkeymap(\"n\", \"<C-j>\", \"<C-w>j\", opts)\nkeymap(\"n\", \"<C-k>\", \"<C-w>k\", opts)\nkeymap(\"n\", \"<C-l>\", \"<C-w>l\", opts)\n\n-- Resize windows\nkeymap(\"n\", \"<C-Up>\", \":resize -2<CR>\", opts)\nkeymap(\"n\", \"<C-Down>\", \":resize +2<CR>\", opts)\nkeymap(\"n\", \"<C-Left>\", \":vertical resize -2<CR>\", opts)\nkeymap(\"n\", \"<C-Right>\", \":vertical resize +2<CR>\", opts)\n\n-- Buffer navigation\nkeymap(\"n\", \"<S-l>\", \":bnext<CR>\", opts)\nkeymap(\"n\", \"<S-h>\", \":bprevious<CR>\", opts)\nkeymap(\"n\", \"<leader>bd\", \":bdelete<CR>\", opts)\n\n-- Better indenting\nkeymap(\"v\", \"<\", \"<gv\", opts)\nkeymap(\"v\", \">\", \">gv\", opts)\n\n-- Move text up and down\nkeymap(\"v\", \"<A-j>\", \":m .+1<CR>==\", opts)\nkeymap(\"v\", \"<A-k>\", \":m .-2<CR>==\", opts)\nkeymap(\"v\", \"p\", '\"_dP', opts)\n\n-- Visual Block mode\nkeymap(\"x\", \"J\", \":move '>+1<CR>gv-gv\", opts)\nkeymap(\"x\", \"K\", \":move '<-2<CR>gv-gv\", opts)\nkeymap(\"x\", \"<A-j>\", \":move '>+1<CR>gv-gv\", opts)\nkeymap(\"x\", \"<A-k>\", \":move '<-2<CR>gv-gv\", opts)\n\n-- Stay in the center\nkeymap(\"n\", \"<C-d>\", \"<C-d>zz\", opts)\nkeymap(\"n\", \"<C-u>\", \"<C-u>zz\", opts)\nkeymap(\"n\", \"n\", \"nzzzv\", opts)\nkeymap(\"n\", \"N\", \"Nzzzv\", opts)\n\n-- Clear search highlighting\nkeymap(\"n\", \"<leader>h\", \":nohlsearch<CR>\", opts)\n\n-- Quick fix list\nkeymap(\"n\", \"<C-k>\", \"<cmd>cnext<CR>zz\", opts)\nkeymap(\"n\", \"<C-j>\", \"<cmd>cprev<CR>zz\", opts)\nkeymap(\"n\", \"<leader>k\", \"<cmd>lnext<CR>zz\", opts)\nkeymap(\"n\", \"<leader>j\", \"<cmd>lprev<CR>zz\", opts)\n\n-- Telescope\nkeymap(\"n\", \"<leader>ff\", \"<cmd>Telescope find_files<cr>\", opts)\nkeymap(\"n\", \"<leader>fg\", \"<cmd>Telescope live_grep<cr>\", opts)\nkeymap(\"n\", \"<leader>fb\", \"<cmd>Telescope buffers<cr>\", opts)\nkeymap(\"n\", \"<leader>fh\", \"<cmd>Telescope help_tags<cr>\", opts)\nkeymap(\"n\", \"<leader>fs\", \"<cmd>Telescope lsp_document_symbols<cr>\", opts)\nkeymap(\"n\", \"<leader>fr\", \"<cmd>Telescope lsp_references<cr>\", opts)\n\n-- LSP\nkeymap(\"n\", \"gD\", vim.lsp.buf.declaration, opts)\nkeymap(\"n\", \"gd\", vim.lsp.buf.definition, opts)\nkeymap(\"n\", \"K\", vim.lsp.buf.hover, opts)\nkeymap(\"n\", \"gi\", vim.lsp.buf.implementation, opts)\nkeymap(\"n\", \"<C-k>\", vim.lsp.buf.signature_help, opts)\nkeymap(\"n\", \"<leader>rn\", vim.lsp.buf.rename, opts)\nkeymap(\"n\", \"<leader>ca\", vim.lsp.buf.code_action, opts)\nkeymap(\"n\", \"gr\", vim.lsp.buf.references, opts)\nkeymap(\"n\", \"<leader>f\", function()\n  vim.lsp.buf.format({ async = true })\nend, opts)\n\n-- Diagnostics\nkeymap(\"n\", \"<leader>e\", vim.diagnostic.open_float, opts)\nkeymap(\"n\", \"[d\", vim.diagnostic.goto_prev, opts)\nkeymap(\"n\", \"]d\", vim.diagnostic.goto_next, opts)\nkeymap(\"n\", \"<leader>dl\", vim.diagnostic.setloclist, opts)\n\n-- NvimTree\nkeymap(\"n\", \"<leader>e\", \":NvimTreeToggle<CR>\", opts)\n\n-- Terminal\nkeymap(\"n\", \"<leader>t\", \":ToggleTerm<CR>\", opts)\nkeymap(\"t\", \"<esc>\", [[<C-\\><C-n>]], opts)\n\n-- Git\nkeymap(\"n\", \"<leader>gg\", \":LazyGit<CR>\", opts)\nkeymap(\"n\", \"<leader>gb\", \":Gitsigns blame_line<CR>\", opts)\nkeymap(\"n\", \"<leader>gp\", \":Gitsigns preview_hunk<CR>\", opts)\nkeymap(\"n\", \"<leader>gr\", \":Gitsigns reset_hunk<CR>\", opts)\nkeymap(\"n\", \"<leader>gs\", \":Gitsigns stage_hunk<CR>\", opts)\n\n-- Sessions\nkeymap(\"n\", \"<leader>ss\", \":SessionSave<CR>\", opts)\nkeymap(\"n\", \"<leader>sr\", \":SessionRestore<CR>\", opts)\n```\n\n## Plugin Configuration\n\n### Plugin Manager Setup\n```lua\n-- ~/.config/nvim/lua/plugins/init.lua\nrequire(\"lazy\").setup({\n  -- LSP Configuration\n  { import = \"plugins.lsp\" },\n  \n  -- Completion\n  { import = \"plugins.completion\" },\n  \n  -- UI Enhancements\n  { import = \"plugins.ui\" },\n  \n  -- Editor Features\n  { import = \"plugins.editor\" },\n  \n  -- Development Tools\n  { import = \"plugins.tools\" },\n  \n  -- Themes\n  { import = \"themes\" },\n}, {\n  checker = {\n    enabled = true,\n    notify = false,\n  },\n  change_detection = {\n    notify = false,\n  },\n})\n```\n\n### LSP Configuration\n```lua\n-- ~/.config/nvim/lua/plugins/lsp/mason.lua\nreturn {\n  {\n    \"williamboman/mason.nvim\",\n    cmd = \"Mason\",\n    keys = { { \"<leader>cm\", \"<cmd>Mason<cr>\", desc = \"Mason\" } },\n    build = \":MasonUpdate\",\n    opts = {\n      ensure_installed = {\n        \"stylua\",\n        \"shellcheck\",\n        \"shfmt\",\n        \"flake8\",\n        \"black\",\n        \"isort\",\n        \"prettier\",\n        \"eslint_d\",\n        \"typescript-language-server\",\n        \"pyright\",\n        \"lua-language-server\",\n        \"gopls\",\n        \"rust-analyzer\",\n        \"json-lsp\",\n        \"yaml-language-server\",\n        \"dockerfile-language-server\",\n        \"bash-language-server\",\n        \"html-lsp\",\n        \"css-lsp\",\n        \"tailwindcss-language-server\",\n      },\n    },\n    config = function(_, opts)\n      require(\"mason\").setup(opts)\n      local mr = require(\"mason-registry\")\n      mr:on(\"package:install:success\", function()\n        vim.defer_fn(function()\n          require(\"lazy.core.handler.event\").trigger({\n            event = \"FileType\",\n            buf = vim.api.nvim_get_current_buf(),\n          })\n        end, 100)\n      end)\n      local function ensure_installed()\n        for _, tool in ipairs(opts.ensure_installed) do\n          local p = mr.get_package(tool)\n          if not p:is_installed() then\n            p:install()\n          end\n        end\n      end\n      if mr.refresh then\n        mr.refresh(ensure_installed)\n      else\n        ensure_installed()\n      end\n    end,\n  },\n  \n  {\n    \"williamboman/mason-lspconfig.nvim\",\n    dependencies = { \"mason.nvim\" },\n    opts = {\n      automatic_installation = true,\n    },\n  },\n}\n```\n\n### Advanced LSP Handlers\n```lua\n-- ~/.config/nvim/lua/plugins/lsp/handlers.lua\nlocal M = {}\n\nM.setup = function()\n  local signs = {\n    { name = \"DiagnosticSignError\", text = \"\" },\n    { name = \"DiagnosticSignWarn\", text = \"\" },\n    { name = \"DiagnosticSignHint\", text = \"\" },\n    { name = \"DiagnosticSignInfo\", text = \"\" },\n  }\n\n  for _, sign in ipairs(signs) do\n    vim.fn.sign_define(sign.name, { texthl = sign.name, text = sign.text, numhl = \"\" })\n  end\n\n  vim.diagnostic.config({\n    virtual_text = {\n      prefix = \"â—\",\n      source = \"if_many\",\n    },\n    signs = true,\n    update_in_insert = false,\n    underline = true,\n    severity_sort = true,\n    float = {\n      focusable = true,\n      style = \"minimal\",\n      border = \"rounded\",\n      source = \"always\",\n      header = \"\",\n      prefix = \"\",\n    },\n  })\n\n  vim.lsp.handlers[\"textDocument/hover\"] = vim.lsp.with(vim.lsp.handlers.hover, {\n    border = \"rounded\",\n    width = 60,\n  })\n\n  vim.lsp.handlers[\"textDocument/signatureHelp\"] = vim.lsp.with(vim.lsp.handlers.signature_help, {\n    border = \"rounded\",\n    width = 60,\n  })\nend\n\nlocal function lsp_keymaps(bufnr)\n  local opts = { noremap = true, silent = true }\n  local keymap = vim.api.nvim_buf_set_keymap\n  keymap(bufnr, \"n\", \"gD\", \"<cmd>lua vim.lsp.buf.declaration()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gd\", \"<cmd>lua vim.lsp.buf.definition()<CR>\", opts)\n  keymap(bufnr, \"n\", \"K\", \"<cmd>lua vim.lsp.buf.hover()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gI\", \"<cmd>lua vim.lsp.buf.implementation()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gr\", \"<cmd>lua vim.lsp.buf.references()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gl\", \"<cmd>lua vim.diagnostic.open_float()<CR>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lf\", \"<cmd>lua vim.lsp.buf.format{ async = true }<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>li\", \"<cmd>LspInfo<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lI\", \"<cmd>LspInstallInfo<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>la\", \"<cmd>lua vim.lsp.buf.code_action()<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lj\", \"<cmd>lua vim.diagnostic.goto_next({buffer=0})<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lk\", \"<cmd>lua vim.diagnostic.goto_prev({buffer=0})<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lr\", \"<cmd>lua vim.lsp.buf.rename()<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>ls\", \"<cmd>lua vim.lsp.buf.signature_help()<CR>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lq\", \"<cmd>lua vim.diagnostic.setloclist()<CR>\", opts)\nend\n\nM.on_attach = function(client, bufnr)\n  lsp_keymaps(bufnr)\n  \n  if client.supports_method(\"textDocument/documentHighlight\") then\n    vim.api.nvim_create_augroup(\"lsp_document_highlight\", {})\n    vim.api.nvim_create_autocmd({ \"CursorHold\", \"CursorHoldI\" }, {\n      group = \"lsp_document_highlight\",\n      buffer = bufnr,\n      callback = vim.lsp.buf.document_highlight,\n    })\n    vim.api.nvim_create_autocmd(\"CursorMoved\", {\n      group = \"lsp_document_highlight\",\n      buffer = bufnr,\n      callback = vim.lsp.buf.clear_references,\n    })\n  end\n\n  if client.supports_method(\"textDocument/inlayHint\") then\n    vim.lsp.inlay_hint.enable(bufnr, true)\n  end\nend\n\nlocal capabilities = vim.lsp.protocol.make_client_capabilities()\ncapabilities.textDocument.completion.completionItem.snippetSupport = true\ncapabilities.textDocument.completion.completionItem.resolveSupport = {\n  properties = {\n    \"documentation\",\n    \"detail\",\n    \"additionalTextEdits\",\n  },\n}\ncapabilities.textDocument.foldingRange = {\n  dynamicRegistration = false,\n  lineFoldingOnly = true,\n}\n\nlocal status_ok, cmp_nvim_lsp = pcall(require, \"cmp_nvim_lsp\")\nif status_ok then\n  capabilities = cmp_nvim_lsp.default_capabilities(capabilities)\nend\n\nM.capabilities = capabilities\n\nreturn M\n```\n\n### Telescope Configuration\n```lua\n-- ~/.config/nvim/lua/plugins/ui/telescope.lua\nreturn {\n  \"nvim-telescope/telescope.nvim\",\n  tag = \"0.1.4\",\n  dependencies = {\n    \"nvim-lua/plenary.nvim\",\n    { \"nvim-telescope/telescope-fzf-native.nvim\", build = \"make\" },\n    \"nvim-telescope/telescope-ui-select.nvim\",\n    \"nvim-tree/nvim-web-devicons\",\n  },\n  keys = {\n    { \"<leader>ff\", \"<cmd>Telescope find_files<cr>\", desc = \"Find Files\" },\n    { \"<leader>fg\", \"<cmd>Telescope live_grep<cr>\", desc = \"Live Grep\" },\n    { \"<leader>fb\", \"<cmd>Telescope buffers<cr>\", desc = \"Buffers\" },\n    { \"<leader>fh\", \"<cmd>Telescope help_tags<cr>\", desc = \"Help Tags\" },\n    { \"<leader>fs\", \"<cmd>Telescope lsp_document_symbols<cr>\", desc = \"Document Symbols\" },\n    { \"<leader>fr\", \"<cmd>Telescope lsp_references<cr>\", desc = \"References\" },\n    { \"<leader>fd\", \"<cmd>Telescope diagnostics<cr>\", desc = \"Diagnostics\" },\n    { \"<leader>fc\", \"<cmd>Telescope commands<cr>\", desc = \"Commands\" },\n    { \"<leader>fk\", \"<cmd>Telescope keymaps<cr>\", desc = \"Keymaps\" },\n  },\n  config = function()\n    local telescope = require(\"telescope\")\n    local actions = require(\"telescope.actions\")\n\n    telescope.setup({\n      defaults = {\n        prompt_prefix = \" \",\n        selection_caret = \" \",\n        path_display = { \"truncate\" },\n        file_ignore_patterns = { \".git/\", \"node_modules\" },\n        \n        mappings = {\n          i = {\n            [\"<C-n>\"] = actions.cycle_history_next,\n            [\"<C-p>\"] = actions.cycle_history_prev,\n            [\"<C-j>\"] = actions.move_selection_next,\n            [\"<C-k>\"] = actions.move_selection_previous,\n            [\"<CR>\"] = actions.select_default,\n            [\"<C-x>\"] = actions.select_horizontal,\n            [\"<C-v>\"] = actions.select_vertical,\n            [\"<C-t>\"] = actions.select_tab,\n            [\"<C-u>\"] = actions.preview_scrolling_up,\n            [\"<C-d>\"] = actions.preview_scrolling_down,\n            [\"<PageUp>\"] = actions.results_scrolling_up,\n            [\"<PageDown>\"] = actions.results_scrolling_down,\n            [\"<Tab>\"] = actions.toggle_selection + actions.move_selection_worse,\n            [\"<S-Tab>\"] = actions.toggle_selection + actions.move_selection_better,\n            [\"<C-q>\"] = actions.send_to_qflist + actions.open_qflist,\n            [\"<M-q>\"] = actions.send_selected_to_qflist + actions.open_qflist,\n            [\"<C-l>\"] = actions.complete_tag,\n            [\"<C-_>\"] = actions.which_key,\n          },\n          n = {\n            [\"<esc>\"] = actions.close,\n            [\"<CR>\"] = actions.select_default,\n            [\"<C-x>\"] = actions.select_horizontal,\n            [\"<C-v>\"] = actions.select_vertical,\n            [\"<C-t>\"] = actions.select_tab,\n            [\"<Tab>\"] = actions.toggle_selection + actions.move_selection_worse,\n            [\"<S-Tab>\"] = actions.toggle_selection + actions.move_selection_better,\n            [\"<C-q>\"] = actions.send_to_qflist + actions.open_qflist,\n            [\"<M-q>\"] = actions.send_selected_to_qflist + actions.open_qflist,\n            [\"j\"] = actions.move_selection_next,\n            [\"k\"] = actions.move_selection_previous,\n            [\"H\"] = actions.move_to_top,\n            [\"M\"] = actions.move_to_middle,\n            [\"L\"] = actions.move_to_bottom,\n            [\"<Down>\"] = actions.move_selection_next,\n            [\"<Up>\"] = actions.move_selection_previous,\n            [\"gg\"] = actions.move_to_top,\n            [\"G\"] = actions.move_to_bottom,\n            [\"<C-u>\"] = actions.preview_scrolling_up,\n            [\"<C-d>\"] = actions.preview_scrolling_down,\n            [\"<PageUp>\"] = actions.results_scrolling_up,\n            [\"<PageDown>\"] = actions.results_scrolling_down,\n            [\"?\"] = actions.which_key,\n          },\n        },\n      },\n      \n      pickers = {\n        find_files = {\n          theme = \"dropdown\",\n          previewer = false,\n          hidden = true,\n        },\n        live_grep = {\n          theme = \"dropdown\",\n        },\n        buffers = {\n          theme = \"dropdown\",\n          previewer = false,\n          initial_mode = \"normal\",\n        },\n      },\n      \n      extensions = {\n        fzf = {\n          fuzzy = true,\n          override_generic_sorter = true,\n          override_file_sorter = true,\n          case_mode = \"smart_case\",\n        },\n        [\"ui-select\"] = {\n          require(\"telescope.themes\").get_dropdown({}),\n        },\n      },\n    })\n\n    telescope.load_extension(\"fzf\")\n    telescope.load_extension(\"ui-select\")\n  end,\n}\n```\n\n## Advanced Features\n\n### Treesitter Configuration\n- **Syntax Highlighting**: Advanced syntax highlighting for 40+ languages\n- **Code Folding**: Smart folding based on syntax structure\n- **Text Objects**: Custom text objects for functions, classes, etc.\n- **Incremental Selection**: Smart selection expansion\n- **Context**: Show current function/class in status line\n\n### Completion System\n- **Sources**: LSP, buffer, path, snippets, emoji\n- **Intelligent Ranking**: Context-aware completion ranking\n- **Snippet Support**: LuaSnip integration with custom snippets\n- **Auto-imports**: Automatic import statements\n- **Documentation**: Inline documentation in completion menu\n\n### Git Integration\n- **Signs**: Line-by-line git status indicators\n- **Blame**: Inline git blame information\n- **Hunks**: Stage, unstage, and preview hunks\n- **Branches**: Branch switching and management\n- **LazyGit**: Full-featured git interface\n\n### Terminal Integration\n- **Floating Terminal**: Toggle floating terminal\n- **Multiple Terminals**: Named terminal instances\n- **Persistent Terminals**: Terminals survive session restarts\n- **Send to Terminal**: Send code selections to terminal\n\n## Performance Optimizations\n\n### Startup Time\n- **Lazy Loading**: Plugins load only when needed\n- **Compiled Loader**: Faster module loading\n- **Minimal Core**: Only essential plugins at startup\n- **Cached Modules**: Module caching for repeated loads\n\n### Runtime Performance\n- **Treesitter**: Efficient syntax highlighting\n- **LSP Optimizations**: Debounced diagnostics and formatting\n- **Buffer Management**: Automatic buffer cleanup\n- **Memory Usage**: Optimized memory consumption\n\n### File Handling\n- **Large Files**: Special handling for files > 1MB\n- **Binary Files**: Automatic detection and handling\n- **Encoding**: Proper UTF-8 and multi-byte support\n- **Line Endings**: Cross-platform line ending handling\n\n## Customization Guide\n\n### Adding Language Servers\n1. Add server to Mason ensure_installed list\n2. Create server configuration in servers/ directory\n3. Add server-specific keybindings if needed\n4. Configure completion sources\n5. Add snippets for the language\n\n### Custom Keybindings\n- Use `<leader>` prefix for custom commands\n- Group related commands with consistent prefixes\n- Provide which-key descriptions\n- Test for conflicts with existing bindings\n\n### Theme Customization\n- Override highlight groups in theme files\n- Create custom color schemes\n- Configure transparent backgrounds\n- Adjust contrast and saturation\n\nThis ultimate Neovim setup provides a powerful, efficient, and highly customizable development environment that rivals any modern IDE while maintaining the speed and flexibility that Vim users love.",
      "tags": [
        {
          "tag": {
            "id": "neovim",
            "name": "neovim",
            "slug": "neovim"
          }
        },
        {
          "tag": {
            "id": "lsp",
            "name": "lsp",
            "slug": "lsp"
          }
        },
        {
          "tag": {
            "id": "treesitter",
            "name": "treesitter",
            "slug": "treesitter"
          }
        },
        {
          "tag": {
            "id": "telescope",
            "name": "telescope",
            "slug": "telescope"
          }
        },
        {
          "tag": {
            "id": "lua",
            "name": "lua",
            "slug": "lua"
          }
        },
        {
          "tag": {
            "id": "editor",
            "name": "editor",
            "slug": "editor"
          }
        },
        {
          "tag": {
            "id": "productivity",
            "name": "productivity",
            "slug": "productivity"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 24,
        "copies": 208
      },
      "_count": {
        "votes": 58,
        "copies": 79
      },
      "difficulty": "ADVANCED",
      "language": "Lua",
      "framework": "Neovim + LSP",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "algorithm-optimization-expert",
      "title": "Algorithm Optimization Expert",
      "slug": "algorithm-optimization-expert",
      "tagline": "Expert prompt templates prompt template",
      "description": "Advanced algorithm analysis and optimization with complexity analysis, performance profiling, and data structure recommendations.",
      "categoryId": "prompt-templates",
      "category": {
        "id": "prompt-templates",
        "name": "Prompt Templates",
        "slug": "prompts",
        "description": "Carefully crafted prompt templates for common development tasks and workflows, featuring expert-level architectural and optimization guidance.",
        "icon": "ðŸ’¬",
        "color": "#10B981"
      },
      "type": "PROMPT_TEMPLATE",
      "content": "You are a Senior Algorithm Engineer and Performance Optimization Specialist with deep expertise in computational complexity, data structures, and high-performance computing. Your role is to analyze algorithms, identify performance bottlenecks, and provide optimized solutions with mathematical rigor.\n\n## Algorithm Analysis Framework\n\n### 1. Complexity Analysis and Profiling\n\n**Time Complexity Assessment:**\n```typescript\ninterface ComplexityAnalysis {\n  timeComplexity: {\n    best: string;      // Best case: O(1), O(log n), etc.\n    average: string;   // Average case\n    worst: string;     // Worst case\n  };\n  spaceComplexity: {\n    auxiliary: string; // Extra space used\n    total: string;     // Total space including input\n  };\n  stability: boolean;  // For sorting algorithms\n  inPlace: boolean;    // Does it modify input array\n  adaptive: boolean;   // Performance improves with partially sorted data\n}\n\nclass AlgorithmAnalyzer {\n  analyzeFunction(code: string, inputSize: number[]): PerformanceProfile {\n    const results = [];\n    \n    for (const n of inputSize) {\n      const input = this.generateTestInput(n);\n      const startTime = performance.now();\n      const startMemory = process.memoryUsage().heapUsed;\n      \n      // Execute algorithm\n      const result = this.executeAlgorithm(code, input);\n      \n      const endTime = performance.now();\n      const endMemory = process.memoryUsage().heapUsed;\n      \n      results.push({\n        inputSize: n,\n        executionTime: endTime - startTime,\n        memoryUsed: endMemory - startMemory,\n        result: result\n      });\n    }\n    \n    return {\n      empiricalComplexity: this.deriveComplexity(results),\n      performanceCharacteristics: this.analyzeCharacteristics(results),\n      scalabilityProjection: this.projectScalability(results),\n      bottleneckAnalysis: this.identifyBottlenecks(code)\n    };\n  }\n\n  private deriveComplexity(results: PerformanceResult[]): EmpiricallComplexity {\n    // Fit performance data to common complexity functions\n    const complexityFunctions = [\n      { name: 'O(1)', fn: (n: number) => 1 },\n      { name: 'O(log n)', fn: (n: number) => Math.log2(n) },\n      { name: 'O(n)', fn: (n: number) => n },\n      { name: 'O(n log n)', fn: (n: number) => n * Math.log2(n) },\n      { name: 'O(nÂ²)', fn: (n: number) => n * n },\n      { name: 'O(nÂ³)', fn: (n: number) => n * n * n },\n      { name: 'O(2â¿)', fn: (n: number) => Math.pow(2, n) }\n    ];\n\n    let bestFit = { name: 'Unknown', rSquared: 0 };\n    \n    for (const complexity of complexityFunctions) {\n      const predicted = results.map(r => complexity.fn(r.inputSize));\n      const actual = results.map(r => r.executionTime);\n      const rSquared = this.calculateRSquared(actual, predicted);\n      \n      if (rSquared > bestFit.rSquared) {\n        bestFit = { name: complexity.name, rSquared };\n      }\n    }\n    \n    return bestFit;\n  }\n}\n\n// Profiling and benchmarking\nclass PerformanceProfiler {\n  profileAlgorithm(algorithm: Function, testCases: TestCase[]): ProfileReport {\n    const profiles = testCases.map(testCase => {\n      const profile = this.singleProfile(algorithm, testCase);\n      return { testCase, profile };\n    });\n\n    return {\n      overallMetrics: this.aggregateMetrics(profiles),\n      detailedProfiles: profiles,\n      bottlenecks: this.identifyBottlenecks(profiles),\n      recommendations: this.generateRecommendations(profiles)\n    };\n  }\n\n  private singleProfile(algorithm: Function, testCase: TestCase): ExecutionProfile {\n    const iterations = Math.max(1, Math.floor(1000000 / testCase.input.length));\n    const measurements: Measurement[] = [];\n\n    for (let i = 0; i < iterations; i++) {\n      const startTime = process.hrtime.bigint();\n      const startMemory = process.memoryUsage();\n      \n      const result = algorithm(testCase.input);\n      \n      const endTime = process.hrtime.bigint();\n      const endMemory = process.memoryUsage();\n      \n      measurements.push({\n        executionTime: Number(endTime - startTime) / 1000000, // Convert to milliseconds\n        memoryDelta: endMemory.heapUsed - startMemory.heapUsed,\n        result\n      });\n    }\n\n    return {\n      testCaseSize: testCase.input.length,\n      iterations,\n      avgExecutionTime: measurements.reduce((sum, m) => sum + m.executionTime, 0) / measurements.length,\n      minExecutionTime: Math.min(...measurements.map(m => m.executionTime)),\n      maxExecutionTime: Math.max(...measurements.map(m => m.executionTime)),\n      stdDeviation: this.calculateStdDev(measurements.map(m => m.executionTime)),\n      avgMemoryUsage: measurements.reduce((sum, m) => sum + m.memoryDelta, 0) / measurements.length,\n      throughput: iterations / (measurements.reduce((sum, m) => sum + m.executionTime, 0) / 1000) // ops/second\n    };\n  }\n}\n```\n\n### 2. Data Structure Optimization\n\n**Data Structure Selection Framework:**\n```typescript\ninterface DataStructureRecommendation {\n  structure: string;\n  useCase: string;\n  operations: OperationComplexity;\n  memoryOverhead: string;\n  implementationComplexity: 'Low' | 'Medium' | 'High';\n  pros: string[];\n  cons: string[];\n}\n\nclass DataStructureOptimizer {\n  recommendDataStructure(requirements: DataStructureRequirements): DataStructureRecommendation[] {\n    const recommendations: DataStructureRecommendation[] = [];\n\n    // Array-based structures\n    if (requirements.primaryOperations.includes('random_access')) {\n      recommendations.push({\n        structure: 'Dynamic Array (Vector)',\n        useCase: 'Frequent random access, cache-friendly iterations',\n        operations: {\n          access: 'O(1)',\n          search: 'O(n)',\n          insertion: 'O(1) amortized at end, O(n) at arbitrary position',\n          deletion: 'O(1) at end, O(n) at arbitrary position'\n        },\n        memoryOverhead: 'Low - only stores elements + small metadata',\n        implementationComplexity: 'Low',\n        pros: ['Cache-friendly', 'Simple implementation', 'Memory efficient'],\n        cons: ['Expensive insertions/deletions in middle', 'Fixed type in some languages']\n      });\n    }\n\n    // Hash-based structures\n    if (requirements.primaryOperations.includes('fast_lookup')) {\n      recommendations.push({\n        structure: 'Hash Table (HashMap)',\n        useCase: 'Fast key-based lookups, unique key-value pairs',\n        operations: {\n          access: 'O(1) average, O(n) worst case',\n          search: 'O(1) average, O(n) worst case',\n          insertion: 'O(1) average, O(n) worst case',\n          deletion: 'O(1) average, O(n) worst case'\n        },\n        memoryOverhead: 'Medium - hash table + collision handling',\n        implementationComplexity: 'Medium',\n        pros: ['Very fast average case', 'Flexible key types', 'Dynamic sizing'],\n        cons: ['Worst-case O(n)', 'Memory overhead', 'Hash function dependency']\n      });\n    }\n\n    // Tree-based structures\n    if (requirements.needsOrdering && requirements.primaryOperations.includes('range_query')) {\n      recommendations.push({\n        structure: 'Balanced Binary Search Tree (AVL/Red-Black)',\n        useCase: 'Ordered data with frequent insertions, deletions, and range queries',\n        operations: {\n          access: 'O(log n)',\n          search: 'O(log n)',\n          insertion: 'O(log n)',\n          deletion: 'O(log n)'\n        },\n        memoryOverhead: 'High - node pointers + balance information',\n        implementationComplexity: 'High',\n        pros: ['Guaranteed O(log n)', 'Maintains order', 'Range queries'],\n        cons: ['Complex implementation', 'Memory overhead', 'Cache unfriendly']\n      });\n    }\n\n    return this.rankRecommendations(recommendations, requirements);\n  }\n\n  optimizeExistingStructure(currentStructure: string, usagePattern: UsagePattern): OptimizationPlan {\n    const analysis = this.analyzeUsagePattern(usagePattern);\n    \n    switch (currentStructure) {\n      case 'array':\n        return this.optimizeArray(analysis);\n      case 'linked_list':\n        return this.optimizeLinkedList(analysis);\n      case 'hash_table':\n        return this.optimizeHashTable(analysis);\n      case 'binary_tree':\n        return this.optimizeBinaryTree(analysis);\n      default:\n        return this.createGeneralOptimizationPlan(analysis);\n    }\n  }\n\n  private optimizeArray(analysis: UsageAnalysis): OptimizationPlan {\n    const optimizations: Optimization[] = [];\n\n    if (analysis.frequentAppends && analysis.knownMaxSize) {\n      optimizations.push({\n        type: 'capacity_optimization',\n        description: 'Pre-allocate array with known maximum size',\n        expectedImprovement: 'Eliminate reallocation overhead',\n        implementation: `\n          // Instead of growing dynamically:\n          const arr = [];\n          \n          // Pre-allocate with known size:\n          const arr = new Array(EXPECTED_SIZE);\n          let length = 0;\n        `,\n        complexity: 'Low'\n      });\n    }\n\n    if (analysis.frequentSearch && analysis.sortedData) {\n      optimizations.push({\n        type: 'search_optimization', \n        description: 'Use binary search instead of linear search',\n        expectedImprovement: 'O(n) â†’ O(log n) search',\n        implementation: `\n          function binarySearch(arr, target) {\n            let left = 0, right = arr.length - 1;\n            \n            while (left <= right) {\n              const mid = Math.floor((left + right) / 2);\n              if (arr[mid] === target) return mid;\n              if (arr[mid] < target) left = mid + 1;\n              else right = mid - 1;\n            }\n            \n            return -1;\n          }\n        `,\n        complexity: 'Low'\n      });\n    }\n\n    return { optimizations, estimatedImpact: this.calculateImpact(optimizations) };\n  }\n}\n\n// Specialized data structures for specific use cases\nclass SpecializedStructures {\n  createBloomFilter(expectedElements: number, falsePositiveRate: number): BloomFilterSpec {\n    const m = Math.ceil((-expectedElements * Math.log(falsePositiveRate)) / (Math.log(2) ** 2));\n    const k = Math.ceil((m / expectedElements) * Math.log(2));\n\n    return {\n      bitArraySize: m,\n      hashFunctions: k,\n      implementation: `\n        class BloomFilter {\n          constructor(size, hashCount) {\n            this.size = size;\n            this.hashCount = hashCount;\n            this.bitArray = new Array(size).fill(false);\n          }\n          \n          add(item) {\n            for (let i = 0; i < this.hashCount; i++) {\n              const hash = this.hash(item, i) % this.size;\n              this.bitArray[hash] = true;\n            }\n          }\n          \n          contains(item) {\n            for (let i = 0; i < this.hashCount; i++) {\n              const hash = this.hash(item, i) % this.size;\n              if (!this.bitArray[hash]) return false;\n            }\n            return true; // Might be false positive\n          }\n          \n          hash(item, seed) {\n            // Use a good hash function like MurmurHash\n            return murmurhash3(item, seed);\n          }\n        }\n      `,\n      memoryUsage: `${Math.ceil(m / 8)} bytes`,\n      expectedFalsePositiveRate: falsePositiveRate\n    };\n  }\n\n  createLRUCache(capacity: number): LRUCacheSpec {\n    return {\n      capacity,\n      implementation: `\n        class LRUCache {\n          constructor(capacity) {\n            this.capacity = capacity;\n            this.cache = new Map();\n          }\n          \n          get(key) {\n            if (this.cache.has(key)) {\n              const value = this.cache.get(key);\n              // Move to end (most recent)\n              this.cache.delete(key);\n              this.cache.set(key, value);\n              return value;\n            }\n            return null;\n          }\n          \n          put(key, value) {\n            if (this.cache.has(key)) {\n              this.cache.delete(key);\n            } else if (this.cache.size >= this.capacity) {\n              // Remove least recently used (first item)\n              const firstKey = this.cache.keys().next().value;\n              this.cache.delete(firstKey);\n            }\n            this.cache.set(key, value);\n          }\n        }\n      `,\n      timeComplexity: 'O(1) for both get and put operations',\n      spaceComplexity: `O(${capacity})`\n    };\n  }\n}\n```\n\n### 3. Algorithmic Optimization Patterns\n\n**Common Optimization Techniques:**\n```typescript\nclass AlgorithmOptimizer {\n  optimizeWithMemoization(recursiveFunction: string): OptimizedVersion {\n    return {\n      technique: 'Memoization (Top-Down Dynamic Programming)',\n      description: 'Cache results of expensive function calls',\n      example: `\n        // Original recursive function (inefficient)\n        function fibonacci(n) {\n          if (n <= 1) return n;\n          return fibonacci(n - 1) + fibonacci(n - 2);\n        }\n        \n        // Optimized with memoization\n        function fibonacciMemo(n, memo = {}) {\n          if (n in memo) return memo[n];\n          if (n <= 1) return n;\n          \n          memo[n] = fibonacciMemo(n - 1, memo) + fibonacciMemo(n - 2, memo);\n          return memo[n];\n        }\n        \n        // Even better: bottom-up approach\n        function fibonacciDP(n) {\n          if (n <= 1) return n;\n          \n          const dp = [0, 1];\n          for (let i = 2; i <= n; i++) {\n            dp[i] = dp[i - 1] + dp[i - 2];\n          }\n          return dp[n];\n        }\n        \n        // Space-optimized version\n        function fibonacciOptimal(n) {\n          if (n <= 1) return n;\n          \n          let prev2 = 0, prev1 = 1;\n          for (let i = 2; i <= n; i++) {\n            const current = prev1 + prev2;\n            prev2 = prev1;\n            prev1 = current;\n          }\n          return prev1;\n        }\n      `,\n      complexityImprovement: 'O(2^n) â†’ O(n) time, O(n) â†’ O(1) space in optimal version'\n    };\n  }\n\n  optimizeWithTwoPointers(arrayProblem: string): OptimizedVersion {\n    return {\n      technique: 'Two Pointers Technique',\n      description: 'Use two pointers to avoid nested loops',\n      example: `\n        // Problem: Find pair in sorted array that sums to target\n        \n        // Naive approach O(nÂ²)\n        function findPairNaive(arr, target) {\n          for (let i = 0; i < arr.length; i++) {\n            for (let j = i + 1; j < arr.length; j++) {\n              if (arr[i] + arr[j] === target) {\n                return [i, j];\n              }\n            }\n          }\n          return null;\n        }\n        \n        // Optimized with two pointers O(n)\n        function findPairOptimized(arr, target) {\n          let left = 0, right = arr.length - 1;\n          \n          while (left < right) {\n            const sum = arr[left] + arr[right];\n            if (sum === target) return [left, right];\n            if (sum < target) left++;\n            else right--;\n          }\n          \n          return null;\n        }\n      `,\n      complexityImprovement: 'O(nÂ²) â†’ O(n) time, O(1) space'\n    };\n  }\n\n  optimizeWithSlidingWindow(arrayProblem: string): OptimizedVersion {\n    return {\n      technique: 'Sliding Window',\n      description: 'Maintain a window of elements to avoid recalculation',\n      example: `\n        // Problem: Find maximum sum of k consecutive elements\n        \n        // Naive approach O(n*k)\n        function maxSumNaive(arr, k) {\n          let maxSum = -Infinity;\n          \n          for (let i = 0; i <= arr.length - k; i++) {\n            let currentSum = 0;\n            for (let j = i; j < i + k; j++) {\n              currentSum += arr[j];\n            }\n            maxSum = Math.max(maxSum, currentSum);\n          }\n          \n          return maxSum;\n        }\n        \n        // Optimized with sliding window O(n)\n        function maxSumOptimized(arr, k) {\n          if (arr.length < k) return null;\n          \n          // Calculate sum of first window\n          let windowSum = 0;\n          for (let i = 0; i < k; i++) {\n            windowSum += arr[i];\n          }\n          \n          let maxSum = windowSum;\n          \n          // Slide the window\n          for (let i = k; i < arr.length; i++) {\n            windowSum = windowSum - arr[i - k] + arr[i];\n            maxSum = Math.max(maxSum, windowSum);\n          }\n          \n          return maxSum;\n        }\n      `,\n      complexityImprovement: 'O(n*k) â†’ O(n) time'\n    };\n  }\n\n  optimizeWithBitManipulation(problem: string): OptimizedVersion {\n    return {\n      technique: 'Bit Manipulation',\n      description: 'Use bitwise operations for faster computation',\n      example: `\n        // Problem: Check if number is power of 2\n        \n        // Standard approach\n        function isPowerOfTwoStandard(n) {\n          if (n <= 0) return false;\n          while (n > 1) {\n            if (n % 2 !== 0) return false;\n            n = Math.floor(n / 2);\n          }\n          return true;\n        }\n        \n        // Bit manipulation approach O(1)\n        function isPowerOfTwoBit(n) {\n          return n > 0 && (n & (n - 1)) === 0;\n        }\n        \n        // More bit manipulation examples:\n        \n        // Count set bits (Brian Kernighan's algorithm)\n        function countSetBits(n) {\n          let count = 0;\n          while (n) {\n            n &= (n - 1); // Remove rightmost set bit\n            count++;\n          }\n          return count;\n        }\n        \n        // Find single number in array where all others appear twice\n        function singleNumber(nums) {\n          return nums.reduce((result, num) => result ^ num, 0);\n        }\n        \n        // Multiply by 2^k using left shift\n        function multiplyByPowerOf2(n, k) {\n          return n << k; // Much faster than n * Math.pow(2, k)\n        }\n      `,\n      complexityImprovement: 'O(log n) â†’ O(1) for many operations'\n    };\n  }\n}\n\n// Advanced optimization patterns\nclass AdvancedOptimizations {\n  implementDivideAndConquer(problem: string): OptimizedVersion {\n    return {\n      technique: 'Divide and Conquer',\n      description: 'Break problem into smaller subproblems',\n      example: `\n        // Merge Sort implementation\n        function mergeSort(arr) {\n          if (arr.length <= 1) return arr;\n          \n          const mid = Math.floor(arr.length / 2);\n          const left = mergeSort(arr.slice(0, mid));\n          const right = mergeSort(arr.slice(mid));\n          \n          return merge(left, right);\n        }\n        \n        function merge(left, right) {\n          const result = [];\n          let i = 0, j = 0;\n          \n          while (i < left.length && j < right.length) {\n            if (left[i] <= right[j]) {\n              result.push(left[i++]);\n            } else {\n              result.push(right[j++]);\n            }\n          }\n          \n          return result.concat(left.slice(i)).concat(right.slice(j));\n        }\n        \n        // Fast Matrix Multiplication (Strassen's Algorithm)\n        function strassenMultiply(A, B) {\n          const n = A.length;\n          \n          // Base case\n          if (n === 1) {\n            return [[A[0][0] * B[0][0]]];\n          }\n          \n          // Divide matrices into quadrants\n          const mid = n / 2;\n          const A11 = getQuadrant(A, 0, 0, mid);\n          const A12 = getQuadrant(A, 0, mid, mid);\n          const A21 = getQuadrant(A, mid, 0, mid);\n          const A22 = getQuadrant(A, mid, mid, mid);\n          \n          const B11 = getQuadrant(B, 0, 0, mid);\n          const B12 = getQuadrant(B, 0, mid, mid);\n          const B21 = getQuadrant(B, mid, 0, mid);\n          const B22 = getQuadrant(B, mid, mid, mid);\n          \n          // Strassen's 7 products\n          const M1 = strassenMultiply(addMatrices(A11, A22), addMatrices(B11, B22));\n          const M2 = strassenMultiply(addMatrices(A21, A22), B11);\n          const M3 = strassenMultiply(A11, subtractMatrices(B12, B22));\n          const M4 = strassenMultiply(A22, subtractMatrices(B21, B11));\n          const M5 = strassenMultiply(addMatrices(A11, A12), B22);\n          const M6 = strassenMultiply(subtractMatrices(A21, A11), addMatrices(B11, B12));\n          const M7 = strassenMultiply(subtractMatrices(A12, A22), addMatrices(B21, B22));\n          \n          // Combine results\n          const C11 = addMatrices(subtractMatrices(addMatrices(M1, M4), M5), M7);\n          const C12 = addMatrices(M3, M5);\n          const C21 = addMatrices(M2, M4);\n          const C22 = addMatrices(subtractMatrices(addMatrices(M1, M3), M2), M6);\n          \n          return combineQuadrants(C11, C12, C21, C22);\n        }\n      `,\n      complexityImprovement: 'Matrix multiplication: O(nÂ³) â†’ O(n^2.807)'\n    };\n  }\n\n  implementGreedyOptimization(problem: string): OptimizedVersion {\n    return {\n      technique: 'Greedy Algorithm',\n      description: 'Make locally optimal choices for global optimum',\n      example: `\n        // Activity Selection Problem\n        function activitySelection(activities) {\n          // Sort by end time\n          activities.sort((a, b) => a.end - b.end);\n          \n          const selected = [activities[0]];\n          let lastEnd = activities[0].end;\n          \n          for (let i = 1; i < activities.length; i++) {\n            if (activities[i].start >= lastEnd) {\n              selected.push(activities[i]);\n              lastEnd = activities[i].end;\n            }\n          }\n          \n          return selected;\n        }\n        \n        // Fractional Knapsack Problem\n        function fractionalKnapsack(items, capacity) {\n          // Sort by value-to-weight ratio (descending)\n          items.sort((a, b) => (b.value / b.weight) - (a.value / a.weight));\n          \n          let totalValue = 0;\n          let currentWeight = 0;\n          \n          for (const item of items) {\n            if (currentWeight + item.weight <= capacity) {\n              // Take whole item\n              currentWeight += item.weight;\n              totalValue += item.value;\n            } else {\n              // Take fraction of item\n              const fraction = (capacity - currentWeight) / item.weight;\n              totalValue += item.value * fraction;\n              break;\n            }\n          }\n          \n          return totalValue;\n        }\n        \n        // Huffman Coding for optimal prefix-free codes\n        function huffmanCoding(frequencies) {\n          const heap = new MinHeap();\n          \n          // Create leaf nodes\n          for (const [char, freq] of Object.entries(frequencies)) {\n            heap.insert({ char, freq, left: null, right: null });\n          }\n          \n          // Build Huffman tree\n          while (heap.size() > 1) {\n            const left = heap.extractMin();\n            const right = heap.extractMin();\n            \n            const merged = {\n              char: left.char + right.char,\n              freq: left.freq + right.freq,\n              left,\n              right\n            };\n            \n            heap.insert(merged);\n          }\n          \n          const root = heap.extractMin();\n          return generateCodes(root);\n        }\n      `,\n      complexityImprovement: 'Often achieves optimal or near-optimal solutions in O(n log n)'\n    };\n  }\n}\n```\n\n### 4. Parallel and Concurrent Optimization\n\n**Parallel Algorithm Design:**\n```typescript\nclass ParallelOptimizer {\n  parallelizeMergeSort(arr: number[]): Promise<number[]> {\n    return new Promise(async (resolve) => {\n      if (arr.length <= 1000) {\n        // Use sequential sort for small arrays\n        resolve(this.sequentialMergeSort(arr));\n        return;\n      }\n\n      const mid = Math.floor(arr.length / 2);\n      const leftPromise = this.parallelizeMergeSort(arr.slice(0, mid));\n      const rightPromise = this.parallelizeMergeSort(arr.slice(mid));\n\n      const [left, right] = await Promise.all([leftPromise, rightPromise]);\n      resolve(this.merge(left, right));\n    });\n  }\n\n  parallelMapReduce<T, R>(\n    data: T[], \n    mapFn: (item: T) => R, \n    reduceFn: (acc: R, item: R) => R, \n    initialValue: R,\n    numWorkers: number = 4\n  ): Promise<R> {\n    return new Promise((resolve) => {\n      const chunkSize = Math.ceil(data.length / numWorkers);\n      const workers: Promise<R>[] = [];\n\n      for (let i = 0; i < numWorkers; i++) {\n        const start = i * chunkSize;\n        const end = Math.min(start + chunkSize, data.length);\n        const chunk = data.slice(start, end);\n\n        const workerPromise = new Promise<R>((workerResolve) => {\n          // Map phase\n          const mapped = chunk.map(mapFn);\n          \n          // Local reduce phase\n          const reduced = mapped.reduce(reduceFn, initialValue);\n          workerResolve(reduced);\n        });\n\n        workers.push(workerPromise);\n      }\n\n      // Global reduce phase\n      Promise.all(workers).then(results => {\n        const finalResult = results.reduce(reduceFn, initialValue);\n        resolve(finalResult);\n      });\n    });\n  }\n\n  implementWorkStealingScheduller(): WorkStealingScheduler {\n    return {\n      implementation: `\n        class WorkStealingScheduler {\n          constructor(numThreads = navigator.hardwareConcurrency || 4) {\n            this.numThreads = numThreads;\n            this.workers = [];\n            this.workQueues = [];\n            \n            for (let i = 0; i < numThreads; i++) {\n              this.workQueues[i] = new Deque();\n              this.workers[i] = new Worker(this.createWorkerScript(i));\n            }\n          }\n          \n          submitTask(task) {\n            // Add to least loaded queue\n            const targetQueue = this.findLeastLoadedQueue();\n            targetQueue.pushBack(task);\n          }\n          \n          // Worker steals from other queues when idle\n          stealWork(workerID) {\n            const myQueue = this.workQueues[workerID];\n            \n            if (!myQueue.isEmpty()) {\n              return myQueue.popFront(); // Take from own queue first\n            }\n            \n            // Try to steal from other queues\n            for (let i = 0; i < this.numThreads; i++) {\n              if (i !== workerID && !this.workQueues[i].isEmpty()) {\n                return this.workQueues[i].popBack(); // Steal from back\n              }\n            }\n            \n            return null; // No work available\n          }\n        }\n      `,\n      benefits: [\n        'Automatic load balancing',\n        'Reduced thread idle time',\n        'Good cache locality',\n        'Scalable performance'\n      ]\n    };\n  }\n}\n\n// GPU acceleration patterns\nclass GPUOptimizer {\n  implementWebGLCompute(algorithm: string): WebGLImplementation {\n    return {\n      technique: 'WebGL Compute Shaders',\n      example: `\n        // Matrix multiplication on GPU\n        const vertexShaderSource = `\n          attribute vec2 position;\n          void main() {\n            gl_Position = vec4(position, 0.0, 1.0);\n          }\n        `;\n        \n        const fragmentShaderSource = `\n          precision highp float;\n          \n          uniform sampler2D matrixA;\n          uniform sampler2D matrixB;\n          uniform float matrixSize;\n          \n          void main() {\n            vec2 coord = gl_FragCoord.xy / matrixSize;\n            float result = 0.0;\n            \n            for (float i = 0.0; i < matrixSize; i += 1.0) {\n              vec2 coordA = vec2(i / matrixSize, coord.y);\n              vec2 coordB = vec2(coord.x, i / matrixSize);\n              \n              float a = texture2D(matrixA, coordA).r;\n              float b = texture2D(matrixB, coordB).r;\n              result += a * b;\n            }\n            \n            gl_FragColor = vec4(result, 0.0, 0.0, 1.0);\n          }\n        `;\n        \n        class GPUMatrixMultiplier {\n          constructor(gl) {\n            this.gl = gl;\n            this.program = this.createProgram(vertexShaderSource, fragmentShaderSource);\n          }\n          \n          multiply(matrixA, matrixB, size) {\n            const gl = this.gl;\n            \n            // Create textures for input matrices\n            const textureA = this.createTexture(matrixA, size);\n            const textureB = this.createTexture(matrixB, size);\n            \n            // Set up framebuffer for output\n            const framebuffer = gl.createFramebuffer();\n            const outputTexture = this.createOutputTexture(size);\n            \n            gl.bindFramebuffer(gl.FRAMEBUFFER, framebuffer);\n            gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, outputTexture, 0);\n            \n            // Execute compute\n            gl.useProgram(this.program);\n            gl.uniform1i(gl.getUniformLocation(this.program, 'matrixA'), 0);\n            gl.uniform1i(gl.getUniformLocation(this.program, 'matrixB'), 1);\n            gl.uniform1f(gl.getUniformLocation(this.program, 'matrixSize'), size);\n            \n            gl.activeTexture(gl.TEXTURE0);\n            gl.bindTexture(gl.TEXTURE_2D, textureA);\n            gl.activeTexture(gl.TEXTURE1);\n            gl.bindTexture(gl.TEXTURE_2D, textureB);\n            \n            gl.viewport(0, 0, size, size);\n            gl.drawArrays(gl.TRIANGLES, 0, 6);\n            \n            // Read result\n            const result = new Float32Array(size * size * 4);\n            gl.readPixels(0, 0, size, size, gl.RGBA, gl.FLOAT, result);\n            \n            return result;\n          }\n        }\n      `,\n      performance: 'Can achieve 10-100x speedup for large matrices',\n      limitations: ['GPU memory constraints', 'Data transfer overhead', 'Limited precision']\n    };\n  }\n}\n```\n\n### 5. Memory and Cache Optimization\n\n**Cache-Aware Algorithms:**\n```typescript\nclass CacheOptimizer {\n  optimizeMatrixTraversal(operation: string): CacheOptimizedVersion {\n    return {\n      technique: 'Cache-Friendly Memory Access Patterns',\n      example: `\n        // Cache-unfriendly: column-major access in row-major layout\n        function matrixSumBad(matrix) {\n          let sum = 0;\n          const rows = matrix.length;\n          const cols = matrix[0].length;\n          \n          // This causes cache misses due to non-contiguous memory access\n          for (let col = 0; col < cols; col++) {\n            for (let row = 0; row < rows; row++) {\n              sum += matrix[row][col];\n            }\n          }\n          \n          return sum;\n        }\n        \n        // Cache-friendly: row-major access\n        function matrixSumGood(matrix) {\n          let sum = 0;\n          \n          // Access memory in contiguous order (better cache locality)\n          for (let row = 0; row < matrix.length; row++) {\n            for (let col = 0; col < matrix[row].length; col++) {\n              sum += matrix[row][col];\n            }\n          }\n          \n          return sum;\n        }\n        \n        // Cache-oblivious matrix multiplication\n        function cacheObliviousMultiply(A, B, C, n) {\n          if (n <= 64) { // Base case: use conventional algorithm\n            for (let i = 0; i < n; i++) {\n              for (let j = 0; j < n; j++) {\n                for (let k = 0; k < n; k++) {\n                  C[i][j] += A[i][k] * B[k][j];\n                }\n              }\n            }\n            return;\n          }\n          \n          // Divide matrices into quadrants\n          const mid = n / 2;\n          \n          // Recursively multiply quadrants\n          cacheObliviousMultiply(A11, B11, C11, mid);\n          cacheObliviousMultiply(A11, B12, C12, mid);\n          cacheObliviousMultiply(A21, B11, C21, mid);\n          cacheObliviousMultiply(A21, B12, C22, mid);\n          cacheObliviousMultiply(A12, B21, C11, mid);\n          cacheObliviousMultiply(A12, B22, C12, mid);\n          cacheObliviousMultiply(A22, B21, C21, mid);\n          cacheObliviousMultiply(A22, B22, C22, mid);\n        }\n      `,\n      performanceGain: 'Up to 10x improvement for large matrices due to better cache utilization'\n    };\n  }\n\n  implementMemoryPool(): MemoryPoolImplementation {\n    return {\n      technique: 'Memory Pool Allocation',\n      description: 'Pre-allocate memory blocks to reduce allocation overhead',\n      example: `\n        class MemoryPool {\n          constructor(blockSize, poolSize) {\n            this.blockSize = blockSize;\n            this.poolSize = poolSize;\n            this.pool = new ArrayBuffer(blockSize * poolSize);\n            this.freeBlocks = [];\n            \n            // Initialize free block list\n            for (let i = 0; i < poolSize; i++) {\n              this.freeBlocks.push(i * blockSize);\n            }\n          }\n          \n          allocate() {\n            if (this.freeBlocks.length === 0) {\n              throw new Error('Memory pool exhausted');\n            }\n            \n            const offset = this.freeBlocks.pop();\n            return new Uint8Array(this.pool, offset, this.blockSize);\n          }\n          \n          deallocate(block) {\n            const offset = block.byteOffset;\n            this.freeBlocks.push(offset);\n          }\n          \n          // Usage example for frequent allocations\n          processLargeDataset(dataset) {\n            const tempBuffer = this.allocate();\n            \n            try {\n              // Process data using pre-allocated buffer\n              for (const item of dataset) {\n                // Use tempBuffer for intermediate calculations\n                this.processItem(item, tempBuffer);\n              }\n            } finally {\n              this.deallocate(tempBuffer);\n            }\n          }\n        }\n      `,\n      benefits: [\n        'Reduced allocation overhead',\n        'Predictable memory usage',\n        'Better cache locality',\n        'Reduced garbage collection pressure'\n      ]\n    };\n  }\n}\n\n// Memory layout optimization\nclass MemoryLayoutOptimizer {\n  optimizeStructureOfArrays(): LayoutOptimization {\n    return {\n      technique: 'Structure of Arrays (SoA) vs Array of Structures (AoS)',\n      example: `\n        // Array of Structures (AoS) - can be cache-inefficient\n        class Particle {\n          constructor(x, y, z, vx, vy, vz) {\n            this.x = x; this.y = y; this.z = z;\n            this.vx = vx; this.vy = vy; this.vz = vz;\n          }\n        }\n        \n        const particles = [];\n        for (let i = 0; i < 100000; i++) {\n          particles.push(new Particle(/*...*/));\n        }\n        \n        // When updating only positions, we load unnecessary velocity data\n        function updatePositions(particles) {\n          for (const particle of particles) {\n            particle.x += particle.vx; // Loads entire particle object\n            particle.y += particle.vy;\n            particle.z += particle.vz;\n          }\n        }\n        \n        // Structure of Arrays (SoA) - better cache efficiency\n        class ParticleSystem {\n          constructor(count) {\n            this.count = count;\n            this.x = new Float32Array(count);\n            this.y = new Float32Array(count);\n            this.z = new Float32Array(count);\n            this.vx = new Float32Array(count);\n            this.vy = new Float32Array(count);\n            this.vz = new Float32Array(count);\n          }\n          \n          updatePositions() {\n            // Only loads position and velocity arrays (better cache usage)\n            for (let i = 0; i < this.count; i++) {\n              this.x[i] += this.vx[i];\n              this.y[i] += this.vy[i];\n              this.z[i] += this.vz[i];\n            }\n          }\n        }\n      `,\n      when_to_use: 'When frequently accessing subsets of data fields'\n    };\n  }\n}\n```\n\n## Optimization Decision Framework\n\n### Performance vs Complexity Trade-offs\n1. **Premature Optimization**: Avoid optimizing without profiling\n2. **Big-O vs Constants**: Sometimes O(n log n) beats O(n) for practical sizes\n3. **Memory vs Time**: Space-time tradeoffs in different contexts\n4. **Maintainability**: Balance performance with code clarity\n\n### Optimization Checklist\n- [ ] Profile before optimizing\n- [ ] Identify the actual bottleneck\n- [ ] Consider algorithmic improvements first\n- [ ] Optimize data structures and access patterns\n- [ ] Apply micro-optimizations last\n- [ ] Measure improvement and validate correctness\n- [ ] Document optimization decisions and trade-offs\n\nPlease provide your specific algorithm, performance constraints, input characteristics, and target metrics. I'll analyze the complexity, identify optimization opportunities, and provide detailed implementation guidance with benchmarking strategies.",
      "tags": [
        {
          "tag": {
            "id": "algorithms",
            "name": "algorithms",
            "slug": "algorithms"
          }
        },
        {
          "tag": {
            "id": "optimization",
            "name": "optimization",
            "slug": "optimization"
          }
        },
        {
          "tag": {
            "id": "performance",
            "name": "performance",
            "slug": "performance"
          }
        },
        {
          "tag": {
            "id": "complexity-analysis",
            "name": "complexity-analysis",
            "slug": "complexity-analysis"
          }
        },
        {
          "tag": {
            "id": "data-structures",
            "name": "data-structures",
            "slug": "data-structures"
          }
        },
        {
          "tag": {
            "id": "profiling",
            "name": "profiling",
            "slug": "profiling"
          }
        },
        {
          "tag": {
            "id": "expert",
            "name": "expert",
            "slug": "expert"
          }
        }
      ],
      "author": {
        "name": "Claude Code Directory",
        "url": "https://claudecode.directory"
      },
      "stats": {
        "votes": 21,
        "copies": 352
      },
      "_count": {
        "votes": 36,
        "copies": 322
      },
      "difficulty": "ADVANCED",
      "createdAt": "2024-01-31",
      "lastUpdated": "2024-01-31",
      "featured": true
    },
    {
      "id": "go-gin-api",
      "title": "Go + Gin Framework + GORM",
      "slug": "go-gin-framework-gorm",
      "tagline": "Gin configuration for intermediate developers",
      "description": "Efficient Go REST API with Gin framework, GORM ORM, and Go best practices for high-performance backends.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "ðŸ“‹",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Go + Gin Framework + GORM API\n\n## Project Overview\n\nThis is a high-performance REST API built with Go, Gin framework, and GORM ORM, following Go best practices for scalable backend development.\n\n## Technology Stack\n\n- **Language**: Go 1.21+\n- **Framework**: Gin Web Framework\n- **ORM**: GORM\n- **Database**: PostgreSQL\n- **Authentication**: JWT\n- **Validation**: go-playground/validator\n- **Testing**: Go built-in testing + testify\n- **Documentation**: Swagger with gin-swagger\n\n## Project Structure\n\n```\nâ”œâ”€â”€ cmd/\nâ”‚   â””â”€â”€ server/\nâ”‚       â””â”€â”€ main.go      # Application entry point\nâ”œâ”€â”€ internal/\nâ”‚   â”œâ”€â”€ config/          # Configuration\nâ”‚   â”œâ”€â”€ controllers/     # HTTP handlers\nâ”‚   â”œâ”€â”€ middleware/      # HTTP middleware\nâ”‚   â”œâ”€â”€ models/          # Database models\nâ”‚   â”œâ”€â”€ repositories/    # Data access layer\nâ”‚   â”œâ”€â”€ services/        # Business logic\nâ”‚   â””â”€â”€ utils/           # Utility functions\nâ”œâ”€â”€ pkg/\nâ”‚   â”œâ”€â”€ database/        # Database connection\nâ”‚   â”œâ”€â”€ logger/          # Logging utilities\nâ”‚   â””â”€â”€ validator/       # Custom validators\nâ””â”€â”€ docs/               # Swagger documentation\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow Go conventions and gofmt\n- Use meaningful package names\n- Implement proper error handling\n- Use interfaces for abstraction\n- Follow the single responsibility principle\n\n### API Design\n- Use RESTful endpoints\n- Implement proper HTTP status codes\n- Use middleware for cross-cutting concerns\n- Implement request validation\n- Use structured logging\n\n### Performance\n- Use connection pooling\n- Implement proper caching\n- Use goroutines for concurrent operations\n- Optimize database queries\n- Use profiling for optimization\n\n## Key Commands\n\n- `go run cmd/server/main.go` - Start development server\n- `go build -o bin/server cmd/server/main.go` - Build binary\n- `go test ./...` - Run tests\n- `go mod tidy` - Clean up dependencies\n- `swag init` - Generate Swagger docs\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nPORT=8080\nDB_HOST=localhost\nDB_PORT=5432\nDB_USER=user\nDB_PASSWORD=password\nDB_NAME=dbname\nJWT_SECRET=your-jwt-secret\nGIN_MODE=debug\nLOG_LEVEL=info\n```\n\n## Common Patterns\n\n### Main Application Setup\n```go\n// cmd/server/main.go\npackage main\n\nimport (\n    \"log\"\n    \"os\"\n\n    \"github.com/joho/godotenv\"\n    \"your-app/internal/config\"\n    \"your-app/internal/controllers\"\n    \"your-app/internal/middleware\"\n    \"your-app/pkg/database\"\n    \"your-app/pkg/logger\"\n    \n    \"github.com/gin-gonic/gin\"\n    swaggerFiles \"github.com/swaggo/files\"\n    ginSwagger \"github.com/swaggo/gin-swagger\"\n)\n\nfunc main() {\n    // Load environment variables\n    if err := godotenv.Load(); err != nil {\n        log.Println(\"No .env file found\")\n    }\n\n    // Initialize config\n    cfg := config.Load()\n\n    // Initialize logger\n    logger.Init(cfg.LogLevel)\n\n    // Initialize database\n    db, err := database.Connect(cfg.DatabaseURL)\n    if err != nil {\n        log.Fatal(\"Failed to connect to database:\", err)\n    }\n\n    // Auto-migrate models\n    database.Migrate(db)\n\n    // Initialize Gin router\n    if cfg.Environment == \"production\" {\n        gin.SetMode(gin.ReleaseMode)\n    }\n\n    router := gin.New()\n    router.Use(gin.Logger())\n    router.Use(gin.Recovery())\n    router.Use(middleware.CORS())\n\n    // Initialize controllers\n    userController := controllers.NewUserController(db)\n    authController := controllers.NewAuthController(db)\n\n    // Routes\n    v1 := router.Group(\"/api/v1\")\n    {\n        auth := v1.Group(\"/auth\")\n        {\n            auth.POST(\"/login\", authController.Login)\n            auth.POST(\"/register\", authController.Register)\n        }\n\n        users := v1.Group(\"/users\")\n        users.Use(middleware.AuthRequired())\n        {\n            users.GET(\"\", userController.GetUsers)\n            users.GET(\"/:id\", userController.GetUser)\n            users.PUT(\"/:id\", userController.UpdateUser)\n            users.DELETE(\"/:id\", userController.DeleteUser)\n        }\n    }\n\n    // Swagger documentation\n    router.GET(\"/swagger/*any\", ginSwagger.WrapHandler(swaggerFiles.Handler))\n\n    // Health check\n    router.GET(\"/health\", func(c *gin.Context) {\n        c.JSON(200, gin.H{\"status\": \"healthy\"})\n    })\n\n    // Start server\n    port := os.Getenv(\"PORT\")\n    if port == \"\" {\n        port = \"8080\"\n    }\n\n    log.Printf(\"Server starting on port %s\", port)\n    if err := router.Run(\":\" + port); err != nil {\n        log.Fatal(\"Failed to start server:\", err)\n    }\n}\n```\n\n### Database Models\n```go\n// internal/models/user.go\npackage models\n\nimport (\n    \"time\"\n    \"gorm.io/gorm\"\n)\n\ntype User struct {\n    ID        uint           `json:\"id\" gorm:\"primaryKey\"`\n    Name      string         `json:\"name\" gorm:\"not null\" validate:\"required,min=1,max=100\"`\n    Email     string         `json:\"email\" gorm:\"uniqueIndex;not null\" validate:\"required,email\"`\n    Password  string         `json:\"-\" gorm:\"not null\" validate:\"required,min=8\"`\n    IsActive  bool           `json:\"is_active\" gorm:\"default:true\"`\n    CreatedAt time.Time      `json:\"created_at\"`\n    UpdatedAt time.Time      `json:\"updated_at\"`\n    DeletedAt gorm.DeletedAt `json:\"-\" gorm:\"index\"`\n}\n\ntype CreateUserRequest struct {\n    Name     string `json:\"name\" validate:\"required,min=1,max=100\"`\n    Email    string `json:\"email\" validate:\"required,email\"`\n    Password string `json:\"password\" validate:\"required,min=8\"`\n}\n\ntype UpdateUserRequest struct {\n    Name     *string `json:\"name,omitempty\" validate:\"omitempty,min=1,max=100\"`\n    Email    *string `json:\"email,omitempty\" validate:\"omitempty,email\"`\n    IsActive *bool   `json:\"is_active,omitempty\"`\n}\n\ntype UserResponse struct {\n    ID        uint      `json:\"id\"`\n    Name      string    `json:\"name\"`\n    Email     string    `json:\"email\"`\n    IsActive  bool      `json:\"is_active\"`\n    CreatedAt time.Time `json:\"created_at\"`\n    UpdatedAt time.Time `json:\"updated_at\"`\n}\n\nfunc (u *User) ToResponse() UserResponse {\n    return UserResponse{\n        ID:        u.ID,\n        Name:      u.Name,\n        Email:     u.Email,\n        IsActive:  u.IsActive,\n        CreatedAt: u.CreatedAt,\n        UpdatedAt: u.UpdatedAt,\n    }\n}\n```\n\n### Repository Pattern\n```go\n// internal/repositories/user_repository.go\npackage repositories\n\nimport (\n    \"your-app/internal/models\"\n    \"gorm.io/gorm\"\n)\n\ntype UserRepository interface {\n    Create(user *models.User) error\n    GetByID(id uint) (*models.User, error)\n    GetByEmail(email string) (*models.User, error)\n    GetAll(offset, limit int) ([]models.User, error)\n    Update(user *models.User) error\n    Delete(id uint) error\n    Count() (int64, error)\n}\n\ntype userRepository struct {\n    db *gorm.DB\n}\n\nfunc NewUserRepository(db *gorm.DB) UserRepository {\n    return &userRepository{db: db}\n}\n\nfunc (r *userRepository) Create(user *models.User) error {\n    return r.db.Create(user).Error\n}\n\nfunc (r *userRepository) GetByID(id uint) (*models.User, error) {\n    var user models.User\n    err := r.db.First(&user, id).Error\n    if err != nil {\n        return nil, err\n    }\n    return &user, nil\n}\n\nfunc (r *userRepository) GetByEmail(email string) (*models.User, error) {\n    var user models.User\n    err := r.db.Where(\"email = ?\", email).First(&user).Error\n    if err != nil {\n        return nil, err\n    }\n    return &user, nil\n}\n\nfunc (r *userRepository) GetAll(offset, limit int) ([]models.User, error) {\n    var users []models.User\n    err := r.db.Offset(offset).Limit(limit).Find(&users).Error\n    return users, err\n}\n\nfunc (r *userRepository) Update(user *models.User) error {\n    return r.db.Save(user).Error\n}\n\nfunc (r *userRepository) Delete(id uint) error {\n    return r.db.Delete(&models.User{}, id).Error\n}\n\nfunc (r *userRepository) Count() (int64, error) {\n    var count int64\n    err := r.db.Model(&models.User{}).Count(&count).Error\n    return count, err\n}\n```\n\n### Service Layer\n```go\n// internal/services/user_service.go\npackage services\n\nimport (\n    \"errors\"\n    \"your-app/internal/models\"\n    \"your-app/internal/repositories\"\n    \"your-app/pkg/utils\"\n    \n    \"golang.org/x/crypto/bcrypt\"\n    \"gorm.io/gorm\"\n)\n\ntype UserService interface {\n    CreateUser(req *models.CreateUserRequest) (*models.User, error)\n    GetUser(id uint) (*models.User, error)\n    GetUsers(page, limit int) ([]models.User, int64, error)\n    UpdateUser(id uint, req *models.UpdateUserRequest) (*models.User, error)\n    DeleteUser(id uint) error\n    AuthenticateUser(email, password string) (*models.User, error)\n}\n\ntype userService struct {\n    userRepo repositories.UserRepository\n}\n\nfunc NewUserService(userRepo repositories.UserRepository) UserService {\n    return &userService{\n        userRepo: userRepo,\n    }\n}\n\nfunc (s *userService) CreateUser(req *models.CreateUserRequest) (*models.User, error) {\n    // Check if user already exists\n    existingUser, err := s.userRepo.GetByEmail(req.Email)\n    if err == nil && existingUser != nil {\n        return nil, errors.New(\"user with this email already exists\")\n    }\n\n    // Hash password\n    hashedPassword, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost)\n    if err != nil {\n        return nil, err\n    }\n\n    user := &models.User{\n        Name:     req.Name,\n        Email:    req.Email,\n        Password: string(hashedPassword),\n        IsActive: true,\n    }\n\n    err = s.userRepo.Create(user)\n    if err != nil {\n        return nil, err\n    }\n\n    return user, nil\n}\n\nfunc (s *userService) GetUser(id uint) (*models.User, error) {\n    return s.userRepo.GetByID(id)\n}\n\nfunc (s *userService) GetUsers(page, limit int) ([]models.User, int64, error) {\n    offset := (page - 1) * limit\n    users, err := s.userRepo.GetAll(offset, limit)\n    if err != nil {\n        return nil, 0, err\n    }\n\n    total, err := s.userRepo.Count()\n    if err != nil {\n        return nil, 0, err\n    }\n\n    return users, total, nil\n}\n\nfunc (s *userService) AuthenticateUser(email, password string) (*models.User, error) {\n    user, err := s.userRepo.GetByEmail(email)\n    if err != nil {\n        if errors.Is(err, gorm.ErrRecordNotFound) {\n            return nil, errors.New(\"invalid credentials\")\n        }\n        return nil, err\n    }\n\n    err = bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(password))\n    if err != nil {\n        return nil, errors.New(\"invalid credentials\")\n    }\n\n    if !user.IsActive {\n        return nil, errors.New(\"account is deactivated\")\n    }\n\n    return user, nil\n}\n```\n\n### HTTP Controllers\n```go\n// internal/controllers/user_controller.go\npackage controllers\n\nimport (\n    \"net/http\"\n    \"strconv\"\n\n    \"your-app/internal/models\"\n    \"your-app/internal/services\"\n    \"your-app/pkg/utils\"\n\n    \"github.com/gin-gonic/gin\"\n    \"github.com/go-playground/validator/v10\"\n)\n\ntype UserController struct {\n    userService services.UserService\n    validator   *validator.Validate\n}\n\nfunc NewUserController(userService services.UserService) *UserController {\n    return &UserController{\n        userService: userService,\n        validator:   validator.New(),\n    }\n}\n\n// GetUsers godoc\n// @Summary Get users\n// @Description Get list of users with pagination\n// @Tags users\n// @Accept json\n// @Produce json\n// @Param page query int false \"Page number\" default(1)\n// @Param limit query int false \"Items per page\" default(10)\n// @Success 200 {object} utils.PaginatedResponse\n// @Failure 400 {object} utils.ErrorResponse\n// @Router /users [get]\nfunc (c *UserController) GetUsers(ctx *gin.Context) {\n    page, _ := strconv.Atoi(ctx.DefaultQuery(\"page\", \"1\"))\n    limit, _ := strconv.Atoi(ctx.DefaultQuery(\"limit\", \"10\"))\n\n    if page < 1 {\n        page = 1\n    }\n    if limit < 1 || limit > 100 {\n        limit = 10\n    }\n\n    users, total, err := c.userService.GetUsers(page, limit)\n    if err != nil {\n        utils.ErrorResponse(ctx, http.StatusInternalServerError, \"Failed to get users\", err)\n        return\n    }\n\n    var userResponses []models.UserResponse\n    for _, user := range users {\n        userResponses = append(userResponses, user.ToResponse())\n    }\n\n    utils.PaginatedResponse(ctx, userResponses, page, limit, total)\n}\n\n// CreateUser godoc\n// @Summary Create user\n// @Description Create a new user\n// @Tags users\n// @Accept json\n// @Produce json\n// @Param user body models.CreateUserRequest true \"User data\"\n// @Success 201 {object} models.UserResponse\n// @Failure 400 {object} utils.ErrorResponse\n// @Router /users [post]\nfunc (c *UserController) CreateUser(ctx *gin.Context) {\n    var req models.CreateUserRequest\n    if err := ctx.ShouldBindJSON(&req); err != nil {\n        utils.ValidationErrorResponse(ctx, err)\n        return\n    }\n\n    if err := c.validator.Struct(&req); err != nil {\n        utils.ValidationErrorResponse(ctx, err)\n        return\n    }\n\n    user, err := c.userService.CreateUser(&req)\n    if err != nil {\n        utils.ErrorResponse(ctx, http.StatusBadRequest, \"Failed to create user\", err)\n        return\n    }\n\n    ctx.JSON(http.StatusCreated, user.ToResponse())\n}\n```\n\n### Authentication Middleware\n```go\n// internal/middleware/auth.go\npackage middleware\n\nimport (\n    \"net/http\"\n    \"strings\"\n\n    \"your-app/pkg/utils\"\n\n    \"github.com/gin-gonic/gin\"\n    \"github.com/golang-jwt/jwt/v4\"\n)\n\nfunc AuthRequired() gin.HandlerFunc {\n    return func(c *gin.Context) {\n        tokenString := c.GetHeader(\"Authorization\")\n        if tokenString == \"\" {\n            utils.ErrorResponse(c, http.StatusUnauthorized, \"Authorization header required\", nil)\n            c.Abort()\n            return\n        }\n\n        // Remove \"Bearer \" prefix\n        tokenString = strings.TrimPrefix(tokenString, \"Bearer \")\n\n        token, err := jwt.Parse(tokenString, func(token *jwt.Token) (interface{}, error) {\n            return []byte(utils.GetEnv(\"JWT_SECRET\", \"secret\")), nil\n        })\n\n        if err != nil || !token.Valid {\n            utils.ErrorResponse(c, http.StatusUnauthorized, \"Invalid token\", err)\n            c.Abort()\n            return\n        }\n\n        if claims, ok := token.Claims.(jwt.MapClaims); ok {\n            c.Set(\"user_id\", claims[\"user_id\"])\n            c.Set(\"email\", claims[\"email\"])\n        }\n\n        c.Next()\n    }\n}\n```\n\n## Testing\n\n- Use Go's built-in testing package\n- Write unit tests for services and repositories\n- Use testify for assertions\n- Mock dependencies with interfaces\n- Write integration tests for controllers\n\n## Database\n\n- Use GORM for ORM operations\n- Implement database migrations\n- Use connection pooling\n- Implement proper indexing\n- Handle transactions properly\n\n## Deployment\n\n- Build static binary with Go\n- Use Docker for containerization\n- Deploy with proper environment configuration\n- Set up health checks and monitoring\n- Use graceful shutdown",
      "tags": [
        {
          "tag": {
            "id": "go",
            "name": "go",
            "slug": "go"
          }
        },
        {
          "tag": {
            "id": "gin",
            "name": "gin",
            "slug": "gin"
          }
        },
        {
          "tag": {
            "id": "gorm",
            "name": "gorm",
            "slug": "gorm"
          }
        },
        {
          "tag": {
            "id": "rest-api",
            "name": "rest-api",
            "slug": "rest-api"
          }
        },
        {
          "tag": {
            "id": "golang",
            "name": "golang",
            "slug": "golang"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 20,
        "copies": 162
      },
      "_count": {
        "votes": 59,
        "copies": 177
      },
      "difficulty": "INTERMEDIATE",
      "language": "Go",
      "framework": "Gin",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "microservices-design-master",
      "title": "Microservices Design Master",
      "slug": "microservices-design-master",
      "tagline": "Expert prompt templates prompt template",
      "description": "Expert-level prompt for designing scalable microservices architectures with service mesh, domain boundaries, and communication patterns.",
      "categoryId": "prompt-templates",
      "category": {
        "id": "prompt-templates",
        "name": "Prompt Templates",
        "slug": "prompts",
        "description": "Carefully crafted prompt templates for common development tasks and workflows, featuring expert-level architectural and optimization guidance.",
        "icon": "ðŸ’¬",
        "color": "#10B981"
      },
      "type": "PROMPT_TEMPLATE",
      "content": "You are a Senior Solutions Architect specializing in microservices architecture design. Your expertise spans distributed systems, domain-driven design, service mesh architectures, and enterprise-scale system design.\n\n## Context Analysis\n\nBefore providing recommendations, analyze these key aspects:\n\n1. **Business Domain Complexity**\n   - Identify core business capabilities and bounded contexts\n   - Map domain relationships and dependencies\n   - Assess organizational structure (Conway's Law implications)\n   - Evaluate regulatory and compliance requirements\n\n2. **Technical Landscape**\n   - Current system architecture and technology stack\n   - Data consistency and transaction requirements\n   - Performance and scalability requirements\n   - Security and compliance constraints\n   - Team expertise and organizational maturity\n\n3. **Operational Constraints**\n   - Deployment and infrastructure capabilities\n   - Monitoring and observability requirements\n   - Disaster recovery and business continuity needs\n   - Budget and resource limitations\n\n## Microservices Design Framework\n\n### 1. Domain Decomposition Strategy\n\nProvide a systematic approach to breaking down the monolith:\n\n**Bounded Context Identification:**\n- Apply Domain-Driven Design principles\n- Identify aggregate boundaries\n- Map business capabilities to services\n- Define service ownership and team boundaries\n\n**Service Sizing Guidelines:**\n- Single Responsibility Principle application\n- Database-per-service considerations\n- Team cognitive load assessment\n- Independent deployability requirements\n\n**Example Output:**\n```\nCore Services Identified:\n1. User Management Service\n   - Bounded Context: Identity & Access\n   - Aggregates: User, Role, Permission\n   - Team: Platform Team\n   - Data: User profiles, authentication data\n\n2. Order Processing Service\n   - Bounded Context: Order Management\n   - Aggregates: Order, OrderItem, PaymentInfo\n   - Team: Commerce Team\n   - Data: Orders, payment transactions\n\n3. Inventory Service\n   - Bounded Context: Product Catalog & Inventory\n   - Aggregates: Product, Inventory, Category\n   - Team: Product Team\n   - Data: Product catalog, stock levels\n```\n\n### 2. Service Communication Patterns\n\nDesign comprehensive communication strategies:\n\n**Synchronous Communication:**\n- RESTful API design with proper versioning\n- GraphQL federation for complex queries\n- gRPC for high-performance internal communication\n- Circuit breaker and retry mechanisms\n\n**Asynchronous Communication:**\n- Event-driven architecture patterns\n- Message broker selection (Kafka, RabbitMQ, AWS SQS)\n- Event sourcing and CQRS implementation\n- Saga pattern for distributed transactions\n\n**Example Architecture:**\n```typescript\n// Event-driven communication example\ninterface OrderCreatedEvent {\n  eventId: string;\n  timestamp: string;\n  aggregateId: string;\n  version: number;\n  payload: {\n    orderId: string;\n    customerId: string;\n    items: OrderItem[];\n    totalAmount: number;\n  };\n}\n\n// Saga orchestration for order processing\nclass OrderProcessingSaga {\n  async handle(event: OrderCreatedEvent) {\n    try {\n      await this.reserveInventory(event.payload);\n      await this.processPayment(event.payload);\n      await this.createShipment(event.payload);\n      await this.sendOrderConfirmation(event.payload);\n    } catch (error) {\n      await this.compensate(event, error);\n    }\n  }\n}\n```\n\n### 3. Data Management Strategy\n\nAddress distributed data challenges:\n\n**Database Design:**\n- Database-per-service implementation\n- Polyglot persistence considerations\n- Data consistency patterns (eventual consistency vs strong consistency)\n- Cross-service data synchronization\n\n**Data Patterns:**\n- Event Sourcing for audit trails\n- CQRS for read/write separation\n- Materialized views for complex queries\n- Data lake/warehouse integration\n\n### 4. Service Mesh Architecture\n\nDesign advanced networking and security:\n\n**Service Mesh Selection:**\n- Istio vs Linkerd vs Consul Connect comparison\n- Performance and resource overhead analysis\n- Feature requirements (security, observability, traffic management)\n\n**Traffic Management:**\n- Load balancing strategies\n- Circuit breaking and fault tolerance\n- Blue-green and canary deployments\n- A/B testing infrastructure\n\n**Security Implementation:**\n- mTLS for service-to-service communication\n- Identity and access management integration\n- Policy enforcement (authorization, rate limiting)\n- Certificate lifecycle management\n\n### 5. Observability and Monitoring\n\nImplement comprehensive monitoring:\n\n**Distributed Tracing:**\n- OpenTelemetry implementation across services\n- Trace correlation and context propagation\n- Performance bottleneck identification\n- Error tracking and root cause analysis\n\n**Metrics and Alerting:**\n- Business metrics vs technical metrics\n- SLI/SLO definition for each service\n- Alerting strategies and escalation procedures\n- Cost monitoring and optimization\n\n**Logging Strategy:**\n- Structured logging with correlation IDs\n- Centralized log aggregation (ELK, Fluentd)\n- Log retention and compliance policies\n- Real-time log analysis and alerting\n\n### 6. Deployment and DevOps Strategy\n\nDesign CI/CD for microservices:\n\n**Deployment Patterns:**\n- Independent service deployments\n- Database migration strategies\n- Rollback procedures and data consistency\n- Blue-green deployment implementation\n\n**Container Orchestration:**\n- Kubernetes deployment strategies\n- Service discovery and load balancing\n- Auto-scaling and resource management\n- Multi-environment promotion pipeline\n\n## Architecture Decision Template\n\nFor each major decision, provide this structured analysis:\n\n### Decision: [Service Boundary Definition]\n\n**Context:**\n- Current business requirements\n- Technical constraints\n- Team structure and capabilities\n\n**Options Considered:**\n1. **Option A**: [Description]\n   - Pros: [List advantages]\n   - Cons: [List disadvantages]\n   - Trade-offs: [Key trade-offs]\n\n2. **Option B**: [Description]\n   - Pros: [List advantages]\n   - Cons: [List disadvantages]\n   - Trade-offs: [Key trade-offs]\n\n**Decision:**\n- Chosen approach with rationale\n- Success criteria and metrics\n- Monitoring and validation plan\n\n**Consequences:**\n- Short-term implications\n- Long-term architectural impact\n- Risk mitigation strategies\n\n## Implementation Roadmap\n\nProvide a phased approach:\n\n### Phase 1: Foundation (Months 1-3)\n- Service identification and boundary definition\n- Core infrastructure setup (service mesh, monitoring)\n- First 2-3 services extraction\n- Development team training\n\n### Phase 2: Core Services (Months 4-6)\n- Extract remaining critical services\n- Implement inter-service communication\n- Establish CI/CD pipelines\n- Performance optimization\n\n### Phase 3: Advanced Features (Months 7-9)\n- Advanced patterns (CQRS, Event Sourcing)\n- Security hardening\n- Disaster recovery implementation\n- Performance optimization\n\n### Phase 4: Optimization (Months 10-12)\n- Cost optimization\n- Advanced monitoring and alerting\n- Chaos engineering implementation\n- Documentation and knowledge transfer\n\n## Anti-Patterns to Avoid\n\nHighlight common pitfalls:\n\n1. **Distributed Monolith**: Services too tightly coupled\n2. **Chatty Interfaces**: Excessive inter-service communication\n3. **Shared Database**: Multiple services accessing same database\n4. **Synchronous Everything**: Over-reliance on synchronous communication\n5. **Premature Optimization**: Complex patterns before necessary\n\n## Risk Assessment and Mitigation\n\nIdentify and address key risks:\n\n**Technical Risks:**\n- Network latency and reliability\n- Data consistency challenges\n- Operational complexity\n- Security vulnerabilities\n\n**Organizational Risks:**\n- Team coordination overhead\n- Skill gap and learning curve\n- Increased infrastructure costs\n- Deployment complexity\n\n**Mitigation Strategies:**\n- Comprehensive testing strategies\n- Gradual migration approach\n- Team training and skill development\n- Automation and tooling investment\n\n## Success Metrics\n\nDefine measurable outcomes:\n\n**Technical Metrics:**\n- Service availability (99.9% uptime)\n- Response time improvements (< 200ms p95)\n- Deployment frequency (multiple times per day)\n- Mean time to recovery (< 1 hour)\n\n**Business Metrics:**\n- Feature delivery velocity\n- Time to market for new features\n- System scalability and performance\n- Operational cost efficiency\n\nPlease provide the current system context, business requirements, and any specific constraints or challenges you're facing. I'll create a tailored microservices architecture design with detailed implementation guidance, architectural decision records, and a practical migration roadmap.",
      "tags": [
        {
          "tag": {
            "id": "microservices",
            "name": "microservices",
            "slug": "microservices"
          }
        },
        {
          "tag": {
            "id": "architecture",
            "name": "architecture",
            "slug": "architecture"
          }
        },
        {
          "tag": {
            "id": "domain-driven-design",
            "name": "domain-driven-design",
            "slug": "domain-driven-design"
          }
        },
        {
          "tag": {
            "id": "service-mesh",
            "name": "service-mesh",
            "slug": "service-mesh"
          }
        },
        {
          "tag": {
            "id": "distributed-systems",
            "name": "distributed-systems",
            "slug": "distributed-systems"
          }
        },
        {
          "tag": {
            "id": "api-design",
            "name": "api-design",
            "slug": "api-design"
          }
        },
        {
          "tag": {
            "id": "scalability",
            "name": "scalability",
            "slug": "scalability"
          }
        },
        {
          "tag": {
            "id": "expert",
            "name": "expert",
            "slug": "expert"
          }
        }
      ],
      "author": {
        "name": "Claude Code Directory",
        "url": "https://claudecode.directory"
      },
      "stats": {
        "votes": 20,
        "copies": 304
      },
      "_count": {
        "votes": 87,
        "copies": 290
      },
      "difficulty": "ADVANCED",
      "createdAt": "2024-01-31",
      "lastUpdated": "2024-01-31",
      "featured": true
    },
    {
      "id": "ruby-rails-api",
      "title": "Ruby on Rails API + ActiveRecord",
      "slug": "ruby-rails-api-activerecord",
      "tagline": "Ruby on Rails configuration for intermediate developers",
      "description": "Ruby on Rails API application with ActiveRecord, modern Rails patterns, and comprehensive backend features.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "ðŸ“‹",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Ruby on Rails API + ActiveRecord\n\n## Project Overview\n\nThis is a Ruby on Rails API application using ActiveRecord ORM, following Rails conventions and modern Ruby development practices for scalable backend systems.\n\n## Technology Stack\n\n- **Language**: Ruby 3.2+\n- **Framework**: Ruby on Rails 7.1+\n- **ORM**: ActiveRecord\n- **Database**: PostgreSQL\n- **Authentication**: JWT with Devise\n- **Testing**: RSpec with FactoryBot\n- **Documentation**: RSwag for API docs\n- **Background Jobs**: Sidekiq\n\n## Project Structure\n\n```\napp/\nâ”œâ”€â”€ controllers/         # API controllers\nâ”‚   â””â”€â”€ api/\nâ”‚       â””â”€â”€ v1/         # API version 1\nâ”œâ”€â”€ models/             # ActiveRecord models\nâ”œâ”€â”€ serializers/        # JSON serializers\nâ”œâ”€â”€ services/           # Business logic services\nâ”œâ”€â”€ jobs/               # Background jobs\nâ””â”€â”€ lib/                # Custom libraries\nconfig/\nâ”œâ”€â”€ routes.rb           # Route definitions\nâ”œâ”€â”€ database.yml        # Database configuration\nâ””â”€â”€ application.rb      # Application configuration\ndb/\nâ”œâ”€â”€ migrate/            # Database migrations\nâ””â”€â”€ seeds.rb           # Database seeds\nspec/                  # RSpec tests\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow Ruby style guide and Rails conventions\n- Use Rubocop for code formatting\n- Implement proper error handling\n- Use ActiveRecord validations\n- Follow RESTful resource patterns\n\n### API Design\n- Use Rails API mode\n- Implement proper serialization\n- Use consistent error responses\n- Implement pagination with Kaminari\n- Follow JSON API standards\n\n### Performance\n- Use database indexing\n- Implement eager loading to avoid N+1 queries\n- Use caching with Redis\n- Implement background jobs for heavy tasks\n- Use database connection pooling\n\n## Key Commands\n\n- `rails server` - Start development server\n- `rails console` - Open Rails console\n- `rails generate` - Generate Rails components\n- `rails db:migrate` - Run database migrations\n- `rspec` - Run tests\n- `rubocop` - Run code linting\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nDATABASE_URL=postgresql://user:password@localhost/myapp_development\nREDIS_URL=redis://localhost:6379/0\nJWT_SECRET=your-jwt-secret-key\nRAILS_ENV=development\nSECRET_KEY_BASE=your-secret-key-base\n```\n\n## Common Patterns\n\n### Rails Application Configuration\n```ruby\n# config/application.rb\nrequire_relative \"boot\"\nrequire \"rails\"\n\n# Pick the frameworks you want:\nrequire \"active_model/railtie\"\nrequire \"active_job/railtie\"\nrequire \"active_record/railtie\"\nrequire \"active_storage/engine\"\nrequire \"action_controller/railtie\"\nrequire \"action_mailer/railtie\"\nrequire \"action_mailbox/engine\"\nrequire \"action_text/engine\"\nrequire \"action_view/railtie\"\nrequire \"action_cable/engine\"\n\nBundler.require(*Rails.groups)\n\nmodule MyApp\n  class Application < Rails::Application\n    config.load_defaults 7.1\n\n    # API-only application\n    config.api_only = true\n\n    # CORS configuration\n    config.middleware.insert_before 0, Rack::Cors do\n      allow do\n        origins '*'\n        resource '*',\n                 headers: :any,\n                 methods: [:get, :post, :put, :patch, :delete, :options, :head],\n                 credentials: false\n      end\n    end\n\n    # Timezone\n    config.time_zone = 'UTC'\n\n    # Autoload paths\n    config.autoload_paths += %W(#{config.root}/lib)\n\n    # Active Job queue adapter\n    config.active_job.queue_adapter = :sidekiq\n  end\nend\n```\n\n### ActiveRecord Models\n```ruby\n# app/models/user.rb\nclass User < ApplicationRecord\n  has_secure_password\n\n  has_many :posts, dependent: :destroy\n  has_many :comments, dependent: :destroy\n\n  validates :name, presence: true, length: { minimum: 1, maximum: 100 }\n  validates :email, presence: true, uniqueness: { case_sensitive: false },\n            format: { with: URI::MailTo::EMAIL_REGEXP }\n  validates :password, length: { minimum: 8 }, allow_nil: true\n\n  scope :active, -> { where(is_active: true) }\n  scope :by_name, ->(name) { where('name ILIKE ?', \"%#{name}%\") }\n\n  before_save :downcase_email\n\n  def full_name\n    \"#{first_name} #{last_name}\".strip\n  end\n\n  def active?\n    is_active\n  end\n\n  private\n\n  def downcase_email\n    self.email = email.downcase if email.present?\n  end\nend\n\n# app/models/post.rb\nclass Post < ApplicationRecord\n  belongs_to :user\n  has_many :comments, dependent: :destroy\n\n  validates :title, presence: true, length: { maximum: 255 }\n  validates :content, presence: true\n  validates :status, inclusion: { in: %w[draft published archived] }\n\n  scope :published, -> { where(status: 'published') }\n  scope :by_user, ->(user) { where(user: user) }\n  scope :recent, -> { order(created_at: :desc) }\n\n  enum status: { draft: 0, published: 1, archived: 2 }\n\n  def published?\n    status == 'published'\n  end\nend\n```\n\n### API Controllers\n```ruby\n# app/controllers/application_controller.rb\nclass ApplicationController < ActionController::API\n  include ActionController::HttpAuthentication::Token::ControllerMethods\n\n  before_action :authenticate_user!\n\n  rescue_from ActiveRecord::RecordNotFound, with: :record_not_found\n  rescue_from ActiveRecord::RecordInvalid, with: :record_invalid\n  rescue_from ActionController::ParameterMissing, with: :parameter_missing\n\n  private\n\n  def authenticate_user!\n    token = request.headers['Authorization']&.split(' ')&.last\n    \n    if token.blank?\n      render json: { error: 'Authorization token required' }, status: :unauthorized\n      return\n    end\n\n    begin\n      decoded_token = JWT.decode(token, Rails.application.secret_key_base, true, algorithm: 'HS256')\n      user_id = decoded_token[0]['user_id']\n      @current_user = User.find(user_id)\n    rescue JWT::DecodeError, JWT::ExpiredSignature, ActiveRecord::RecordNotFound\n      render json: { error: 'Invalid or expired token' }, status: :unauthorized\n    end\n  end\n\n  def current_user\n    @current_user\n  end\n\n  def record_not_found(exception)\n    render json: { error: 'Record not found' }, status: :not_found\n  end\n\n  def record_invalid(exception)\n    render json: { \n      error: 'Validation failed', \n      details: exception.record.errors.full_messages \n    }, status: :unprocessable_entity\n  end\n\n  def parameter_missing(exception)\n    render json: { error: exception.message }, status: :bad_request\n  end\nend\n\n# app/controllers/api/v1/users_controller.rb\nclass Api::V1::UsersController < ApplicationController\n  before_action :set_user, only: [:show, :update, :destroy]\n\n  # GET /api/v1/users\n  def index\n    @users = User.active\n    @users = @users.by_name(params[:search]) if params[:search].present?\n    @users = @users.page(params[:page]).per(params[:per_page] || 25)\n\n    render json: {\n      data: ActiveModelSerializers::SerializableResource.new(@users),\n      meta: pagination_meta(@users)\n    }\n  end\n\n  # GET /api/v1/users/:id\n  def show\n    render json: @user, serializer: UserSerializer\n  end\n\n  # POST /api/v1/users\n  def create\n    @user = User.new(user_params)\n\n    if @user.save\n      render json: @user, serializer: UserSerializer, status: :created\n    else\n      render json: { \n        error: 'User creation failed', \n        details: @user.errors.full_messages \n      }, status: :unprocessable_entity\n    end\n  end\n\n  # PATCH/PUT /api/v1/users/:id\n  def update\n    if @user.update(user_params)\n      render json: @user, serializer: UserSerializer\n    else\n      render json: { \n        error: 'User update failed', \n        details: @user.errors.full_messages \n      }, status: :unprocessable_entity\n    end\n  end\n\n  # DELETE /api/v1/users/:id\n  def destroy\n    @user.destroy\n    head :no_content\n  end\n\n  private\n\n  def set_user\n    @user = User.find(params[:id])\n  end\n\n  def user_params\n    params.require(:user).permit(:name, :email, :password, :is_active)\n  end\n\n  def pagination_meta(collection)\n    {\n      current_page: collection.current_page,\n      per_page: collection.limit_value,\n      total_pages: collection.total_pages,\n      total_count: collection.total_count\n    }\n  end\nend\n```\n\n### Serializers\n```ruby\n# app/serializers/user_serializer.rb\nclass UserSerializer < ActiveModel::Serializer\n  attributes :id, :name, :email, :is_active, :created_at, :updated_at\n\n  has_many :posts, serializer: PostSerializer\n\n  def created_at\n    object.created_at.iso8601\n  end\n\n  def updated_at\n    object.updated_at.iso8601\n  end\nend\n\n# app/serializers/post_serializer.rb\nclass PostSerializer < ActiveModel::Serializer\n  attributes :id, :title, :content, :status, :created_at, :updated_at\n\n  belongs_to :user, serializer: UserSerializer\n  has_many :comments, serializer: CommentSerializer\n\n  def created_at\n    object.created_at.iso8601\n  end\n\n  def updated_at\n    object.updated_at.iso8601\n  end\nend\n```\n\n### Service Objects\n```ruby\n# app/services/user_service.rb\nclass UserService\n  def self.create(params)\n    user = User.new(params)\n    \n    if user.save\n      # Send welcome email in background\n      UserMailer.welcome_email(user).deliver_later\n      \n      # Create user profile\n      UserProfile.create(user: user)\n      \n      user\n    else\n      raise ActiveRecord::RecordInvalid, user\n    end\n  end\n\n  def self.authenticate(email, password)\n    user = User.find_by(email: email.downcase)\n    \n    if user&.authenticate(password) && user.active?\n      user\n    else\n      nil\n    end\n  end\n\n  def self.generate_jwt_token(user)\n    payload = {\n      user_id: user.id,\n      email: user.email,\n      exp: 24.hours.from_now.to_i\n    }\n    \n    JWT.encode(payload, Rails.application.secret_key_base, 'HS256')\n  end\nend\n```\n\n### Database Migrations\n```ruby\n# db/migrate/20241201000001_create_users.rb\nclass CreateUsers < ActiveRecord::Migration[7.1]\n  def change\n    create_table :users do |t|\n      t.string :name, null: false\n      t.string :email, null: false\n      t.string :password_digest, null: false\n      t.boolean :is_active, default: true\n      t.timestamps\n    end\n\n    add_index :users, :email, unique: true\n    add_index :users, :is_active\n  end\nend\n\n# db/migrate/20241201000002_create_posts.rb\nclass CreatePosts < ActiveRecord::Migration[7.1]\n  def change\n    create_table :posts do |t|\n      t.string :title, null: false\n      t.text :content, null: false\n      t.integer :status, default: 0\n      t.references :user, null: false, foreign_key: true\n      t.timestamps\n    end\n\n    add_index :posts, :status\n    add_index :posts, :created_at\n  end\nend\n```\n\n### Background Jobs\n```ruby\n# app/jobs/email_job.rb\nclass EmailJob < ApplicationJob\n  queue_as :mailers\n\n  def perform(user_id, email_type, options = {})\n    user = User.find(user_id)\n    \n    case email_type\n    when 'welcome'\n      UserMailer.welcome_email(user).deliver_now\n    when 'notification'\n      UserMailer.notification_email(user, options[:message]).deliver_now\n    end\n  end\nend\n```\n\n### Routes Configuration\n```ruby\n# config/routes.rb\nRails.application.routes.draw do\n  namespace :api do\n    namespace :v1 do\n      resources :users\n      resources :posts do\n        resources :comments, only: [:index, :create, :destroy]\n      end\n\n      # Authentication routes\n      post '/auth/login', to: 'authentication#login'\n      post '/auth/register', to: 'authentication#register'\n      delete '/auth/logout', to: 'authentication#logout'\n\n      # Health check\n      get '/health', to: 'health#check'\n    end\n  end\n\n  # Sidekiq web UI\n  require 'sidekiq/web'\n  mount Sidekiq::Web => '/sidekiq'\nend\n```\n\n### Testing with RSpec\n```ruby\n# spec/models/user_spec.rb\nrequire 'rails_helper'\n\nRSpec.describe User, type: :model do\n  describe 'validations' do\n    it 'is valid with valid attributes' do\n      user = build(:user)\n      expect(user).to be_valid\n    end\n\n    it 'is not valid without a name' do\n      user = build(:user, name: nil)\n      expect(user).not_to be_valid\n    end\n\n    it 'is not valid with duplicate email' do\n      create(:user, email: 'test@example.com')\n      user = build(:user, email: 'test@example.com')\n      expect(user).not_to be_valid\n    end\n  end\n\n  describe 'associations' do\n    it 'has many posts' do\n      association = described_class.reflect_on_association(:posts)\n      expect(association.macro).to eq :has_many\n    end\n  end\n\n  describe 'scopes' do\n    let!(:active_user) { create(:user, is_active: true) }\n    let!(:inactive_user) { create(:user, is_active: false) }\n\n    it 'returns only active users' do\n      expect(User.active).to include(active_user)\n      expect(User.active).not_to include(inactive_user)\n    end\n  end\nend\n\n# spec/factories/users.rb\nFactoryBot.define do\n  factory :user do\n    name { Faker::Name.name }\n    email { Faker::Internet.email }\n    password { 'password123' }\n    is_active { true }\n  end\nend\n```\n\n## Testing\n\n- Use RSpec for testing framework\n- Use FactoryBot for test data\n- Test models, controllers, and services\n- Use VCR for external API testing\n- Write integration tests for API endpoints\n\n## Database\n\n- Use ActiveRecord for ORM operations\n- Write proper database migrations\n- Use database indexing for performance\n- Implement database seeds for development\n- Use database transactions for complex operations\n\n## Deployment\n\n- Use Capistrano for deployment\n- Configure for production environment\n- Set up background job processing\n- Implement proper logging\n- Use environment-specific configurations",
      "tags": [
        {
          "tag": {
            "id": "ruby",
            "name": "ruby",
            "slug": "ruby"
          }
        },
        {
          "tag": {
            "id": "rails",
            "name": "rails",
            "slug": "rails"
          }
        },
        {
          "tag": {
            "id": "activerecord",
            "name": "activerecord",
            "slug": "activerecord"
          }
        },
        {
          "tag": {
            "id": "api",
            "name": "api",
            "slug": "api"
          }
        },
        {
          "tag": {
            "id": "ruby-on-rails",
            "name": "ruby-on-rails",
            "slug": "ruby-on-rails"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 16,
        "copies": 87
      },
      "_count": {
        "votes": 52,
        "copies": 100
      },
      "difficulty": "INTERMEDIATE",
      "language": "Ruby",
      "framework": "Ruby on Rails",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "git-workflows-advanced",
      "title": "Advanced Git Workflows + GitOps",
      "slug": "advanced-git-workflows-gitops-automation",
      "tagline": "Git + GitHub Actions + ArgoCD configuration for advanced developers",
      "description": "Comprehensive Git workflow automation with GitOps principles, release automation, advanced branching strategies, and CI/CD integration for enterprise-scale development teams.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "ðŸ“‹",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Advanced Git Workflows + GitOps\n\n## Project Overview\n\nThis is an advanced Git workflow configuration designed for enterprise-scale development teams that require sophisticated branching strategies, automated release management, GitOps principles, and comprehensive CI/CD integration. It provides battle-tested patterns for managing complex codebases with multiple teams and deployment environments.\n\n## Development Philosophy\n\n### GitOps Principles\n1. **Declarative Configuration**: Infrastructure and applications defined declaratively\n2. **Version Controlled**: All configuration stored in Git repositories\n3. **Automated Deployment**: Changes automatically deployed via Git operations\n4. **Convergence**: Systems automatically converge to desired state\n5. **Observability**: Full audit trail of all changes\n\n### Advanced Git Workflow Benefits\n- **Parallel Development**: Multiple features developed simultaneously\n- **Release Management**: Automated versioning and changelog generation\n- **Quality Gates**: Automated testing and approval processes\n- **Rollback Capability**: Safe rollback to any previous state\n- **Branch Protection**: Enforced code review and testing requirements\n- **Semantic Versioning**: Automated version bumping based on commit messages\n\n## Technology Stack\n\n- **Version Control**: Git with advanced hooks and automation\n- **Repository Platform**: GitHub/GitLab with enterprise features\n- **CI/CD**: GitHub Actions / GitLab CI / Jenkins\n- **GitOps**: ArgoCD / Flux for Kubernetes deployments\n- **Release Management**: Semantic Release / Release Please\n- **Code Quality**: Pre-commit hooks, SonarQube, CodeClimate\n- **Security**: Dependabot, Snyk, Git secret scanning\n\n## Project Structure\n\n```\nenterprise-git-workflows/\nâ”œâ”€â”€ .github/                          # GitHub-specific configurations\nâ”‚   â”œâ”€â”€ workflows/                    # GitHub Actions CI/CD\nâ”‚   â”‚   â”œâ”€â”€ ci.yml                   # Continuous Integration\nâ”‚   â”‚   â”œâ”€â”€ cd.yml                   # Continuous Deployment\nâ”‚   â”‚   â”œâ”€â”€ release.yml              # Automated releases\nâ”‚   â”‚   â”œâ”€â”€ security.yml             # Security scanning\nâ”‚   â”‚   â””â”€â”€ gitops-sync.yml          # GitOps synchronization\nâ”‚   â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md     # PR template\nâ”‚   â”œâ”€â”€ ISSUE_TEMPLATE/              # Issue templates\nâ”‚   â””â”€â”€ CODEOWNERS                   # Code ownership rules\nâ”œâ”€â”€ .gitops/                         # GitOps configurations\nâ”‚   â”œâ”€â”€ applications/                # Application definitions\nâ”‚   â”‚   â”œâ”€â”€ staging/                 # Staging environment\nâ”‚   â”‚   â”œâ”€â”€ production/              # Production environment\nâ”‚   â”‚   â””â”€â”€ development/             # Development environment\nâ”‚   â”œâ”€â”€ infrastructure/              # Infrastructure as Code\nâ”‚   â”‚   â”œâ”€â”€ terraform/               # Terraform configurations\nâ”‚   â”‚   â”œâ”€â”€ helm/                    # Helm charts\nâ”‚   â”‚   â””â”€â”€ kustomize/               # Kustomize overlays\nâ”‚   â””â”€â”€ policies/                    # OPA policies\nâ”œâ”€â”€ scripts/                         # Automation scripts\nâ”‚   â”œâ”€â”€ git-hooks/                   # Git hook scripts\nâ”‚   â”‚   â”œâ”€â”€ pre-commit               # Pre-commit validation\nâ”‚   â”‚   â”œâ”€â”€ pre-push                 # Pre-push validation\nâ”‚   â”‚   â”œâ”€â”€ commit-msg               # Commit message validation\nâ”‚   â”‚   â””â”€â”€ post-merge               # Post-merge actions\nâ”‚   â”œâ”€â”€ release/                     # Release automation\nâ”‚   â”‚   â”œâ”€â”€ prepare-release.sh       # Release preparation\nâ”‚   â”‚   â”œâ”€â”€ generate-changelog.sh    # Changelog generation\nâ”‚   â”‚   â””â”€â”€ tag-version.sh          # Version tagging\nâ”‚   â”œâ”€â”€ quality/                     # Code quality scripts\nâ”‚   â”‚   â”œâ”€â”€ run-tests.sh            # Test execution\nâ”‚   â”‚   â”œâ”€â”€ lint-code.sh            # Code linting\nâ”‚   â”‚   â””â”€â”€ security-scan.sh        # Security scanning\nâ”‚   â””â”€â”€ gitops/                      # GitOps utilities\nâ”‚       â”œâ”€â”€ sync-manifests.sh        # Manifest synchronization\nâ”‚       â”œâ”€â”€ promote-release.sh       # Release promotion\nâ”‚       â””â”€â”€ rollback.sh              # Rollback automation\nâ”œâ”€â”€ docs/                            # Documentation\nâ”‚   â”œâ”€â”€ git-workflow-guide.md        # Workflow documentation\nâ”‚   â”œâ”€â”€ branching-strategy.md        # Branching strategy\nâ”‚   â”œâ”€â”€ release-process.md           # Release process\nâ”‚   â””â”€â”€ troubleshooting.md           # Common issues\nâ”œâ”€â”€ .pre-commit-config.yaml          # Pre-commit configuration\nâ”œâ”€â”€ .releaserc.json                  # Semantic release config\nâ”œâ”€â”€ BRANCHING_STRATEGY.md            # Branching strategy doc\nâ””â”€â”€ GIT_WORKFLOW.md                  # Workflow guide\n```\n\n## Branching Strategy\n\n### GitFlow Enhanced Model\n```\nmain (production)           â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€\n                              â”‚      â”‚      â”‚      â”‚\nrelease/v2.1.0               â—â”€â”€â—â”€â”€â—â”€â”€â—      â”‚      â”‚\n                             â”‚     â”‚         â”‚      â”‚\ndevelop (integration)    â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€\n                           â”‚  â”‚     â”‚  â”‚     â”‚  â”‚\nfeature/user-auth       â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â”‚  â”‚     â”‚  â”‚\n                              â”‚     â”‚  â”‚     â”‚  â”‚\nfeature/payment-flow       â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â”‚     â”‚  â”‚\n                                     â”‚     â”‚  â”‚\nhotfix/security-patch             â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â”‚\n                                           â”‚\nbugfix/login-issue                    â”€â”€â—â”€â”€â—â”€â”€\n```\n\n### Branch Types and Rules\n1. **main**: Production-ready code, protected, requires PR\n2. **develop**: Integration branch, automated testing\n3. **feature/***: Feature development, created from develop\n4. **release/***: Release preparation, version bumping\n5. **hotfix/***: Critical fixes, created from main\n6. **bugfix/***: Bug fixes, created from develop\n\n## Git Hooks Configuration\n\n### Pre-commit Hook\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\nset -e\n\necho \"ðŸ” Running pre-commit checks...\"\n\n# Check for merge conflicts\nif grep -r \"<<<<<<< HEAD\" . --exclude-dir=.git; then\n    echo \"âŒ Merge conflict markers found. Please resolve conflicts.\"\n    exit 1\nfi\n\n# Prevent commits to protected branches\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\nPROTECTED_BRANCHES=\"^(main|master|develop)$\"\n\nif [[ $BRANCH =~ $PROTECTED_BRANCHES ]]; then\n    echo \"âŒ Direct commits to $BRANCH are not allowed. Please use a feature branch.\"\n    exit 1\nfi\n\n# Check commit message format\nCOMMIT_MSG=$(cat .git/COMMIT_EDITMSG 2>/dev/null || echo \"\")\nif [[ ! $COMMIT_MSG =~ ^(feat|fix|docs|style|refactor|test|chore|ci|build|perf)((.+))?: .+ ]]; then\n    echo \"âŒ Commit message must follow Conventional Commits format:\"\n    echo \"   <type>[optional scope]: <description>\"\n    echo \"   Example: feat(auth): add OAuth2 login\"\n    exit 1\nfi\n\n# Run code quality checks\necho \"ðŸ§¹ Running code quality checks...\"\n\n# ESLint for JavaScript/TypeScript\nif command -v npx &> /dev/null && [ -f \"package.json\" ]; then\n    npx eslint . --ext .js,.jsx,.ts,.tsx --fix\nfi\n\n# Black for Python\nif command -v black &> /dev/null && find . -name \"*.py\" | grep -q .; then\n    black --check .\nfi\n\n# Go fmt for Go\nif command -v go &> /dev/null && find . -name \"*.go\" | grep -q .; then\n    go fmt ./...\nfi\n\n# Rust fmt for Rust\nif command -v cargo &> /dev/null && [ -f \"Cargo.toml\" ]; then\n    cargo fmt --check\nfi\n\n# Run tests\necho \"ðŸ§ª Running tests...\"\nif [ -f \"package.json\" ] && grep -q '\"test\"' package.json; then\n    npm test -- --passWithNoTests\nfi\n\nif [ -f \"Cargo.toml\" ]; then\n    cargo test\nfi\n\nif find . -name \"*.py\" | grep -q . && [ -f \"requirements.txt\" ]; then\n    python -m pytest --tb=short\nfi\n\n# Security checks\necho \"ðŸ”’ Running security checks...\"\n\n# Check for secrets\nif command -v truffleHog &> /dev/null; then\n    truffleHog --regex --entropy=False .\nfi\n\n# Dependency vulnerability check\nif command -v npm &> /dev/null && [ -f \"package.json\" ]; then\n    npm audit --audit-level=moderate\nfi\n\nif command -v cargo &> /dev/null && [ -f \"Cargo.toml\" ]; then\n    cargo audit\nfi\n\necho \"âœ… All pre-commit checks passed!\"\n```\n\n### Commit Message Hook\n```bash\n#!/bin/bash\n# .git/hooks/commit-msg\n\nCOMMIT_MSG_FILE=$1\nCOMMIT_MSG=$(cat $COMMIT_MSG_FILE)\n\n# Conventional Commits pattern\nPATTERN=\"^(feat|fix|docs|style|refactor|test|chore|ci|build|perf)((.+))?: .{1,50}\"\n\nif [[ ! $COMMIT_MSG =~ $PATTERN ]]; then\n    echo \"âŒ Invalid commit message format!\"\n    echo \"\"\n    echo \"Commit message must follow Conventional Commits specification:\"\n    echo \"\"\n    echo \"Format: <type>[optional scope]: <description>\"\n    echo \"\"\n    echo \"Types:\"\n    echo \"  feat:     A new feature\"\n    echo \"  fix:      A bug fix\"\n    echo \"  docs:     Documentation only changes\"\n    echo \"  style:    Code style changes (formatting, etc.)\"\n    echo \"  refactor: Code refactoring\"\n    echo \"  test:     Adding or updating tests\"\n    echo \"  chore:    Maintenance tasks\"\n    echo \"  ci:       CI/CD related changes\"\n    echo \"  build:    Build system changes\"\n    echo \"  perf:     Performance improvements\"\n    echo \"\"\n    echo \"Examples:\"\n    echo \"  feat(auth): add OAuth2 login support\"\n    echo \"  fix(api): resolve user validation bug\"\n    echo \"  docs: update README with new setup instructions\"\n    echo \"\"\n    exit 1\nfi\n\n# Check description length\nDESCRIPTION=$(echo \"$COMMIT_MSG\" | head -n1 | sed 's/^[^:]*: //')\nif [ ${#DESCRIPTION} -gt 50 ]; then\n    echo \"âš ï¸  Warning: Commit message description is longer than 50 characters\"\n    echo \"   Consider making it more concise for better readability\"\nfi\n\n# Check for body separation\nif [ $(echo \"$COMMIT_MSG\" | wc -l) -gt 1 ]; then\n    SECOND_LINE=$(echo \"$COMMIT_MSG\" | sed -n '2p')\n    if [ -n \"$SECOND_LINE\" ]; then\n        echo \"âŒ Second line of commit message must be blank\"\n        echo \"   Use: <title>\\n\\n<body>\"\n        exit 1\n    fi\nfi\n\necho \"âœ… Commit message format is valid\"\n```\n\n## GitHub Actions Workflows\n\n### Continuous Integration\n```yaml\n# .github/workflows/ci.yml\nname: Continuous Integration\n\non:\n  push:\n    branches: [ develop, 'feature/*', 'bugfix/*' ]\n  pull_request:\n    branches: [ main, develop ]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  quality-checks:\n    name: Code Quality & Security\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run ESLint\n        run: npm run lint\n\n      - name: Run Prettier\n        run: npm run format:check\n\n      - name: Type checking\n        run: npm run type-check\n\n      - name: Security audit\n        run: npm audit --audit-level=moderate\n\n      - name: Run tests with coverage\n        run: npm run test:coverage\n\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v3\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n\n      - name: SonarCloud Scan\n        uses: SonarSource/sonarcloud-github-action@master\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\n\n  build:\n    name: Build & Test\n    runs-on: ubuntu-latest\n    needs: quality-checks\n    \n    strategy:\n      matrix:\n        node-version: [18, 20, 22]\n        \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n\n      - name: Run unit tests\n        run: npm run test:unit\n\n      - name: Run integration tests\n        run: npm run test:integration\n\n  e2e-tests:\n    name: End-to-End Tests\n    runs-on: ubuntu-latest\n    needs: build\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Install Playwright\n        run: npx playwright install --with-deps\n\n      - name: Start application\n        run: npm run dev &\n\n      - name: Wait for application\n        run: npx wait-on http://localhost:3000\n\n      - name: Run E2E tests\n        run: npm run test:e2e\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v3\n        if: failure()\n        with:\n          name: playwright-report\n          path: playwright-report/\n\n  security-scan:\n    name: Security Scanning\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: '.'\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n\n      - name: Upload Trivy scan results\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: 'trivy-results.sarif'\n\n      - name: Snyk security scan\n        uses: snyk/actions/node@master\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        with:\n          args: --severity-threshold=high\n```\n\n### Continuous Deployment\n```yaml\n# .github/workflows/cd.yml\nname: Continuous Deployment\n\non:\n  push:\n    branches: [ main ]\n    tags: [ 'v*' ]\n\njobs:\n  deploy-staging:\n    name: Deploy to Staging\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    environment: staging\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n        env:\n          NODE_ENV: production\n\n      - name: Build Docker image\n        run: |\n          docker build -t myapp:staging .\n          docker tag myapp:staging myregistry.com/myapp:staging\n\n      - name: Login to registry\n        uses: docker/login-action@v2\n        with:\n          registry: myregistry.com\n          username: ${{ secrets.REGISTRY_USERNAME }}\n          password: ${{ secrets.REGISTRY_PASSWORD }}\n\n      - name: Push Docker image\n        run: docker push myregistry.com/myapp:staging\n\n      - name: Update GitOps repository\n        run: |\n          git clone https://${{ secrets.GITOPS_TOKEN }}@github.com/myorg/gitops-repo.git\n          cd gitops-repo\n          sed -i 's|image:.*|image: myregistry.com/myapp:staging|' applications/staging/deployment.yaml\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add .\n          git commit -m \"chore(staging): update image to staging\"\n          git push\n\n  deploy-production:\n    name: Deploy to Production\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/v')\n    environment: production\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Get version from tag\n        id: version\n        run: echo \"version=${GITHUB_REF#refs/tags/}\" >> $GITHUB_OUTPUT\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n        env:\n          NODE_ENV: production\n\n      - name: Build Docker image\n        run: |\n          docker build -t myapp:${{ steps.version.outputs.version }} .\n          docker tag myapp:${{ steps.version.outputs.version }} myregistry.com/myapp:${{ steps.version.outputs.version }}\n          docker tag myapp:${{ steps.version.outputs.version }} myregistry.com/myapp:latest\n\n      - name: Login to registry\n        uses: docker/login-action@v2\n        with:\n          registry: myregistry.com\n          username: ${{ secrets.REGISTRY_USERNAME }}\n          password: ${{ secrets.REGISTRY_PASSWORD }}\n\n      - name: Push Docker images\n        run: |\n          docker push myregistry.com/myapp:${{ steps.version.outputs.version }}\n          docker push myregistry.com/myapp:latest\n\n      - name: Update GitOps repository\n        run: |\n          git clone https://${{ secrets.GITOPS_TOKEN }}@github.com/myorg/gitops-repo.git\n          cd gitops-repo\n          sed -i 's|image:.*|image: myregistry.com/myapp:${{ steps.version.outputs.version }}|' applications/production/deployment.yaml\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add .\n          git commit -m \"chore(production): deploy version ${{ steps.version.outputs.version }}\"\n          git push\n\n      - name: Create GitHub release\n        uses: softprops/action-gh-release@v1\n        with:\n          files: |\n            dist/*.tar.gz\n            dist/*.zip\n          generate_release_notes: true\n```\n\n### Automated Release Management\n```yaml\n# .github/workflows/release.yml\nname: Release Management\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  release:\n    name: Create Release\n    runs-on: ubuntu-latest\n    if: \"!contains(github.event.head_commit.message, 'chore(release)')\"\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run tests\n        run: npm run test\n\n      - name: Build application\n        run: npm run build\n\n      - name: Release\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n```\n\n## GitOps Configuration\n\n### ArgoCD Application\n```yaml\n# .gitops/applications/staging/application.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp-staging\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/myorg/gitops-repo.git\n    targetRevision: HEAD\n    path: applications/staging\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: staging\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n      allowEmpty: false\n    syncOptions:\n      - CreateNamespace=true\n      - PrunePropagationPolicy=foreground\n      - PruneLast=true\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\n  revisionHistoryLimit: 10\n```\n\n### Kustomize Overlay\n```yaml\n# .gitops/applications/staging/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: staging\n\nresources:\n  - ../../base\n\nimages:\n  - name: myapp\n    newName: myregistry.com/myapp\n    newTag: staging\n\npatchesStrategicMerge:\n  - deployment-patch.yaml\n  - service-patch.yaml\n\nconfigMapGenerator:\n  - name: app-config\n    files:\n      - config.yaml\n    options:\n      disableNameSuffixHash: true\n\nsecretGenerator:\n  - name: app-secrets\n    envs:\n      - secrets.env\n    options:\n      disableNameSuffixHash: true\n```\n\n## Release Automation Scripts\n\n### Semantic Release Configuration\n```json\n{\n  \"branches\": [\n    \"main\",\n    {\n      \"name\": \"develop\",\n      \"prerelease\": \"beta\"\n    }\n  ],\n  \"plugins\": [\n    \"@semantic-release/commit-analyzer\",\n    \"@semantic-release/release-notes-generator\",\n    \"@semantic-release/changelog\",\n    \"@semantic-release/npm\",\n    \"@semantic-release/github\",\n    [\n      \"@semantic-release/git\",\n      {\n        \"assets\": [\n          \"CHANGELOG.md\",\n          \"package.json\",\n          \"package-lock.json\"\n        ],\n        \"message\": \"chore(release): ${nextRelease.version} [skip ci]\\n\\n${nextRelease.notes}\"\n      }\n    ]\n  ],\n  \"preset\": \"conventionalcommits\",\n  \"releaseRules\": [\n    { \"type\": \"feat\", \"release\": \"minor\" },\n    { \"type\": \"fix\", \"release\": \"patch\" },\n    { \"type\": \"perf\", \"release\": \"patch\" },\n    { \"type\": \"revert\", \"release\": \"patch\" },\n    { \"type\": \"docs\", \"scope\": \"README\", \"release\": \"patch\" },\n    { \"type\": \"refactor\", \"release\": \"patch\" },\n    { \"type\": \"style\", \"release\": false },\n    { \"type\": \"chore\", \"release\": false },\n    { \"type\": \"test\", \"release\": false },\n    { \"scope\": \"no-release\", \"release\": false }\n  ]\n}\n```\n\n### Release Preparation Script\n```bash\n#!/bin/bash\n# scripts/release/prepare-release.sh\n\nset -e\n\nVERSION_TYPE=${1:-\"auto\"}\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\n\necho \"ðŸš€ Preparing release from branch: $BRANCH\"\n\n# Ensure we're on the right branch\nif [ \"$BRANCH\" != \"main\" ] && [ \"$BRANCH\" != \"develop\" ]; then\n    echo \"âŒ Releases can only be prepared from main or develop branch\"\n    exit 1\nfi\n\n# Ensure working directory is clean\nif [ -n \"$(git status --porcelain)\" ]; then\n    echo \"âŒ Working directory is not clean. Please commit or stash changes.\"\n    exit 1\nfi\n\n# Fetch latest changes\ngit fetch origin\n\n# Ensure branch is up to date\nif [ \"$(git rev-parse HEAD)\" != \"$(git rev-parse @{u})\" ]; then\n    echo \"âŒ Local branch is not up to date with remote. Please pull latest changes.\"\n    exit 1\nfi\n\n# Run tests\necho \"ðŸ§ª Running tests...\"\nnpm run test\n\n# Run linting\necho \"ðŸ§¹ Running linting...\"\nnpm run lint\n\n# Run build\necho \"ðŸ—ï¸  Building application...\"\nnpm run build\n\n# Generate changelog\necho \"ðŸ“ Generating changelog...\"\nnpm run changelog\n\n# Determine version bump\nif [ \"$VERSION_TYPE\" = \"auto\" ]; then\n    # Analyze commits since last release\n    LAST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo \"\")\n    \n    if [ -n \"$LAST_TAG\" ]; then\n        COMMITS=$(git log $LAST_TAG..HEAD --oneline)\n    else\n        COMMITS=$(git log --oneline)\n    fi\n    \n    if echo \"$COMMITS\" | grep -q \"^[a-f0-9]\\+ feat\"; then\n        VERSION_TYPE=\"minor\"\n    elif echo \"$COMMITS\" | grep -q \"^[a-f0-9]\\+ fix\"; then\n        VERSION_TYPE=\"patch\"\n    else\n        VERSION_TYPE=\"patch\"\n    fi\nfi\n\necho \"ðŸ“¦ Version bump type: $VERSION_TYPE\"\n\n# Bump version\nnpm version $VERSION_TYPE --no-git-tag-version\n\nNEW_VERSION=$(node -p \"require('./package.json').version\")\n\necho \"âœ… Prepared release version: v$NEW_VERSION\"\necho \"   Next steps:\"\necho \"   1. Review changes\"\necho \"   2. Commit version bump: git commit -am 'chore(release): v$NEW_VERSION'\"\necho \"   3. Create and push tag: git tag v$NEW_VERSION && git push origin v$NEW_VERSION\"\n```\n\n## Advanced Git Commands and Utilities\n\n### Git Aliases Configuration\n```bash\n# Add to ~/.gitconfig or run as commands\n\n# Workflow aliases\ngit config --global alias.sw 'switch'\ngit config --global alias.co 'checkout'\ngit config --global alias.br 'branch'\ngit config --global alias.st 'status --short --branch'\ngit config --global alias.ci 'commit'\ngit config --global alias.unstage 'reset HEAD --'\ngit config --global alias.last 'log -1 HEAD'\n\n# Enhanced logging\ngit config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit\"\ngit config --global alias.lga \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit --all\"\n\n# Branch management\ngit config --global alias.bra 'branch -a'\ngit config --global alias.brd 'branch -d'\ngit config --global alias.brD 'branch -D'\ngit config --global alias.brc 'checkout -b'\n\n# Stash operations\ngit config --global alias.sl 'stash list'\ngit config --global alias.sa 'stash apply'\ngit config --global alias.ss 'stash save'\ngit config --global alias.sp 'stash pop'\n\n# Advanced operations\ngit config --global alias.amend 'commit --amend --no-edit'\ngit config --global alias.amendm 'commit --amend'\ngit config --global alias.pushf 'push --force-with-lease'\ngit config --global alias.pullr 'pull --rebase'\n\n# Find operations\ngit config --global alias.find 'log --all --full-history -- '\ngit config --global alias.grep 'grep --break --heading --line-number'\n\n# Cleanup operations\ngit config --global alias.cleanup 'remote prune origin'\ngit config --global alias.prune-branches '!git branch --merged | grep -v \"\\*\\|main\\|develop\" | xargs -n 1 git branch -d'\n```\n\n### GitOps Sync Script\n```bash\n#!/bin/bash\n# scripts/gitops/sync-manifests.sh\n\nset -e\n\nENVIRONMENT=${1:-\"staging\"}\nIMAGE_TAG=${2:-\"latest\"}\nGITOPS_REPO=${3:-\"git@github.com:myorg/gitops-repo.git\"}\n\necho \"ðŸ”„ Syncing GitOps manifests for $ENVIRONMENT\"\n\n# Clone or update GitOps repository\nif [ -d \"gitops-repo\" ]; then\n    cd gitops-repo\n    git pull origin main\nelse\n    git clone $GITOPS_REPO gitops-repo\n    cd gitops-repo\nfi\n\n# Update application manifests\nMANIFEST_PATH=\"applications/$ENVIRONMENT\"\n\nif [ ! -d \"$MANIFEST_PATH\" ]; then\n    echo \"âŒ Environment $ENVIRONMENT not found in GitOps repository\"\n    exit 1\nfi\n\n# Update image tag in Kustomization\nsed -i \"s|newTag:.*|newTag: $IMAGE_TAG|\" $MANIFEST_PATH/kustomization.yaml\n\n# Validate manifests\necho \"âœ… Validating Kubernetes manifests...\"\nkubectl kustomize $MANIFEST_PATH > /tmp/manifest-validation.yaml\nkubectl --dry-run=client apply -f /tmp/manifest-validation.yaml\n\n# Commit and push changes\ngit add .\nif git diff --staged --quiet; then\n    echo \"â„¹ï¸  No changes to commit\"\nelse\n    git commit -m \"chore($ENVIRONMENT): update image to $IMAGE_TAG\"\n    git push origin main\n    echo \"âœ… GitOps manifests updated successfully\"\nfi\n\ncd ..\nrm -rf gitops-repo\n```\n\n## Workflow Documentation\n\n### Developer Workflow\n1. **Feature Development**:\n   - Create feature branch from develop: `git checkout -b feature/new-feature develop`\n   - Implement feature with regular commits using conventional commit format\n   - Run local tests and quality checks\n   - Push branch and create Pull Request\n\n2. **Code Review Process**:\n   - Automated CI checks must pass\n   - Require at least 2 approvals from code owners\n   - All conversations must be resolved\n   - Branch must be up to date with target branch\n\n3. **Release Process**:\n   - Merge develop to main triggers release preparation\n   - Semantic versioning automatically determined from commit messages\n   - Automated testing in staging environment\n   - Manual approval required for production deployment\n\n4. **Hotfix Process**:\n   - Create hotfix branch from main: `git checkout -b hotfix/critical-fix main`\n   - Implement fix and test thoroughly\n   - Fast-track review process for critical issues\n   - Deploy to production and back-merge to develop\n\nThis advanced Git workflow provides enterprise-grade automation, quality control, and deployment capabilities while maintaining developer productivity and code quality standards.",
      "tags": [
        {
          "tag": {
            "id": "git",
            "name": "git",
            "slug": "git"
          }
        },
        {
          "tag": {
            "id": "gitops",
            "name": "gitops",
            "slug": "gitops"
          }
        },
        {
          "tag": {
            "id": "cicd",
            "name": "cicd",
            "slug": "cicd"
          }
        },
        {
          "tag": {
            "id": "automation",
            "name": "automation",
            "slug": "automation"
          }
        },
        {
          "tag": {
            "id": "workflows",
            "name": "workflows",
            "slug": "workflows"
          }
        },
        {
          "tag": {
            "id": "branching",
            "name": "branching",
            "slug": "branching"
          }
        },
        {
          "tag": {
            "id": "release-management",
            "name": "release-management",
            "slug": "release-management"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 16,
        "copies": 78
      },
      "_count": {
        "votes": 53,
        "copies": 129
      },
      "difficulty": "ADVANCED",
      "language": "Shell",
      "framework": "Git + GitHub Actions + ArgoCD",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    }
  ],
  "meta": {
    "total": 12,
    "limit": 12,
    "generated_at": "2025-07-31T22:24:27.418Z"
  }
}